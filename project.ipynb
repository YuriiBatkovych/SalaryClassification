{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9929e2d",
   "metadata": {},
   "source": [
    "# Salary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1560f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248124e",
   "metadata": {},
   "source": [
    "Wczytajmy dane znajdujące się w folderze ./data , plik salary.csv i spojrzmy na ich strukturę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172746e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_data(filename, FOLDER=\"./data\"):\n",
    "    csv_path = os.path.join(FOLDER, filename)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0150371d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  salary  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_data(\"salary.csv\")\n",
    "display(data.head())\n",
    "\n",
    "data = data[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf14312",
   "metadata": {},
   "source": [
    "Columns are:\n",
    " - age: continuous.\n",
    " - workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    " - fnlwgt: continuous.\n",
    " - education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th,    10th, Doctorate, 5th-6th, Preschool.\n",
    " - education-num: continuous.\n",
    " - marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    " - occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op- inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    " - relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    " - race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    " - sex: Female, Male.\n",
    " - capital-gain: continuous.\n",
    " - capital-loss: continuous.\n",
    " - hours-per-week: continuous.\n",
    " - native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    " - salary: <=50K or >50K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5ec5f",
   "metadata": {},
   "source": [
    "Just to make some not obvious things clear:\n",
    "\n",
    "- The continuous variable fnlwgt represents final weight, which is the number of units in the target population that the responding unit represents.\n",
    "- education-num - number of years of education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5672110",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.columns] = data[data.columns].replace(\" ?\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4edfb7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             5000 non-null   int64 \n",
      " 1   workclass       4669 non-null   object\n",
      " 2   fnlwgt          5000 non-null   int64 \n",
      " 3   education       5000 non-null   object\n",
      " 4   education-num   5000 non-null   int64 \n",
      " 5   marital-status  5000 non-null   object\n",
      " 6   occupation      4669 non-null   object\n",
      " 7   relationship    5000 non-null   object\n",
      " 8   race            5000 non-null   object\n",
      " 9   sex             5000 non-null   object\n",
      " 10  capital-gain    5000 non-null   int64 \n",
      " 11  capital-loss    5000 non-null   int64 \n",
      " 12  hours-per-week  5000 non-null   int64 \n",
      " 13  native-country  4903 non-null   object\n",
      " 14  salary          5000 non-null   object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 586.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb83db",
   "metadata": {},
   "source": [
    "Widzimy, że w ałym datasecie mamy 32561 wiersze.\n",
    "\n",
    "W kolumnie workclass mamy 1836 wartości null (5.6%) \n",
    "\n",
    "W kolumnie occupation mamy 1843 wartości null (5.7%)\n",
    "\n",
    "W kolumnie native-country mamy 583 wartości null (1.8%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd95d65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 0\n",
       "workclass         331\n",
       "fnlwgt              0\n",
       "education           0\n",
       "education-num       0\n",
       "marital-status      0\n",
       "occupation        331\n",
       "relationship        0\n",
       "race                0\n",
       "sex                 0\n",
       "capital-gain        0\n",
       "capital-loss        0\n",
       "hours-per-week      0\n",
       "native-country     97\n",
       "salary              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73124e5",
   "metadata": {},
   "source": [
    "Widzimy, że mamy stosunkowo mało wierszy z nullową kollumną native-country. \\\n",
    "Najlepszym i najprostszym podejściem w takiej sytuacji wydaje się być po prostu usunięcie tych rekordów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9094541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data['native-country'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30df2635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 0\n",
      "workclass         323\n",
      "fnlwgt              0\n",
      "education           0\n",
      "education-num       0\n",
      "marital-status      0\n",
      "occupation        323\n",
      "relationship        0\n",
      "race                0\n",
      "sex                 0\n",
      "capital-gain        0\n",
      "capital-loss        0\n",
      "hours-per-week      0\n",
      "native-country      0\n",
      "salary              0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4903 entries, 0 to 4999\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             4903 non-null   int64 \n",
      " 1   workclass       4580 non-null   object\n",
      " 2   fnlwgt          4903 non-null   int64 \n",
      " 3   education       4903 non-null   object\n",
      " 4   education-num   4903 non-null   int64 \n",
      " 5   marital-status  4903 non-null   object\n",
      " 6   occupation      4580 non-null   object\n",
      " 7   relationship    4903 non-null   object\n",
      " 8   race            4903 non-null   object\n",
      " 9   sex             4903 non-null   object\n",
      " 10  capital-gain    4903 non-null   int64 \n",
      " 11  capital-loss    4903 non-null   int64 \n",
      " 12  hours-per-week  4903 non-null   int64 \n",
      " 13  native-country  4903 non-null   object\n",
      " 14  salary          4903 non-null   object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 612.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.isna().sum())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d35738c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Exec-managerial      609\n",
       " Prof-specialty       606\n",
       " Craft-repair         603\n",
       " Sales                578\n",
       " Adm-clerical         567\n",
       " Other-service        484\n",
       " Machine-op-inspct    308\n",
       " Transport-moving     242\n",
       " Handlers-cleaners    195\n",
       " Farming-fishing      141\n",
       " Tech-support         139\n",
       " Protective-serv       89\n",
       " Priv-house-serv       17\n",
       " Armed-Forces           2\n",
       "Name: occupation, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"occupation\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66158e31",
   "metadata": {},
   "source": [
    "Wartości nullowe w kolumnie occupation wypełnimy wartością domyślną Other-service, a dla wartości nullowych kolumny workclass wprowadzimy nową wartość Other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d9ac145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Other-service        807\n",
      " Exec-managerial      609\n",
      " Prof-specialty       606\n",
      " Craft-repair         603\n",
      " Sales                578\n",
      " Adm-clerical         567\n",
      " Machine-op-inspct    308\n",
      " Transport-moving     242\n",
      " Handlers-cleaners    195\n",
      " Farming-fishing      141\n",
      " Tech-support         139\n",
      " Protective-serv       89\n",
      " Priv-house-serv       17\n",
      " Armed-Forces           2\n",
      "Name: occupation, dtype: int64\n",
      " Private             3369\n",
      " Self-emp-not-inc     378\n",
      " Local-gov            324\n",
      " Other                323\n",
      " State-gov            189\n",
      " Self-emp-inc         175\n",
      " Federal-gov          144\n",
      " Without-pay            1\n",
      "Name: workclass, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data[\"occupation\"].fillna(value=\" Other-service\", inplace=True)\n",
    "data[\"workclass\"].fillna(value=\" Other\", inplace=True)\n",
    "\n",
    "print(data[\"occupation\"].value_counts())\n",
    "print(data[\"workclass\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76421e6e",
   "metadata": {},
   "source": [
    "### Zamienimy kluczy na binarne wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba57a27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  salary  \n",
       "0          2174             0              40   United-States       1  \n",
       "1             0             0              13   United-States       1  \n",
       "2             0             0              40   United-States       1  \n",
       "3             0             0              40   United-States       1  \n",
       "4             0             0              40            Cuba       1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.replace([\">50K\", \"<=50K\"], [0, 1], regex = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5004d1",
   "metadata": {},
   "source": [
    "### Zrobimy to samo dla innej binarnej kolumny ''sex\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "184ae83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race  sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White    1   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White    1   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White    1   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black    1   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black    0   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  salary  \n",
       "0          2174             0              40   United-States       1  \n",
       "1             0             0              13   United-States       1  \n",
       "2             0             0              40   United-States       1  \n",
       "3             0             0              40   United-States       1  \n",
       "4             0             0              40            Cuba       1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.replace([\"Female\", \"Male\"], [0, 1], regex = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad94f84",
   "metadata": {},
   "source": [
    "Podzielimy dane na kolumny kategoryczne, numeryczne i binarne\n",
    "\n",
    "Kolumny numeryczne : age, fnlwgt, education-num, capital-gain, capital-loss, hours-per-week \\\n",
    "Kolumny kategoryczne : workclass, education, marital-status, occupation, relationship, race, native-country \\\n",
    "Kolumny binarne : sex, salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c422ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_fields = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "categorical_fields = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"native-country\"]\n",
    "binary_fields = [\"sex\", \"salary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98d868",
   "metadata": {},
   "source": [
    "## Przyjżyjmy się dokładniej danym numerycznym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6210fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4903.000000</td>\n",
       "      <td>4.903000e+03</td>\n",
       "      <td>4903.000000</td>\n",
       "      <td>4903.000000</td>\n",
       "      <td>4903.000000</td>\n",
       "      <td>4903.000000</td>\n",
       "      <td>4903.000000</td>\n",
       "      <td>4903.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.574954</td>\n",
       "      <td>1.909784e+05</td>\n",
       "      <td>10.071181</td>\n",
       "      <td>0.672038</td>\n",
       "      <td>1012.826025</td>\n",
       "      <td>92.925352</td>\n",
       "      <td>40.511931</td>\n",
       "      <td>0.757291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.614604</td>\n",
       "      <td>1.069148e+05</td>\n",
       "      <td>2.521639</td>\n",
       "      <td>0.469519</td>\n",
       "      <td>6963.589486</td>\n",
       "      <td>409.241640</td>\n",
       "      <td>12.103281</td>\n",
       "      <td>0.428764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.930200e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.174990e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.795570e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>2.419565e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.033222e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>2547.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age        fnlwgt  education-num          sex  capital-gain  \\\n",
       "count  4903.000000  4.903000e+03    4903.000000  4903.000000   4903.000000   \n",
       "mean     38.574954  1.909784e+05      10.071181     0.672038   1012.826025   \n",
       "std      13.614604  1.069148e+05       2.521639     0.469519   6963.589486   \n",
       "min      17.000000  1.930200e+04       1.000000     0.000000      0.000000   \n",
       "25%      28.000000  1.174990e+05       9.000000     0.000000      0.000000   \n",
       "50%      37.000000  1.795570e+05      10.000000     1.000000      0.000000   \n",
       "75%      47.000000  2.419565e+05      12.000000     1.000000      0.000000   \n",
       "max      90.000000  1.033222e+06      16.000000     1.000000  99999.000000   \n",
       "\n",
       "       capital-loss  hours-per-week       salary  \n",
       "count   4903.000000     4903.000000  4903.000000  \n",
       "mean      92.925352       40.511931     0.757291  \n",
       "std      409.241640       12.103281     0.428764  \n",
       "min        0.000000        1.000000     0.000000  \n",
       "25%        0.000000       40.000000     1.000000  \n",
       "50%        0.000000       40.000000     1.000000  \n",
       "75%        0.000000       45.000000     1.000000  \n",
       "max     2547.000000       99.000000     1.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea161e",
   "metadata": {},
   "source": [
    "Najprawdopodobniej będziemy ignorować kolumny capital-gain i kapital-loss z powodu tego, że prawie zawsze mają wartośc 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf628967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       4655\n",
      "1902      38\n",
      "1977      30\n",
      "1887      23\n",
      "1485       9\n",
      "1848       9\n",
      "2415       9\n",
      "1741       7\n",
      "1762       6\n",
      "1564       6\n",
      "1719       6\n",
      "1876       6\n",
      "1590       6\n",
      "1672       6\n",
      "1740       5\n",
      "1669       5\n",
      "1980       5\n",
      "1628       5\n",
      "2002       4\n",
      "2392       4\n",
      "2339       4\n",
      "1408       4\n",
      "2258       3\n",
      "1340       3\n",
      "1579       3\n",
      "2179       3\n",
      "2001       3\n",
      "625        2\n",
      "2051       2\n",
      "1816       2\n",
      "1721       2\n",
      "1504       2\n",
      "2352       2\n",
      "1380       2\n",
      "2377       2\n",
      "2206       2\n",
      "1651       1\n",
      "1138       1\n",
      "1092       1\n",
      "1668       1\n",
      "880        1\n",
      "1539       1\n",
      "213        1\n",
      "2238       1\n",
      "2205       1\n",
      "2444       1\n",
      "1726       1\n",
      "2174       1\n",
      "2547       1\n",
      "419        1\n",
      "1602       1\n",
      "653        1\n",
      "2042       1\n",
      "1594       1\n",
      "Name: capital-loss, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"capital-loss\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c5473eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        4495\n",
      "15024      64\n",
      "7688       46\n",
      "7298       35\n",
      "99999      21\n",
      "         ... \n",
      "3818        1\n",
      "401         1\n",
      "1111        1\n",
      "1471        1\n",
      "2346        1\n",
      "Name: capital-gain, Length: 81, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"capital-gain\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd21b8",
   "metadata": {},
   "source": [
    "### Przyjrzyjmy się pozostałym kolumnam numerycznym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c13eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123983    5\n",
      "94235     4\n",
      "155343    4\n",
      "163003    4\n",
      "111567    4\n",
      "         ..\n",
      "166744    1\n",
      "238768    1\n",
      "176270    1\n",
      "140592    1\n",
      "54947     1\n",
      "Name: fnlwgt, Length: 4523, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"fnlwgt\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5464b5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "plt.hist(data['fnlwgt'], bins=40, color=\"red\", label='fnlwgt')\n",
    "plt.xlabel('fnlwgt')\n",
    "plt.ylabel('Number of rows')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45763a",
   "metadata": {},
   "source": [
    "#### popatrzmy jak jest rozpodzielony wiek w naszy datasecie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c994174c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "plt.hist(data['age'], bins=73, color=\"blue\", label='Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Number of rows')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c28a67be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "plt.hist(data['education-num'], bins=10, color=\"orange\", label='education-num')\n",
    "plt.xlabel('education-num')\n",
    "plt.ylabel('Number of rows')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac89c80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "plt.hist(data['hours-per-week'], bins=20, color=\"blue\", label='hours-per-week')\n",
    "plt.xlabel('hours-per-week')\n",
    "plt.ylabel('Number of rows')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "199803ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='salary', ylabel='count'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "sns.countplot(data[\"salary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a0cff",
   "metadata": {},
   "source": [
    "Widzimy, że klasa negatywna i pozytywna są przedstawione w datasecie w różnych proporscjach, więc musimy pamiętać, aby koniecznie używać StratifiedKFold()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1a3c8",
   "metadata": {},
   "source": [
    "Resztę pozostawiamy bez zmian "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d2af47",
   "metadata": {},
   "source": [
    "## Przyjrzyjmy się kolumnam kategorycznym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4a1b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60785f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Private             3369\n",
      " Self-emp-not-inc     378\n",
      " Local-gov            324\n",
      " Other                323\n",
      " State-gov            189\n",
      " Self-emp-inc         175\n",
      " Federal-gov          144\n",
      " Without-pay            1\n",
      "Name: workclass, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"workclass\"].value_counts())\n",
    "# le.fit(data[\"workclass\"])\n",
    "# data[\"workclass\"] = le.transform(data[\"workclass\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75e421c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HS-grad         1595\n",
      " Some-college    1106\n",
      " Bachelors        809\n",
      " Masters          246\n",
      " Assoc-voc        215\n",
      " 11th             198\n",
      " Assoc-acdm       161\n",
      " 10th             142\n",
      " 7th-8th           96\n",
      " Prof-school       85\n",
      " 9th               76\n",
      " 12th              53\n",
      " Doctorate         52\n",
      " 5th-6th           42\n",
      " 1st-4th           20\n",
      " Preschool          7\n",
      "Name: education, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"education\"].value_counts())\n",
    "# le.fit(data[\"education\"])\n",
    "# data[\"education\"] = le.transform(data[\"education\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "183ca2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Married-civ-spouse       2219\n",
      " Never-married            1629\n",
      " Divorced                  677\n",
      " Widowed                   159\n",
      " Separated                 153\n",
      " Married-spouse-absent      61\n",
      " Married-AF-spouse           5\n",
      "Name: marital-status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"marital-status\"].value_counts())\n",
    "# le.fit(data[\"marital-status\"])\n",
    "# data[\"marital-status\"] = le.transform(data[\"marital-status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca2ed69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Other-service        807\n",
      " Exec-managerial      609\n",
      " Prof-specialty       606\n",
      " Craft-repair         603\n",
      " Sales                578\n",
      " Adm-clerical         567\n",
      " Machine-op-inspct    308\n",
      " Transport-moving     242\n",
      " Handlers-cleaners    195\n",
      " Farming-fishing      141\n",
      " Tech-support         139\n",
      " Protective-serv       89\n",
      " Priv-house-serv       17\n",
      " Armed-Forces           2\n",
      "Name: occupation, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"occupation\"].value_counts())\n",
    "# le.fit(data[\"occupation\"])\n",
    "# data[\"occupation\"] = le.transform(data[\"occupation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37b13d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Husband           1954\n",
      " Not-in-family     1264\n",
      " Own-child          760\n",
      " Unmarried          537\n",
      " Wife               241\n",
      " Other-relative     147\n",
      "Name: relationship, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"relationship\"].value_counts())\n",
    "# le.fit(data[\"relationship\"])\n",
    "# data[\"relationship\"] = le.transform(data[\"relationship\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc9fd676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " White                 4185\n",
      " Black                  502\n",
      " Asian-Pac-Islander     139\n",
      " Amer-Indian-Eskimo      49\n",
      " Other                   28\n",
      "Name: race, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"race\"].value_counts())\n",
    "# le.fit(data[\"race\"])\n",
    "# data[\"race\"] = le.transform(data[\"race\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a276b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " United-States                 4465\n",
      " Mexico                         104\n",
      " Canada                          28\n",
      " Philippines                     22\n",
      " Germany                         22\n",
      " El-Salvador                     16\n",
      " Puerto-Rico                     16\n",
      " England                         16\n",
      " Jamaica                         15\n",
      " China                           15\n",
      " Cuba                            14\n",
      " Dominican-Republic              13\n",
      " Italy                           12\n",
      " India                           12\n",
      " Iran                            11\n",
      " Vietnam                         10\n",
      " Taiwan                          10\n",
      " Guatemala                       10\n",
      " Poland                           9\n",
      " South                            9\n",
      " Japan                            9\n",
      " Portugal                         7\n",
      " Greece                           6\n",
      " Haiti                            6\n",
      " Columbia                         5\n",
      " Nicaragua                        5\n",
      " Trinadad&Tobago                  4\n",
      " Ireland                          4\n",
      " Ecuador                          4\n",
      " Cambodia                         4\n",
      " France                           3\n",
      " Peru                             3\n",
      " Outlying-US(Guam-USVI-etc)       3\n",
      " Honduras                         3\n",
      " Yugoslavia                       2\n",
      " Thailand                         2\n",
      " Laos                             2\n",
      " Scotland                         1\n",
      " Hong                             1\n",
      "Name: native-country, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"native-country\"].value_counts())\n",
    "# le.fit(data[\"native-country\"])\n",
    "# data[\"native-country\"] = le.transform(data[\"native-country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5450aaff",
   "metadata": {},
   "source": [
    "Będziemy brać pod uwagę wszystkie kolumny kategoryczne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c61b0996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>43</td>\n",
       "      <td>Private</td>\n",
       "      <td>222971</td>\n",
       "      <td>5th-6th</td>\n",
       "      <td>3</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>259425</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>47</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>212120</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>26</td>\n",
       "      <td>Private</td>\n",
       "      <td>245880</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>58</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>54947</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4903 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age          workclass  fnlwgt      education  education-num  \\\n",
       "0      39          State-gov   77516      Bachelors             13   \n",
       "1      50   Self-emp-not-inc   83311      Bachelors             13   \n",
       "2      38            Private  215646        HS-grad              9   \n",
       "3      53            Private  234721           11th              7   \n",
       "4      28            Private  338409      Bachelors             13   \n",
       "...   ...                ...     ...            ...            ...   \n",
       "4995   43            Private  222971        5th-6th              3   \n",
       "4996   31            Private  259425        HS-grad              9   \n",
       "4997   47       Self-emp-inc  212120        HS-grad              9   \n",
       "4998   26            Private  245880        HS-grad              9   \n",
       "4999   58          Local-gov   54947   Some-college             10   \n",
       "\n",
       "           marital-status          occupation    relationship    race  sex  \\\n",
       "0           Never-married        Adm-clerical   Not-in-family   White    1   \n",
       "1      Married-civ-spouse     Exec-managerial         Husband   White    1   \n",
       "2                Divorced   Handlers-cleaners   Not-in-family   White    1   \n",
       "3      Married-civ-spouse   Handlers-cleaners         Husband   Black    1   \n",
       "4      Married-civ-spouse      Prof-specialty            Wife   Black    0   \n",
       "...                   ...                 ...             ...     ...  ...   \n",
       "4995        Never-married   Machine-op-inspct       Unmarried   White    0   \n",
       "4996   Married-civ-spouse        Craft-repair         Husband   White    1   \n",
       "4997   Married-civ-spouse        Craft-repair         Husband   White    1   \n",
       "4998        Never-married        Adm-clerical   Not-in-family   White    1   \n",
       "4999        Never-married      Prof-specialty   Not-in-family   White    0   \n",
       "\n",
       "      capital-gain  capital-loss  hours-per-week  native-country  salary  \n",
       "0             2174             0              40   United-States       1  \n",
       "1                0             0              13   United-States       1  \n",
       "2                0             0              40   United-States       1  \n",
       "3                0             0              40   United-States       1  \n",
       "4                0             0              40            Cuba       1  \n",
       "...            ...           ...             ...             ...     ...  \n",
       "4995             0             0              40          Mexico       1  \n",
       "4996             0             0              40   United-States       0  \n",
       "4997             0             0              40   United-States       0  \n",
       "4998             0             0              60   United-States       1  \n",
       "4999             0             0              55   United-States       1  \n",
       "\n",
       "[4903 rows x 15 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffde781",
   "metadata": {},
   "source": [
    "### Stworzymy DataFrame Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f345d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced98f5e",
   "metadata": {},
   "source": [
    "Zbudujmy pipeline dla atrybutów numerycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26f912f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numerical_fields = [\"age\", \"fnlwgt\", \"education-num\", \"hours-per-week\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        (\"select_numeric\", DataFrameSelector(numerical_fields)),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b57e85",
   "metadata": {},
   "source": [
    "Imputer dla kategorycznych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cf315f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n",
    "                                        index=X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.most_frequent_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc034f9b",
   "metadata": {},
   "source": [
    "Zbudujmy pipeline dla atrybutów kategorycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50e262ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "catbin = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"native-country\", \"sex\"]\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        (\"select_cat\", DataFrameSelector(catbin)),\n",
    "        (\"imputer\", MostFrequentImputer()),\n",
    "        (\"cat_encoder\", OneHotEncoder(sparse=False, handle_unknown = 'ignore')),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e756056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "preprocess_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7de22b",
   "metadata": {},
   "source": [
    "## Podzielimy zbiór danych na X i y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c93a1926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>43</td>\n",
       "      <td>Private</td>\n",
       "      <td>222971</td>\n",
       "      <td>5th-6th</td>\n",
       "      <td>3</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>259425</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>47</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>212120</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>26</td>\n",
       "      <td>Private</td>\n",
       "      <td>245880</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>58</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>54947</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4903 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age          workclass  fnlwgt      education  education-num  \\\n",
       "0      39          State-gov   77516      Bachelors             13   \n",
       "1      50   Self-emp-not-inc   83311      Bachelors             13   \n",
       "2      38            Private  215646        HS-grad              9   \n",
       "3      53            Private  234721           11th              7   \n",
       "4      28            Private  338409      Bachelors             13   \n",
       "...   ...                ...     ...            ...            ...   \n",
       "4995   43            Private  222971        5th-6th              3   \n",
       "4996   31            Private  259425        HS-grad              9   \n",
       "4997   47       Self-emp-inc  212120        HS-grad              9   \n",
       "4998   26            Private  245880        HS-grad              9   \n",
       "4999   58          Local-gov   54947   Some-college             10   \n",
       "\n",
       "           marital-status          occupation    relationship    race  sex  \\\n",
       "0           Never-married        Adm-clerical   Not-in-family   White    1   \n",
       "1      Married-civ-spouse     Exec-managerial         Husband   White    1   \n",
       "2                Divorced   Handlers-cleaners   Not-in-family   White    1   \n",
       "3      Married-civ-spouse   Handlers-cleaners         Husband   Black    1   \n",
       "4      Married-civ-spouse      Prof-specialty            Wife   Black    0   \n",
       "...                   ...                 ...             ...     ...  ...   \n",
       "4995        Never-married   Machine-op-inspct       Unmarried   White    0   \n",
       "4996   Married-civ-spouse        Craft-repair         Husband   White    1   \n",
       "4997   Married-civ-spouse        Craft-repair         Husband   White    1   \n",
       "4998        Never-married        Adm-clerical   Not-in-family   White    1   \n",
       "4999        Never-married      Prof-specialty   Not-in-family   White    0   \n",
       "\n",
       "      capital-gain  capital-loss  hours-per-week  native-country  \n",
       "0             2174             0              40   United-States  \n",
       "1                0             0              13   United-States  \n",
       "2                0             0              40   United-States  \n",
       "3                0             0              40   United-States  \n",
       "4                0             0              40            Cuba  \n",
       "...            ...           ...             ...             ...  \n",
       "4995             0             0              40          Mexico  \n",
       "4996             0             0              40   United-States  \n",
       "4997             0             0              40   United-States  \n",
       "4998             0             0              60   United-States  \n",
       "4999             0             0              55   United-States  \n",
       "\n",
       "[4903 rows x 14 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop(['salary'], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78aff974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['salary'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a0be84",
   "metadata": {},
   "source": [
    "## Podzielimy zbiór danych na train i test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "049a9ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da172503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.80000e+01, 3.55259e+05, 1.40000e+01, 4.00000e+01],\n",
       "       [4.20000e+01, 3.44920e+05, 1.00000e+01, 5.00000e+01],\n",
       "       [6.20000e+01, 1.59841e+05, 9.00000e+00, 2.40000e+01],\n",
       "       ...,\n",
       "       [2.70000e+01, 1.63127e+05, 1.10000e+01, 3.50000e+01],\n",
       "       [2.00000e+01, 1.70038e+05, 9.00000e+00, 4.00000e+01],\n",
       "       [4.50000e+01, 1.33969e+05, 9.00000e+00, 5.00000e+01]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd9fa38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d512c0",
   "metadata": {},
   "source": [
    "# Płytkie uczenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bff8127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "seed=123\n",
    "kfold = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2000a1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['memory', 'steps', 'verbose', 'preprocessing', 'classifier', 'preprocessing__n_jobs', 'preprocessing__transformer_list', 'preprocessing__transformer_weights', 'preprocessing__verbose', 'preprocessing__num_pipeline', 'preprocessing__cat_pipeline', 'preprocessing__num_pipeline__memory', 'preprocessing__num_pipeline__steps', 'preprocessing__num_pipeline__verbose', 'preprocessing__num_pipeline__select_numeric', 'preprocessing__num_pipeline__imputer', 'preprocessing__num_pipeline__select_numeric__attribute_names', 'preprocessing__num_pipeline__imputer__add_indicator', 'preprocessing__num_pipeline__imputer__copy', 'preprocessing__num_pipeline__imputer__fill_value', 'preprocessing__num_pipeline__imputer__missing_values', 'preprocessing__num_pipeline__imputer__strategy', 'preprocessing__num_pipeline__imputer__verbose', 'preprocessing__cat_pipeline__memory', 'preprocessing__cat_pipeline__steps', 'preprocessing__cat_pipeline__verbose', 'preprocessing__cat_pipeline__select_cat', 'preprocessing__cat_pipeline__imputer', 'preprocessing__cat_pipeline__cat_encoder', 'preprocessing__cat_pipeline__select_cat__attribute_names', 'preprocessing__cat_pipeline__cat_encoder__categories', 'preprocessing__cat_pipeline__cat_encoder__drop', 'preprocessing__cat_pipeline__cat_encoder__dtype', 'preprocessing__cat_pipeline__cat_encoder__handle_unknown', 'preprocessing__cat_pipeline__cat_encoder__sparse', 'classifier__bootstrap', 'classifier__ccp_alpha', 'classifier__class_weight', 'classifier__criterion', 'classifier__max_depth', 'classifier__max_features', 'classifier__max_leaf_nodes', 'classifier__max_samples', 'classifier__min_impurity_decrease', 'classifier__min_impurity_split', 'classifier__min_samples_leaf', 'classifier__min_samples_split', 'classifier__min_weight_fraction_leaf', 'classifier__n_estimators', 'classifier__n_jobs', 'classifier__oob_score', 'classifier__random_state', 'classifier__verbose', 'classifier__warm_start'])\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "[CV 1/5; 1/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 1/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.803 total time=   0.3s\n",
      "[CV 2/5; 1/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 1/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.821 total time=   0.3s\n",
      "[CV 3/5; 1/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 1/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.792 total time=   0.3s\n",
      "[CV 4/5; 1/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 1/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.808 total time=   0.3s\n",
      "[CV 5/5; 1/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 1/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.819 total time=   0.3s\n",
      "[CV 1/5; 2/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 2/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.798 total time=   1.0s\n",
      "[CV 2/5; 2/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 2/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.825 total time=   1.0s\n",
      "[CV 3/5; 2/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 2/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.790 total time=   2.2s\n",
      "[CV 4/5; 2/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 2/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.806 total time=   1.0s\n",
      "[CV 5/5; 2/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 2/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.822 total time=   0.9s\n",
      "[CV 1/5; 3/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 3/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.796 total time=   0.3s\n",
      "[CV 2/5; 3/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 3/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.827 total time=   0.3s\n",
      "[CV 3/5; 3/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 3/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.787 total time=   0.3s\n",
      "[CV 4/5; 3/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 3/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.803 total time=   0.6s\n",
      "[CV 5/5; 3/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 3/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.824 total time=   0.8s\n",
      "[CV 1/5; 4/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 4/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.798 total time=   1.4s\n",
      "[CV 2/5; 4/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 4/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.817 total time=   0.9s\n",
      "[CV 3/5; 4/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 4/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.790 total time=   0.9s\n",
      "[CV 4/5; 4/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 4/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.805 total time=   1.1s\n",
      "[CV 5/5; 4/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 4/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.821 total time=   1.9s\n",
      "[CV 1/5; 5/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 5/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.769 total time=   0.3s\n",
      "[CV 2/5; 5/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.777 total time=   0.3s\n",
      "[CV 3/5; 5/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 5/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.774 total time=   0.3s\n",
      "[CV 4/5; 5/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 5/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.773 total time=   0.3s\n",
      "[CV 5/5; 5/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 5/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.787 total time=   0.3s\n",
      "[CV 1/5; 6/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 6/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.767 total time=   0.8s\n",
      "[CV 2/5; 6/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 6/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.774 total time=   1.5s\n",
      "[CV 3/5; 6/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 6/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.776 total time=   1.4s\n",
      "[CV 4/5; 6/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 6/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.771 total time=   0.8s\n",
      "[CV 5/5; 6/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 6/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.789 total time=   0.8s\n",
      "[CV 1/5; 7/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 7/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.808 total time=   0.3s\n",
      "[CV 2/5; 7/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 7/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.825 total time=   0.3s\n",
      "[CV 3/5; 7/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 7/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.805 total time=   0.5s\n",
      "[CV 4/5; 7/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 7/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.809 total time=   0.9s\n",
      "[CV 5/5; 7/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 7/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.825 total time=   0.8s\n",
      "[CV 1/5; 8/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 8/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.803 total time=   1.0s\n",
      "[CV 2/5; 8/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 8/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.831 total time=   1.0s\n",
      "[CV 3/5; 8/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 8/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.802 total time=   1.0s\n",
      "[CV 4/5; 8/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 8/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.808 total time=   2.2s\n",
      "[CV 5/5; 8/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 8/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.825 total time=   1.1s\n",
      "[CV 1/5; 9/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 9/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.801 total time=   0.4s\n",
      "[CV 2/5; 9/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 9/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.830 total time=   0.4s\n",
      "[CV 3/5; 9/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 9/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.806 total time=   0.7s\n",
      "[CV 4/5; 9/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 9/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.810 total time=   0.9s\n",
      "[CV 5/5; 9/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 9/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.827 total time=   0.9s\n",
      "[CV 1/5; 10/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 10/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.803 total time=   1.4s\n",
      "[CV 2/5; 10/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 10/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.827 total time=   1.0s\n",
      "[CV 3/5; 10/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 10/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.803 total time=   1.0s\n",
      "[CV 4/5; 10/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 10/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.810 total time=   1.6s\n",
      "[CV 5/5; 10/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 10/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.824 total time=   1.6s\n",
      "[CV 1/5; 11/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 11/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.801 total time=   0.3s\n",
      "[CV 2/5; 11/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 11/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.799 total time=   0.3s\n",
      "[CV 3/5; 11/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 11/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.786 total time=   0.3s\n",
      "[CV 4/5; 11/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 11/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.781 total time=   0.3s\n",
      "[CV 5/5; 11/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 11/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.809 total time=   0.3s\n",
      "[CV 1/5; 12/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 12/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.789 total time=   1.0s\n",
      "[CV 2/5; 12/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 12/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.798 total time=   2.0s\n",
      "[CV 3/5; 12/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 12/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.787 total time=   1.5s\n",
      "[CV 4/5; 12/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 12/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.800 total time=   0.9s\n",
      "[CV 5/5; 12/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 12/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.818 total time=   0.9s\n",
      "[CV 1/5; 13/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 13/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.811 total time=   0.4s\n",
      "[CV 2/5; 13/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 13/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.833 total time=   0.4s\n",
      "[CV 3/5; 13/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 13/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.813 total time=   0.9s\n",
      "[CV 4/5; 13/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 13/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.816 total time=   0.9s\n",
      "[CV 5/5; 13/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 13/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.824 total time=   0.9s\n",
      "[CV 1/5; 14/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 14/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.812 total time=   1.3s\n",
      "[CV 2/5; 14/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 14/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.836 total time=   1.0s\n",
      "[CV 3/5; 14/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 14/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.816 total time=   1.0s\n",
      "[CV 4/5; 14/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 14/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.818 total time=   2.1s\n",
      "[CV 5/5; 14/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 14/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.824 total time=   1.8s\n",
      "[CV 1/5; 15/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 15/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.815 total time=   0.4s\n",
      "[CV 2/5; 15/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 15/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.836 total time=   0.4s\n",
      "[CV 3/5; 15/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 15/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.810 total time=   0.4s\n",
      "[CV 4/5; 15/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 15/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.816 total time=   0.6s\n",
      "[CV 5/5; 15/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 15/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.827 total time=   0.9s\n",
      "[CV 1/5; 16/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 16/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.814 total time=   2.2s\n",
      "[CV 2/5; 16/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 16/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.837 total time=   1.0s\n",
      "[CV 3/5; 16/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 16/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.815 total time=   1.1s\n",
      "[CV 4/5; 16/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 16/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.818 total time=   1.6s\n",
      "[CV 5/5; 16/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 16/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.825 total time=   2.4s\n",
      "[CV 1/5; 17/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 17/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.801 total time=   0.4s\n",
      "[CV 2/5; 17/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 17/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.817 total time=   0.3s\n",
      "[CV 3/5; 17/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 17/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.793 total time=   0.3s\n",
      "[CV 4/5; 17/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 17/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.806 total time=   0.3s\n",
      "[CV 5/5; 17/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 17/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.816 total time=   0.3s\n",
      "[CV 1/5; 18/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 18/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.802 total time=   0.9s\n",
      "[CV 2/5; 18/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 18/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.827 total time=   1.8s\n",
      "[CV 3/5; 18/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 18/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.789 total time=   1.4s\n",
      "[CV 4/5; 18/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 18/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.806 total time=   0.9s\n",
      "[CV 5/5; 18/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 18/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.818 total time=   0.9s\n",
      "[CV 1/5; 19/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 19/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.818 total time=   0.4s\n",
      "[CV 2/5; 19/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 19/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.837 total time=   0.8s\n",
      "[CV 3/5; 19/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 19/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.821 total time=   1.0s\n",
      "[CV 4/5; 19/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 19/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.821 total time=   1.0s\n",
      "[CV 5/5; 19/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 19/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.825 total time=   0.7s\n",
      "[CV 1/5; 20/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 20/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.822 total time=   1.1s\n",
      "[CV 2/5; 20/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 20/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.838 total time=   1.5s\n",
      "[CV 3/5; 20/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 20/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.819 total time=   2.5s\n",
      "[CV 4/5; 20/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 20/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.819 total time=   1.3s\n",
      "[CV 5/5; 20/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 20/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.825 total time=   1.5s\n",
      "[CV 1/5; 21/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 21/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.817 total time=   1.2s\n",
      "[CV 2/5; 21/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 21/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.840 total time=   1.1s\n",
      "[CV 3/5; 21/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 21/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.816 total time=   0.6s\n",
      "[CV 4/5; 21/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 21/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.822 total time=   0.4s\n",
      "[CV 5/5; 21/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 21/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.829 total time=   0.4s\n",
      "[CV 1/5; 22/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 22/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.821 total time=   1.3s\n",
      "[CV 2/5; 22/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 22/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.836 total time=   2.5s\n",
      "[CV 3/5; 22/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 22/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.818 total time=   1.3s\n",
      "[CV 4/5; 22/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 22/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.824 total time=   1.1s\n",
      "[CV 5/5; 22/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 22/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.825 total time=   2.3s\n",
      "[CV 1/5; 23/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 23/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.805 total time=   0.4s\n",
      "[CV 2/5; 23/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 23/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.827 total time=   0.3s\n",
      "[CV 3/5; 23/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 23/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.796 total time=   0.3s\n",
      "[CV 4/5; 23/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 23/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.809 total time=   0.3s\n",
      "[CV 5/5; 23/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 23/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.825 total time=   0.5s\n",
      "[CV 1/5; 24/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 24/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.811 total time=   2.3s\n",
      "[CV 2/5; 24/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 24/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.828 total time=   1.4s\n",
      "[CV 3/5; 24/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 24/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.802 total time=   0.9s\n",
      "[CV 4/5; 24/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 24/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.810 total time=   1.6s\n",
      "[CV 5/5; 24/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 24/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.824 total time=   2.2s\n",
      "[CV 1/5; 25/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 25/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.825 total time=   0.4s\n",
      "[CV 2/5; 25/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 25/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.837 total time=   0.4s\n",
      "[CV 3/5; 25/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 25/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.822 total time=   0.4s\n",
      "[CV 4/5; 25/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 25/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.825 total time=   0.4s\n",
      "[CV 5/5; 25/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 25/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.829 total time=   0.9s\n",
      "[CV 1/5; 26/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 26/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.817 total time=   2.5s\n",
      "[CV 2/5; 26/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 26/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.837 total time=   1.4s\n",
      "[CV 3/5; 26/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 26/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.813 total time=   2.2s\n",
      "[CV 4/5; 26/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 26/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.828 total time=   2.2s\n",
      "[CV 5/5; 26/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 26/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.835 total time=   1.1s\n",
      "[CV 1/5; 27/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 27/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.822 total time=   0.4s\n",
      "[CV 2/5; 27/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 27/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.833 total time=   0.9s\n",
      "[CV 3/5; 27/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 27/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.818 total time=   1.0s\n",
      "[CV 4/5; 27/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 27/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.825 total time=   1.0s\n",
      "[CV 5/5; 27/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 27/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.838 total time=   0.6s\n",
      "[CV 1/5; 28/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 28/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.827 total time=   1.1s\n",
      "[CV 2/5; 28/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 28/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.834 total time=   1.9s\n",
      "[CV 3/5; 28/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 28/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.818 total time=   2.3s\n",
      "[CV 4/5; 28/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 28/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.825 total time=   1.2s\n",
      "[CV 5/5; 28/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 28/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.832 total time=   2.0s\n",
      "[CV 1/5; 29/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 29/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.817 total time=   0.9s\n",
      "[CV 2/5; 29/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 29/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.836 total time=   0.9s\n",
      "[CV 3/5; 29/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 29/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.805 total time=   0.4s\n",
      "[CV 4/5; 29/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 29/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.810 total time=   0.4s\n",
      "[CV 5/5; 29/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 29/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.821 total time=   0.4s\n",
      "[CV 1/5; 30/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 30/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.814 total time=   1.3s\n",
      "[CV 2/5; 30/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 30/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.833 total time=   2.3s\n",
      "[CV 3/5; 30/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 30/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.810 total time=   1.3s\n",
      "[CV 4/5; 30/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 30/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.815 total time=   1.0s\n",
      "[CV 5/5; 30/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 30/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.828 total time=   1.9s\n",
      "[CV 1/5; 31/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 31/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.790 total time=   1.1s\n",
      "[CV 2/5; 31/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 31/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.820 total time=   0.8s\n",
      "[CV 3/5; 31/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 31/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.786 total time=   0.4s\n",
      "[CV 4/5; 31/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 31/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.806 total time=   0.4s\n",
      "[CV 5/5; 31/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 31/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.822 total time=   0.3s\n",
      "[CV 1/5; 32/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 32/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.792 total time=   1.7s\n",
      "[CV 2/5; 32/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 32/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.814 total time=   2.1s\n",
      "[CV 3/5; 32/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 32/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.793 total time=   0.9s\n",
      "[CV 4/5; 32/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 32/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.806 total time=   0.9s\n",
      "[CV 5/5; 32/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 32/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.821 total time=   1.9s\n",
      "[CV 1/5; 33/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 33/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.803 total time=   0.8s\n",
      "[CV 2/5; 33/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 33/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.827 total time=   0.7s\n",
      "[CV 3/5; 33/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 33/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.789 total time=   0.3s\n",
      "[CV 4/5; 33/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 33/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.806 total time=   0.3s\n",
      "[CV 5/5; 33/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 33/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.813 total time=   0.3s\n",
      "[CV 1/5; 34/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 34/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.792 total time=   1.1s\n",
      "[CV 2/5; 34/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 34/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.812 total time=   2.1s\n",
      "[CV 3/5; 34/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 34/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.789 total time=   1.0s\n",
      "[CV 4/5; 34/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 34/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.808 total time=   1.0s\n",
      "[CV 5/5; 34/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 34/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.827 total time=   2.1s\n",
      "[CV 1/5; 35/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 35/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.770 total time=   0.7s\n",
      "[CV 2/5; 35/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 35/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.764 total time=   0.5s\n",
      "[CV 3/5; 35/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 35/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.777 total time=   0.3s\n",
      "[CV 4/5; 35/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 35/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.764 total time=   0.3s\n",
      "[CV 5/5; 35/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 35/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.771 total time=   0.3s\n",
      "[CV 1/5; 36/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 36/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.767 total time=   0.9s\n",
      "[CV 2/5; 36/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 36/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.764 total time=   1.9s\n",
      "[CV 3/5; 36/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 36/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.773 total time=   1.5s\n",
      "[CV 4/5; 36/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 36/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.773 total time=   0.8s\n",
      "[CV 5/5; 36/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 36/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.773 total time=   0.8s\n",
      "[CV 1/5; 37/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 37/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.805 total time=   0.7s\n",
      "[CV 2/5; 37/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 37/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.833 total time=   1.0s\n",
      "[CV 3/5; 37/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 37/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.794 total time=   1.0s\n",
      "[CV 4/5; 37/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 37/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.808 total time=   0.6s\n",
      "[CV 5/5; 37/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 37/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.822 total time=   0.4s\n",
      "[CV 1/5; 38/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 38/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.805 total time=   1.0s\n",
      "[CV 2/5; 38/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 38/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.831 total time=   1.8s\n",
      "[CV 3/5; 38/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 38/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.806 total time=   2.1s\n",
      "[CV 4/5; 38/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 38/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.808 total time=   1.0s\n",
      "[CV 5/5; 38/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 38/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.824 total time=   1.1s\n",
      "[CV 1/5; 39/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 39/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.799 total time=   0.9s\n",
      "[CV 2/5; 39/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 39/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.828 total time=   0.9s\n",
      "[CV 3/5; 39/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 39/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.800 total time=   0.9s\n",
      "[CV 4/5; 39/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 39/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.808 total time=   0.5s\n",
      "[CV 5/5; 39/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 39/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.824 total time=   0.4s\n",
      "[CV 1/5; 40/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 40/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.801 total time=   1.0s\n",
      "[CV 2/5; 40/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 40/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.834 total time=   2.1s\n",
      "[CV 3/5; 40/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 40/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.796 total time=   1.9s\n",
      "[CV 4/5; 40/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 40/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.808 total time=   1.0s\n",
      "[CV 5/5; 40/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 40/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.824 total time=   1.4s\n",
      "[CV 1/5; 41/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 41/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.786 total time=   0.9s\n",
      "[CV 2/5; 41/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 41/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.795 total time=   0.8s\n",
      "[CV 3/5; 41/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 41/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.786 total time=   0.8s\n",
      "[CV 4/5; 41/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 41/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.802 total time=   0.3s\n",
      "[CV 5/5; 41/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 41/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.809 total time=   0.3s\n",
      "[CV 1/5; 42/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 42/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.789 total time=   0.9s\n",
      "[CV 2/5; 42/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 42/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.801 total time=   1.5s\n",
      "[CV 3/5; 42/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 42/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.792 total time=   2.0s\n",
      "[CV 4/5; 42/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 42/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.796 total time=   0.9s\n",
      "[CV 5/5; 42/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 42/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.799 total time=   0.9s\n",
      "[CV 1/5; 43/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 43/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.815 total time=   0.8s\n",
      "[CV 2/5; 43/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 43/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.830 total time=   1.1s\n",
      "[CV 3/5; 43/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 43/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.812 total time=   1.1s\n",
      "[CV 4/5; 43/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 43/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.815 total time=   0.7s\n",
      "[CV 5/5; 43/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 43/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.825 total time=   0.4s\n",
      "[CV 1/5; 44/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 44/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.811 total time=   1.1s\n",
      "[CV 2/5; 44/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 44/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.828 total time=   2.2s\n",
      "[CV 3/5; 44/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 44/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.810 total time=   1.8s\n",
      "[CV 4/5; 44/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 44/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.818 total time=   1.1s\n",
      "[CV 5/5; 44/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 44/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.825 total time=   1.8s\n",
      "[CV 1/5; 45/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 45/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.814 total time=   0.9s\n",
      "[CV 2/5; 45/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 45/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.831 total time=   0.9s\n",
      "[CV 3/5; 45/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 45/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.809 total time=   0.4s\n",
      "[CV 4/5; 45/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 45/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.810 total time=   0.4s\n",
      "[CV 5/5; 45/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 45/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.825 total time=   0.4s\n",
      "[CV 1/5; 46/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 46/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.811 total time=   1.6s\n",
      "[CV 2/5; 46/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 46/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.834 total time=   2.4s\n",
      "[CV 3/5; 46/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 46/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.816 total time=   1.1s\n",
      "[CV 4/5; 46/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 46/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.819 total time=   1.3s\n",
      "[CV 5/5; 46/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 46/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.827 total time=   2.4s\n",
      "[CV 1/5; 47/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 47/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.799 total time=   0.6s\n",
      "[CV 2/5; 47/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 47/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.830 total time=   0.3s\n",
      "[CV 3/5; 47/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 47/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.794 total time=   0.3s\n",
      "[CV 4/5; 47/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 47/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.808 total time=   0.3s\n",
      "[CV 5/5; 47/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 47/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.819 total time=   0.3s\n",
      "[CV 1/5; 48/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 48/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.802 total time=   1.7s\n",
      "[CV 2/5; 48/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 48/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.817 total time=   2.1s\n",
      "[CV 3/5; 48/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 48/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.789 total time=   1.0s\n",
      "[CV 4/5; 48/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 48/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.800 total time=   0.9s\n",
      "[CV 5/5; 48/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 48/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.822 total time=   2.2s\n",
      "[CV 1/5; 49/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 49/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.815 total time=   1.0s\n",
      "[CV 2/5; 49/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 49/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.836 total time=   0.4s\n",
      "[CV 3/5; 49/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 49/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.818 total time=   0.4s\n",
      "[CV 4/5; 49/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 49/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.824 total time=   0.4s\n",
      "[CV 5/5; 49/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 49/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.821 total time=   0.4s\n",
      "[CV 1/5; 50/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 50/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.817 total time=   2.3s\n",
      "[CV 2/5; 50/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 50/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.836 total time=   1.8s\n",
      "[CV 3/5; 50/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 50/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.818 total time=   1.2s\n",
      "[CV 4/5; 50/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 50/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.819 total time=   2.5s\n",
      "[CV 5/5; 50/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 50/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.828 total time=   1.8s\n",
      "[CV 1/5; 51/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 51/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.820 total time=   0.4s\n",
      "[CV 2/5; 51/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 51/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.836 total time=   0.4s\n",
      "[CV 3/5; 51/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 51/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.813 total time=   0.5s\n",
      "[CV 4/5; 51/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 51/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.822 total time=   1.0s\n",
      "[CV 5/5; 51/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 51/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.825 total time=   1.0s\n",
      "[CV 1/5; 52/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 52/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.818 total time=   1.7s\n",
      "[CV 2/5; 52/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 52/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.838 total time=   1.2s\n",
      "[CV 3/5; 52/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 52/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.822 total time=   2.3s\n",
      "[CV 4/5; 52/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 52/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.818 total time=   2.1s\n",
      "[CV 5/5; 52/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 52/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.827 total time=   1.5s\n",
      "[CV 1/5; 53/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 53/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.812 total time=   1.1s\n",
      "[CV 2/5; 53/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 53/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.834 total time=   1.1s\n",
      "[CV 3/5; 53/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 53/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.800 total time=   0.9s\n",
      "[CV 4/5; 53/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 53/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.812 total time=   0.5s\n",
      "[CV 5/5; 53/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 53/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.825 total time=   0.4s\n",
      "[CV 1/5; 54/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 54/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.809 total time=   1.3s\n",
      "[CV 2/5; 54/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 54/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.830 total time=   2.4s\n",
      "[CV 3/5; 54/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 54/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.799 total time=   1.2s\n",
      "[CV 4/5; 54/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 54/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.809 total time=   1.0s\n",
      "[CV 5/5; 54/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 54/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.822 total time=   1.9s\n",
      "[CV 1/5; 55/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 55/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.824 total time=   1.0s\n",
      "[CV 2/5; 55/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 55/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.838 total time=   0.7s\n",
      "[CV 3/5; 55/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 55/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.815 total time=   0.5s\n",
      "[CV 4/5; 55/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 55/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.824 total time=   0.5s\n",
      "[CV 5/5; 55/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 55/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.825 total time=   0.4s\n",
      "[CV 1/5; 56/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 56/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.822 total time=   2.4s\n",
      "[CV 2/5; 56/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 56/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.837 total time=   1.9s\n",
      "[CV 3/5; 56/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 56/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.816 total time=   1.2s\n",
      "[CV 4/5; 56/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 56/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.825 total time=   2.5s\n",
      "[CV 5/5; 56/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 56/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.831 total time=   1.9s\n",
      "[CV 1/5; 57/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 57/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.822 total time=   0.5s\n",
      "[CV 2/5; 57/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 57/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.838 total time=   0.5s\n",
      "[CV 3/5; 57/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 57/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.816 total time=   1.0s\n",
      "[CV 4/5; 57/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 57/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.825 total time=   1.1s\n",
      "[CV 5/5; 57/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 57/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.831 total time=   1.1s\n",
      "[CV 1/5; 58/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 58/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.822 total time=   1.3s\n",
      "[CV 2/5; 58/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 58/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.836 total time=   1.7s\n",
      "[CV 3/5; 58/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 58/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.816 total time=   2.6s\n",
      "[CV 4/5; 58/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 58/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.821 total time=   1.2s\n",
      "[CV 5/5; 58/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 58/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.828 total time=   1.7s\n",
      "[CV 1/5; 59/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 59/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.815 total time=   0.9s\n",
      "[CV 2/5; 59/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 59/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.831 total time=   0.9s\n",
      "[CV 3/5; 59/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 59/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.805 total time=   0.6s\n",
      "[CV 4/5; 59/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 59/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.812 total time=   0.4s\n",
      "[CV 5/5; 59/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 59/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.824 total time=   0.4s\n",
      "[CV 1/5; 60/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 60/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.815 total time=   1.2s\n",
      "[CV 2/5; 60/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 60/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.831 total time=   2.4s\n",
      "[CV 3/5; 60/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 60/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.808 total time=   1.5s\n",
      "[CV 4/5; 60/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 60/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.816 total time=   1.0s\n",
      "[CV 5/5; 60/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 60/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.829 total time=   2.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=Pipeline(steps=[('preprocessing',\n",
       "                                        FeatureUnion(transformer_list=[('num_pipeline',\n",
       "                                                                        Pipeline(steps=[('select_numeric',\n",
       "                                                                                         DataFrameSelector(attribute_names=['age',\n",
       "                                                                                                                            'fnlwgt',\n",
       "                                                                                                                            'education-num',\n",
       "                                                                                                                            'hours-per-week'])),\n",
       "                                                                                        ('imputer',\n",
       "                                                                                         SimpleImputer(strategy='median'))])),\n",
       "                                                                       ('cat_pipeline',...\n",
       "                                                                                                                            'native-country',\n",
       "                                                                                                                            'sex'])),\n",
       "                                                                                        ('imputer',\n",
       "                                                                                         MostFrequentImputer()),\n",
       "                                                                                        ('cat_encoder',\n",
       "                                                                                         OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                                       sparse=False))]))])),\n",
       "                                       ('classifier',\n",
       "                                        RandomForestClassifier())]),\n",
       "             param_grid={'classifier__criterion': ['gini', 'entropy'],\n",
       "                         'classifier__max_depth': [4, 5, 6, 7, 8],\n",
       "                         'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'classifier__n_estimators': [200, 500]},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline),\n",
    "    ('classifier', RandomForestClassifier())])\n",
    "\n",
    "param_grid = { \n",
    "    'classifier__n_estimators': [200, 500],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'classifier__max_depth': [4,5,6,7,8],\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "forest = GridSearchCV(pipe, param_grid, cv=kfold, verbose=10)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c47ae60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV 1/5; 1/36] START classifier__C=0.001, classifier__gamma=0.001...............\n",
      "[CV 1/5; 1/36] END classifier__C=0.001, classifier__gamma=0.001;, score=0.757 total time=   0.5s\n",
      "[CV 2/5; 1/36] START classifier__C=0.001, classifier__gamma=0.001...............\n",
      "[CV 2/5; 1/36] END classifier__C=0.001, classifier__gamma=0.001;, score=0.757 total time=   0.5s\n",
      "[CV 3/5; 1/36] START classifier__C=0.001, classifier__gamma=0.001...............\n",
      "[CV 3/5; 1/36] END classifier__C=0.001, classifier__gamma=0.001;, score=0.758 total time=   0.5s\n",
      "[CV 4/5; 1/36] START classifier__C=0.001, classifier__gamma=0.001...............\n",
      "[CV 4/5; 1/36] END classifier__C=0.001, classifier__gamma=0.001;, score=0.758 total time=   0.5s\n",
      "[CV 5/5; 1/36] START classifier__C=0.001, classifier__gamma=0.001...............\n",
      "[CV 5/5; 1/36] END classifier__C=0.001, classifier__gamma=0.001;, score=0.758 total time=   0.7s\n",
      "[CV 1/5; 2/36] START classifier__C=0.001, classifier__gamma=0.01................\n",
      "[CV 1/5; 2/36] END classifier__C=0.001, classifier__gamma=0.01;, score=0.757 total time=   1.6s\n",
      "[CV 2/5; 2/36] START classifier__C=0.001, classifier__gamma=0.01................\n",
      "[CV 2/5; 2/36] END classifier__C=0.001, classifier__gamma=0.01;, score=0.757 total time=   0.8s\n",
      "[CV 3/5; 2/36] START classifier__C=0.001, classifier__gamma=0.01................\n",
      "[CV 3/5; 2/36] END classifier__C=0.001, classifier__gamma=0.01;, score=0.758 total time=   0.6s\n",
      "[CV 4/5; 2/36] START classifier__C=0.001, classifier__gamma=0.01................\n",
      "[CV 4/5; 2/36] END classifier__C=0.001, classifier__gamma=0.01;, score=0.758 total time=   0.6s\n",
      "[CV 5/5; 2/36] START classifier__C=0.001, classifier__gamma=0.01................\n",
      "[CV 5/5; 2/36] END classifier__C=0.001, classifier__gamma=0.01;, score=0.758 total time=   0.7s\n",
      "[CV 1/5; 3/36] START classifier__C=0.001, classifier__gamma=0.1.................\n",
      "[CV 1/5; 3/36] END classifier__C=0.001, classifier__gamma=0.1;, score=0.757 total time=   1.3s\n",
      "[CV 2/5; 3/36] START classifier__C=0.001, classifier__gamma=0.1.................\n",
      "[CV 2/5; 3/36] END classifier__C=0.001, classifier__gamma=0.1;, score=0.757 total time=   1.6s\n",
      "[CV 3/5; 3/36] START classifier__C=0.001, classifier__gamma=0.1.................\n",
      "[CV 3/5; 3/36] END classifier__C=0.001, classifier__gamma=0.1;, score=0.758 total time=   0.8s\n",
      "[CV 4/5; 3/36] START classifier__C=0.001, classifier__gamma=0.1.................\n",
      "[CV 4/5; 3/36] END classifier__C=0.001, classifier__gamma=0.1;, score=0.758 total time=   0.8s\n",
      "[CV 5/5; 3/36] START classifier__C=0.001, classifier__gamma=0.1.................\n",
      "[CV 5/5; 3/36] END classifier__C=0.001, classifier__gamma=0.1;, score=0.758 total time=   0.8s\n",
      "[CV 1/5; 4/36] START classifier__C=0.001, classifier__gamma=1...................\n",
      "[CV 1/5; 4/36] END classifier__C=0.001, classifier__gamma=1;, score=0.757 total time=   1.3s\n",
      "[CV 2/5; 4/36] START classifier__C=0.001, classifier__gamma=1...................\n",
      "[CV 2/5; 4/36] END classifier__C=0.001, classifier__gamma=1;, score=0.757 total time=   1.5s\n",
      "[CV 3/5; 4/36] START classifier__C=0.001, classifier__gamma=1...................\n",
      "[CV 3/5; 4/36] END classifier__C=0.001, classifier__gamma=1;, score=0.758 total time=   0.7s\n",
      "[CV 4/5; 4/36] START classifier__C=0.001, classifier__gamma=1...................\n",
      "[CV 4/5; 4/36] END classifier__C=0.001, classifier__gamma=1;, score=0.758 total time=   0.7s\n",
      "[CV 5/5; 4/36] START classifier__C=0.001, classifier__gamma=1...................\n",
      "[CV 5/5; 4/36] END classifier__C=0.001, classifier__gamma=1;, score=0.758 total time=   0.8s\n",
      "[CV 1/5; 5/36] START classifier__C=0.001, classifier__gamma=10..................\n",
      "[CV 1/5; 5/36] END classifier__C=0.001, classifier__gamma=10;, score=0.757 total time=   1.1s\n",
      "[CV 2/5; 5/36] START classifier__C=0.001, classifier__gamma=10..................\n",
      "[CV 2/5; 5/36] END classifier__C=0.001, classifier__gamma=10;, score=0.757 total time=   1.7s\n",
      "[CV 3/5; 5/36] START classifier__C=0.001, classifier__gamma=10..................\n",
      "[CV 3/5; 5/36] END classifier__C=0.001, classifier__gamma=10;, score=0.758 total time=   0.8s\n",
      "[CV 4/5; 5/36] START classifier__C=0.001, classifier__gamma=10..................\n",
      "[CV 4/5; 5/36] END classifier__C=0.001, classifier__gamma=10;, score=0.758 total time=   0.8s\n",
      "[CV 5/5; 5/36] START classifier__C=0.001, classifier__gamma=10..................\n",
      "[CV 5/5; 5/36] END classifier__C=0.001, classifier__gamma=10;, score=0.758 total time=   0.8s\n",
      "[CV 1/5; 6/36] START classifier__C=0.001, classifier__gamma=100.................\n",
      "[CV 1/5; 6/36] END classifier__C=0.001, classifier__gamma=100;, score=0.757 total time=   1.2s\n",
      "[CV 2/5; 6/36] START classifier__C=0.001, classifier__gamma=100.................\n",
      "[CV 2/5; 6/36] END classifier__C=0.001, classifier__gamma=100;, score=0.757 total time=   1.7s\n",
      "[CV 3/5; 6/36] START classifier__C=0.001, classifier__gamma=100.................\n",
      "[CV 3/5; 6/36] END classifier__C=0.001, classifier__gamma=100;, score=0.758 total time=   0.8s\n",
      "[CV 4/5; 6/36] START classifier__C=0.001, classifier__gamma=100.................\n",
      "[CV 4/5; 6/36] END classifier__C=0.001, classifier__gamma=100;, score=0.758 total time=   0.8s\n",
      "[CV 5/5; 6/36] START classifier__C=0.001, classifier__gamma=100.................\n",
      "[CV 5/5; 6/36] END classifier__C=0.001, classifier__gamma=100;, score=0.758 total time=   1.5s\n",
      "[CV 1/5; 7/36] START classifier__C=0.01, classifier__gamma=0.001................\n",
      "[CV 1/5; 7/36] END classifier__C=0.01, classifier__gamma=0.001;, score=0.757 total time=   2.2s\n",
      "[CV 2/5; 7/36] START classifier__C=0.01, classifier__gamma=0.001................\n",
      "[CV 2/5; 7/36] END classifier__C=0.01, classifier__gamma=0.001;, score=0.757 total time=   1.0s\n",
      "[CV 3/5; 7/36] START classifier__C=0.01, classifier__gamma=0.001................\n",
      "[CV 3/5; 7/36] END classifier__C=0.01, classifier__gamma=0.001;, score=0.758 total time=   1.1s\n",
      "[CV 4/5; 7/36] START classifier__C=0.01, classifier__gamma=0.001................\n",
      "[CV 4/5; 7/36] END classifier__C=0.01, classifier__gamma=0.001;, score=0.758 total time=   2.3s\n",
      "[CV 5/5; 7/36] START classifier__C=0.01, classifier__gamma=0.001................\n",
      "[CV 5/5; 7/36] END classifier__C=0.01, classifier__gamma=0.001;, score=0.758 total time=   1.5s\n",
      "[CV 1/5; 8/36] START classifier__C=0.01, classifier__gamma=0.01.................\n",
      "[CV 1/5; 8/36] END classifier__C=0.01, classifier__gamma=0.01;, score=0.757 total time=   1.1s\n",
      "[CV 2/5; 8/36] START classifier__C=0.01, classifier__gamma=0.01.................\n",
      "[CV 2/5; 8/36] END classifier__C=0.01, classifier__gamma=0.01;, score=0.757 total time=   2.1s\n",
      "[CV 3/5; 8/36] START classifier__C=0.01, classifier__gamma=0.01.................\n",
      "[CV 3/5; 8/36] END classifier__C=0.01, classifier__gamma=0.01;, score=0.758 total time=   1.3s\n",
      "[CV 4/5; 8/36] START classifier__C=0.01, classifier__gamma=0.01.................\n",
      "[CV 4/5; 8/36] END classifier__C=0.01, classifier__gamma=0.01;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 8/36] START classifier__C=0.01, classifier__gamma=0.01.................\n",
      "[CV 5/5; 8/36] END classifier__C=0.01, classifier__gamma=0.01;, score=0.758 total time=   2.2s\n",
      "[CV 1/5; 9/36] START classifier__C=0.01, classifier__gamma=0.1..................\n",
      "[CV 1/5; 9/36] END classifier__C=0.01, classifier__gamma=0.1;, score=0.757 total time=   1.7s\n",
      "[CV 2/5; 9/36] START classifier__C=0.01, classifier__gamma=0.1..................\n",
      "[CV 2/5; 9/36] END classifier__C=0.01, classifier__gamma=0.1;, score=0.757 total time=   1.1s\n",
      "[CV 3/5; 9/36] START classifier__C=0.01, classifier__gamma=0.1..................\n",
      "[CV 3/5; 9/36] END classifier__C=0.01, classifier__gamma=0.1;, score=0.758 total time=   1.8s\n",
      "[CV 4/5; 9/36] START classifier__C=0.01, classifier__gamma=0.1..................\n",
      "[CV 4/5; 9/36] END classifier__C=0.01, classifier__gamma=0.1;, score=0.758 total time=   2.1s\n",
      "[CV 5/5; 9/36] START classifier__C=0.01, classifier__gamma=0.1..................\n",
      "[CV 5/5; 9/36] END classifier__C=0.01, classifier__gamma=0.1;, score=0.758 total time=   1.1s\n",
      "[CV 1/5; 10/36] START classifier__C=0.01, classifier__gamma=1...................\n",
      "[CV 1/5; 10/36] END classifier__C=0.01, classifier__gamma=1;, score=0.757 total time=   1.4s\n",
      "[CV 2/5; 10/36] START classifier__C=0.01, classifier__gamma=1...................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 10/36] END classifier__C=0.01, classifier__gamma=1;, score=0.757 total time=   2.6s\n",
      "[CV 3/5; 10/36] START classifier__C=0.01, classifier__gamma=1...................\n",
      "[CV 3/5; 10/36] END classifier__C=0.01, classifier__gamma=1;, score=0.758 total time=   1.1s\n",
      "[CV 4/5; 10/36] START classifier__C=0.01, classifier__gamma=1...................\n",
      "[CV 4/5; 10/36] END classifier__C=0.01, classifier__gamma=1;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 10/36] START classifier__C=0.01, classifier__gamma=1...................\n",
      "[CV 5/5; 10/36] END classifier__C=0.01, classifier__gamma=1;, score=0.758 total time=   2.4s\n",
      "[CV 1/5; 11/36] START classifier__C=0.01, classifier__gamma=10..................\n",
      "[CV 1/5; 11/36] END classifier__C=0.01, classifier__gamma=10;, score=0.757 total time=   1.6s\n",
      "[CV 2/5; 11/36] START classifier__C=0.01, classifier__gamma=10..................\n",
      "[CV 2/5; 11/36] END classifier__C=0.01, classifier__gamma=10;, score=0.757 total time=   1.1s\n",
      "[CV 3/5; 11/36] START classifier__C=0.01, classifier__gamma=10..................\n",
      "[CV 3/5; 11/36] END classifier__C=0.01, classifier__gamma=10;, score=0.758 total time=   2.0s\n",
      "[CV 4/5; 11/36] START classifier__C=0.01, classifier__gamma=10..................\n",
      "[CV 4/5; 11/36] END classifier__C=0.01, classifier__gamma=10;, score=0.758 total time=   1.4s\n",
      "[CV 5/5; 11/36] START classifier__C=0.01, classifier__gamma=10..................\n",
      "[CV 5/5; 11/36] END classifier__C=0.01, classifier__gamma=10;, score=0.758 total time=   1.1s\n",
      "[CV 1/5; 12/36] START classifier__C=0.01, classifier__gamma=100.................\n",
      "[CV 1/5; 12/36] END classifier__C=0.01, classifier__gamma=100;, score=0.757 total time=   2.2s\n",
      "[CV 2/5; 12/36] START classifier__C=0.01, classifier__gamma=100.................\n",
      "[CV 2/5; 12/36] END classifier__C=0.01, classifier__gamma=100;, score=0.757 total time=   1.8s\n",
      "[CV 3/5; 12/36] START classifier__C=0.01, classifier__gamma=100.................\n",
      "[CV 3/5; 12/36] END classifier__C=0.01, classifier__gamma=100;, score=0.758 total time=   1.0s\n",
      "[CV 4/5; 12/36] START classifier__C=0.01, classifier__gamma=100.................\n",
      "[CV 4/5; 12/36] END classifier__C=0.01, classifier__gamma=100;, score=0.758 total time=   1.7s\n",
      "[CV 5/5; 12/36] START classifier__C=0.01, classifier__gamma=100.................\n",
      "[CV 5/5; 12/36] END classifier__C=0.01, classifier__gamma=100;, score=0.758 total time=   2.2s\n",
      "[CV 1/5; 13/36] START classifier__C=0.1, classifier__gamma=0.001................\n",
      "[CV 1/5; 13/36] END classifier__C=0.1, classifier__gamma=0.001;, score=0.757 total time=   1.0s\n",
      "[CV 2/5; 13/36] START classifier__C=0.1, classifier__gamma=0.001................\n",
      "[CV 2/5; 13/36] END classifier__C=0.1, classifier__gamma=0.001;, score=0.757 total time=   1.3s\n",
      "[CV 3/5; 13/36] START classifier__C=0.1, classifier__gamma=0.001................\n",
      "[CV 3/5; 13/36] END classifier__C=0.1, classifier__gamma=0.001;, score=0.758 total time=   2.4s\n",
      "[CV 4/5; 13/36] START classifier__C=0.1, classifier__gamma=0.001................\n",
      "[CV 4/5; 13/36] END classifier__C=0.1, classifier__gamma=0.001;, score=0.758 total time=   1.5s\n",
      "[CV 5/5; 13/36] START classifier__C=0.1, classifier__gamma=0.001................\n",
      "[CV 5/5; 13/36] END classifier__C=0.1, classifier__gamma=0.001;, score=0.758 total time=   1.2s\n",
      "[CV 1/5; 14/36] START classifier__C=0.1, classifier__gamma=0.01.................\n",
      "[CV 1/5; 14/36] END classifier__C=0.1, classifier__gamma=0.01;, score=0.757 total time=   2.8s\n",
      "[CV 2/5; 14/36] START classifier__C=0.1, classifier__gamma=0.01.................\n",
      "[CV 2/5; 14/36] END classifier__C=0.1, classifier__gamma=0.01;, score=0.757 total time=   1.3s\n",
      "[CV 3/5; 14/36] START classifier__C=0.1, classifier__gamma=0.01.................\n",
      "[CV 3/5; 14/36] END classifier__C=0.1, classifier__gamma=0.01;, score=0.758 total time=   1.1s\n",
      "[CV 4/5; 14/36] START classifier__C=0.1, classifier__gamma=0.01.................\n",
      "[CV 4/5; 14/36] END classifier__C=0.1, classifier__gamma=0.01;, score=0.758 total time=   2.3s\n",
      "[CV 5/5; 14/36] START classifier__C=0.1, classifier__gamma=0.01.................\n",
      "[CV 5/5; 14/36] END classifier__C=0.1, classifier__gamma=0.01;, score=0.758 total time=   1.7s\n",
      "[CV 1/5; 15/36] START classifier__C=0.1, classifier__gamma=0.1..................\n",
      "[CV 1/5; 15/36] END classifier__C=0.1, classifier__gamma=0.1;, score=0.757 total time=   1.1s\n",
      "[CV 2/5; 15/36] START classifier__C=0.1, classifier__gamma=0.1..................\n",
      "[CV 2/5; 15/36] END classifier__C=0.1, classifier__gamma=0.1;, score=0.757 total time=   2.0s\n",
      "[CV 3/5; 15/36] START classifier__C=0.1, classifier__gamma=0.1..................\n",
      "[CV 3/5; 15/36] END classifier__C=0.1, classifier__gamma=0.1;, score=0.758 total time=   2.0s\n",
      "[CV 4/5; 15/36] START classifier__C=0.1, classifier__gamma=0.1..................\n",
      "[CV 4/5; 15/36] END classifier__C=0.1, classifier__gamma=0.1;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 15/36] START classifier__C=0.1, classifier__gamma=0.1..................\n",
      "[CV 5/5; 15/36] END classifier__C=0.1, classifier__gamma=0.1;, score=0.758 total time=   1.8s\n",
      "[CV 1/5; 16/36] START classifier__C=0.1, classifier__gamma=1....................\n",
      "[CV 1/5; 16/36] END classifier__C=0.1, classifier__gamma=1;, score=0.757 total time=   2.3s\n",
      "[CV 2/5; 16/36] START classifier__C=0.1, classifier__gamma=1....................\n",
      "[CV 2/5; 16/36] END classifier__C=0.1, classifier__gamma=1;, score=0.757 total time=   1.1s\n",
      "[CV 3/5; 16/36] START classifier__C=0.1, classifier__gamma=1....................\n",
      "[CV 3/5; 16/36] END classifier__C=0.1, classifier__gamma=1;, score=0.758 total time=   1.4s\n",
      "[CV 4/5; 16/36] START classifier__C=0.1, classifier__gamma=1....................\n",
      "[CV 4/5; 16/36] END classifier__C=0.1, classifier__gamma=1;, score=0.758 total time=   2.4s\n",
      "[CV 5/5; 16/36] START classifier__C=0.1, classifier__gamma=1....................\n",
      "[CV 5/5; 16/36] END classifier__C=0.1, classifier__gamma=1;, score=0.758 total time=   1.2s\n",
      "[CV 1/5; 17/36] START classifier__C=0.1, classifier__gamma=10...................\n",
      "[CV 1/5; 17/36] END classifier__C=0.1, classifier__gamma=10;, score=0.757 total time=   1.1s\n",
      "[CV 2/5; 17/36] START classifier__C=0.1, classifier__gamma=10...................\n",
      "[CV 2/5; 17/36] END classifier__C=0.1, classifier__gamma=10;, score=0.757 total time=   2.5s\n",
      "[CV 3/5; 17/36] START classifier__C=0.1, classifier__gamma=10...................\n",
      "[CV 3/5; 17/36] END classifier__C=0.1, classifier__gamma=10;, score=0.758 total time=   1.6s\n",
      "[CV 4/5; 17/36] START classifier__C=0.1, classifier__gamma=10...................\n",
      "[CV 4/5; 17/36] END classifier__C=0.1, classifier__gamma=10;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 17/36] START classifier__C=0.1, classifier__gamma=10...................\n",
      "[CV 5/5; 17/36] END classifier__C=0.1, classifier__gamma=10;, score=0.758 total time=   2.2s\n",
      "[CV 1/5; 18/36] START classifier__C=0.1, classifier__gamma=100..................\n",
      "[CV 1/5; 18/36] END classifier__C=0.1, classifier__gamma=100;, score=0.757 total time=   1.8s\n",
      "[CV 2/5; 18/36] START classifier__C=0.1, classifier__gamma=100..................\n",
      "[CV 2/5; 18/36] END classifier__C=0.1, classifier__gamma=100;, score=0.757 total time=   1.1s\n",
      "[CV 3/5; 18/36] START classifier__C=0.1, classifier__gamma=100..................\n",
      "[CV 3/5; 18/36] END classifier__C=0.1, classifier__gamma=100;, score=0.758 total time=   1.8s\n",
      "[CV 4/5; 18/36] START classifier__C=0.1, classifier__gamma=100..................\n",
      "[CV 4/5; 18/36] END classifier__C=0.1, classifier__gamma=100;, score=0.758 total time=   2.2s\n",
      "[CV 5/5; 18/36] START classifier__C=0.1, classifier__gamma=100..................\n",
      "[CV 5/5; 18/36] END classifier__C=0.1, classifier__gamma=100;, score=0.758 total time=   1.1s\n",
      "[CV 1/5; 19/36] START classifier__C=1, classifier__gamma=0.001..................\n",
      "[CV 1/5; 19/36] END classifier__C=1, classifier__gamma=0.001;, score=0.750 total time=   1.4s\n",
      "[CV 2/5; 19/36] START classifier__C=1, classifier__gamma=0.001..................\n",
      "[CV 2/5; 19/36] END classifier__C=1, classifier__gamma=0.001;, score=0.751 total time=   2.6s\n",
      "[CV 3/5; 19/36] START classifier__C=1, classifier__gamma=0.001..................\n",
      "[CV 3/5; 19/36] END classifier__C=1, classifier__gamma=0.001;, score=0.757 total time=   1.3s\n",
      "[CV 4/5; 19/36] START classifier__C=1, classifier__gamma=0.001..................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 19/36] END classifier__C=1, classifier__gamma=0.001;, score=0.758 total time=   1.3s\n",
      "[CV 5/5; 19/36] START classifier__C=1, classifier__gamma=0.001..................\n",
      "[CV 5/5; 19/36] END classifier__C=1, classifier__gamma=0.001;, score=0.761 total time=   2.5s\n",
      "[CV 1/5; 20/36] START classifier__C=1, classifier__gamma=0.01...................\n",
      "[CV 1/5; 20/36] END classifier__C=1, classifier__gamma=0.01;, score=0.760 total time=   1.3s\n",
      "[CV 2/5; 20/36] START classifier__C=1, classifier__gamma=0.01...................\n",
      "[CV 2/5; 20/36] END classifier__C=1, classifier__gamma=0.01;, score=0.761 total time=   1.1s\n",
      "[CV 3/5; 20/36] START classifier__C=1, classifier__gamma=0.01...................\n",
      "[CV 3/5; 20/36] END classifier__C=1, classifier__gamma=0.01;, score=0.762 total time=   2.3s\n",
      "[CV 4/5; 20/36] START classifier__C=1, classifier__gamma=0.01...................\n",
      "[CV 4/5; 20/36] END classifier__C=1, classifier__gamma=0.01;, score=0.759 total time=   1.7s\n",
      "[CV 5/5; 20/36] START classifier__C=1, classifier__gamma=0.01...................\n",
      "[CV 5/5; 20/36] END classifier__C=1, classifier__gamma=0.01;, score=0.757 total time=   1.1s\n",
      "[CV 1/5; 21/36] START classifier__C=1, classifier__gamma=0.1....................\n",
      "[CV 1/5; 21/36] END classifier__C=1, classifier__gamma=0.1;, score=0.757 total time=   2.5s\n",
      "[CV 2/5; 21/36] START classifier__C=1, classifier__gamma=0.1....................\n",
      "[CV 2/5; 21/36] END classifier__C=1, classifier__gamma=0.1;, score=0.757 total time=   2.0s\n",
      "[CV 3/5; 21/36] START classifier__C=1, classifier__gamma=0.1....................\n",
      "[CV 3/5; 21/36] END classifier__C=1, classifier__gamma=0.1;, score=0.758 total time=   1.2s\n",
      "[CV 4/5; 21/36] START classifier__C=1, classifier__gamma=0.1....................\n",
      "[CV 4/5; 21/36] END classifier__C=1, classifier__gamma=0.1;, score=0.758 total time=   2.5s\n",
      "[CV 5/5; 21/36] START classifier__C=1, classifier__gamma=0.1....................\n",
      "[CV 5/5; 21/36] END classifier__C=1, classifier__gamma=0.1;, score=0.758 total time=   1.7s\n",
      "[CV 1/5; 22/36] START classifier__C=1, classifier__gamma=1......................\n",
      "[CV 1/5; 22/36] END classifier__C=1, classifier__gamma=1;, score=0.757 total time=   1.2s\n",
      "[CV 2/5; 22/36] START classifier__C=1, classifier__gamma=1......................\n",
      "[CV 2/5; 22/36] END classifier__C=1, classifier__gamma=1;, score=0.757 total time=   2.4s\n",
      "[CV 3/5; 22/36] START classifier__C=1, classifier__gamma=1......................\n",
      "[CV 3/5; 22/36] END classifier__C=1, classifier__gamma=1;, score=0.758 total time=   1.8s\n",
      "[CV 4/5; 22/36] START classifier__C=1, classifier__gamma=1......................\n",
      "[CV 4/5; 22/36] END classifier__C=1, classifier__gamma=1;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 22/36] START classifier__C=1, classifier__gamma=1......................\n",
      "[CV 5/5; 22/36] END classifier__C=1, classifier__gamma=1;, score=0.758 total time=   2.5s\n",
      "[CV 1/5; 23/36] START classifier__C=1, classifier__gamma=10.....................\n",
      "[CV 1/5; 23/36] END classifier__C=1, classifier__gamma=10;, score=0.757 total time=   1.8s\n",
      "[CV 2/5; 23/36] START classifier__C=1, classifier__gamma=10.....................\n",
      "[CV 2/5; 23/36] END classifier__C=1, classifier__gamma=10;, score=0.757 total time=   1.2s\n",
      "[CV 3/5; 23/36] START classifier__C=1, classifier__gamma=10.....................\n",
      "[CV 3/5; 23/36] END classifier__C=1, classifier__gamma=10;, score=0.758 total time=   2.3s\n",
      "[CV 4/5; 23/36] START classifier__C=1, classifier__gamma=10.....................\n",
      "[CV 4/5; 23/36] END classifier__C=1, classifier__gamma=10;, score=0.758 total time=   1.8s\n",
      "[CV 5/5; 23/36] START classifier__C=1, classifier__gamma=10.....................\n",
      "[CV 5/5; 23/36] END classifier__C=1, classifier__gamma=10;, score=0.758 total time=   1.1s\n",
      "[CV 1/5; 24/36] START classifier__C=1, classifier__gamma=100....................\n",
      "[CV 1/5; 24/36] END classifier__C=1, classifier__gamma=100;, score=0.757 total time=   2.2s\n",
      "[CV 2/5; 24/36] START classifier__C=1, classifier__gamma=100....................\n",
      "[CV 2/5; 24/36] END classifier__C=1, classifier__gamma=100;, score=0.757 total time=   2.0s\n",
      "[CV 3/5; 24/36] START classifier__C=1, classifier__gamma=100....................\n",
      "[CV 3/5; 24/36] END classifier__C=1, classifier__gamma=100;, score=0.758 total time=   1.2s\n",
      "[CV 4/5; 24/36] START classifier__C=1, classifier__gamma=100....................\n",
      "[CV 4/5; 24/36] END classifier__C=1, classifier__gamma=100;, score=0.758 total time=   2.2s\n",
      "[CV 5/5; 24/36] START classifier__C=1, classifier__gamma=100....................\n",
      "[CV 5/5; 24/36] END classifier__C=1, classifier__gamma=100;, score=0.758 total time=   2.0s\n",
      "[CV 1/5; 25/36] START classifier__C=10, classifier__gamma=0.001.................\n",
      "[CV 1/5; 25/36] END classifier__C=10, classifier__gamma=0.001;, score=0.737 total time=   1.1s\n",
      "[CV 2/5; 25/36] START classifier__C=10, classifier__gamma=0.001.................\n",
      "[CV 2/5; 25/36] END classifier__C=10, classifier__gamma=0.001;, score=0.707 total time=   2.0s\n",
      "[CV 3/5; 25/36] START classifier__C=10, classifier__gamma=0.001.................\n",
      "[CV 3/5; 25/36] END classifier__C=10, classifier__gamma=0.001;, score=0.732 total time=   2.1s\n",
      "[CV 4/5; 25/36] START classifier__C=10, classifier__gamma=0.001.................\n",
      "[CV 4/5; 25/36] END classifier__C=10, classifier__gamma=0.001;, score=0.735 total time=   1.1s\n",
      "[CV 5/5; 25/36] START classifier__C=10, classifier__gamma=0.001.................\n",
      "[CV 5/5; 25/36] END classifier__C=10, classifier__gamma=0.001;, score=0.746 total time=   1.9s\n",
      "[CV 1/5; 26/36] START classifier__C=10, classifier__gamma=0.01..................\n",
      "[CV 1/5; 26/36] END classifier__C=10, classifier__gamma=0.01;, score=0.764 total time=   2.3s\n",
      "[CV 2/5; 26/36] START classifier__C=10, classifier__gamma=0.01..................\n",
      "[CV 2/5; 26/36] END classifier__C=10, classifier__gamma=0.01;, score=0.764 total time=   1.2s\n",
      "[CV 3/5; 26/36] START classifier__C=10, classifier__gamma=0.01..................\n",
      "[CV 3/5; 26/36] END classifier__C=10, classifier__gamma=0.01;, score=0.762 total time=   2.0s\n",
      "[CV 4/5; 26/36] START classifier__C=10, classifier__gamma=0.01..................\n",
      "[CV 4/5; 26/36] END classifier__C=10, classifier__gamma=0.01;, score=0.761 total time=   2.2s\n",
      "[CV 5/5; 26/36] START classifier__C=10, classifier__gamma=0.01..................\n",
      "[CV 5/5; 26/36] END classifier__C=10, classifier__gamma=0.01;, score=0.758 total time=   1.1s\n",
      "[CV 1/5; 27/36] START classifier__C=10, classifier__gamma=0.1...................\n",
      "[CV 1/5; 27/36] END classifier__C=10, classifier__gamma=0.1;, score=0.757 total time=   1.8s\n",
      "[CV 2/5; 27/36] START classifier__C=10, classifier__gamma=0.1...................\n",
      "[CV 2/5; 27/36] END classifier__C=10, classifier__gamma=0.1;, score=0.758 total time=   2.3s\n",
      "[CV 3/5; 27/36] START classifier__C=10, classifier__gamma=0.1...................\n",
      "[CV 3/5; 27/36] END classifier__C=10, classifier__gamma=0.1;, score=0.759 total time=   1.1s\n",
      "[CV 4/5; 27/36] START classifier__C=10, classifier__gamma=0.1...................\n",
      "[CV 4/5; 27/36] END classifier__C=10, classifier__gamma=0.1;, score=0.758 total time=   1.6s\n",
      "[CV 5/5; 27/36] START classifier__C=10, classifier__gamma=0.1...................\n",
      "[CV 5/5; 27/36] END classifier__C=10, classifier__gamma=0.1;, score=0.758 total time=   2.5s\n",
      "[CV 1/5; 28/36] START classifier__C=10, classifier__gamma=1.....................\n",
      "[CV 1/5; 28/36] END classifier__C=10, classifier__gamma=1;, score=0.757 total time=   1.3s\n",
      "[CV 2/5; 28/36] START classifier__C=10, classifier__gamma=1.....................\n",
      "[CV 2/5; 28/36] END classifier__C=10, classifier__gamma=1;, score=0.757 total time=   1.8s\n",
      "[CV 3/5; 28/36] START classifier__C=10, classifier__gamma=1.....................\n",
      "[CV 3/5; 28/36] END classifier__C=10, classifier__gamma=1;, score=0.758 total time=   2.4s\n",
      "[CV 4/5; 28/36] START classifier__C=10, classifier__gamma=1.....................\n",
      "[CV 4/5; 28/36] END classifier__C=10, classifier__gamma=1;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 28/36] START classifier__C=10, classifier__gamma=1.....................\n",
      "[CV 5/5; 28/36] END classifier__C=10, classifier__gamma=1;, score=0.758 total time=   1.7s\n",
      "[CV 1/5; 29/36] START classifier__C=10, classifier__gamma=10....................\n",
      "[CV 1/5; 29/36] END classifier__C=10, classifier__gamma=10;, score=0.757 total time=   2.5s\n",
      "[CV 2/5; 29/36] START classifier__C=10, classifier__gamma=10....................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 29/36] END classifier__C=10, classifier__gamma=10;, score=0.757 total time=   1.1s\n",
      "[CV 3/5; 29/36] START classifier__C=10, classifier__gamma=10....................\n",
      "[CV 3/5; 29/36] END classifier__C=10, classifier__gamma=10;, score=0.758 total time=   1.5s\n",
      "[CV 4/5; 29/36] START classifier__C=10, classifier__gamma=10....................\n",
      "[CV 4/5; 29/36] END classifier__C=10, classifier__gamma=10;, score=0.758 total time=   2.7s\n",
      "[CV 5/5; 29/36] START classifier__C=10, classifier__gamma=10....................\n",
      "[CV 5/5; 29/36] END classifier__C=10, classifier__gamma=10;, score=0.758 total time=   1.2s\n",
      "[CV 1/5; 30/36] START classifier__C=10, classifier__gamma=100...................\n",
      "[CV 1/5; 30/36] END classifier__C=10, classifier__gamma=100;, score=0.757 total time=   1.4s\n",
      "[CV 2/5; 30/36] START classifier__C=10, classifier__gamma=100...................\n",
      "[CV 2/5; 30/36] END classifier__C=10, classifier__gamma=100;, score=0.757 total time=   2.6s\n",
      "[CV 3/5; 30/36] START classifier__C=10, classifier__gamma=100...................\n",
      "[CV 3/5; 30/36] END classifier__C=10, classifier__gamma=100;, score=0.758 total time=   1.2s\n",
      "[CV 4/5; 30/36] START classifier__C=10, classifier__gamma=100...................\n",
      "[CV 4/5; 30/36] END classifier__C=10, classifier__gamma=100;, score=0.758 total time=   1.3s\n",
      "[CV 5/5; 30/36] START classifier__C=10, classifier__gamma=100...................\n",
      "[CV 5/5; 30/36] END classifier__C=10, classifier__gamma=100;, score=0.758 total time=   2.6s\n",
      "[CV 1/5; 31/36] START classifier__C=100, classifier__gamma=0.001................\n",
      "[CV 1/5; 31/36] END classifier__C=100, classifier__gamma=0.001;, score=0.737 total time=   1.4s\n",
      "[CV 2/5; 31/36] START classifier__C=100, classifier__gamma=0.001................\n",
      "[CV 2/5; 31/36] END classifier__C=100, classifier__gamma=0.001;, score=0.706 total time=   1.2s\n",
      "[CV 3/5; 31/36] START classifier__C=100, classifier__gamma=0.001................\n",
      "[CV 3/5; 31/36] END classifier__C=100, classifier__gamma=0.001;, score=0.732 total time=   2.7s\n",
      "[CV 4/5; 31/36] START classifier__C=100, classifier__gamma=0.001................\n",
      "[CV 4/5; 31/36] END classifier__C=100, classifier__gamma=0.001;, score=0.733 total time=   1.8s\n",
      "[CV 5/5; 31/36] START classifier__C=100, classifier__gamma=0.001................\n",
      "[CV 5/5; 31/36] END classifier__C=100, classifier__gamma=0.001;, score=0.746 total time=   1.4s\n",
      "[CV 1/5; 32/36] START classifier__C=100, classifier__gamma=0.01.................\n",
      "[CV 1/5; 32/36] END classifier__C=100, classifier__gamma=0.01;, score=0.764 total time=   2.6s\n",
      "[CV 2/5; 32/36] START classifier__C=100, classifier__gamma=0.01.................\n",
      "[CV 2/5; 32/36] END classifier__C=100, classifier__gamma=0.01;, score=0.764 total time=   1.3s\n",
      "[CV 3/5; 32/36] START classifier__C=100, classifier__gamma=0.01.................\n",
      "[CV 3/5; 32/36] END classifier__C=100, classifier__gamma=0.01;, score=0.762 total time=   1.3s\n",
      "[CV 4/5; 32/36] START classifier__C=100, classifier__gamma=0.01.................\n",
      "[CV 4/5; 32/36] END classifier__C=100, classifier__gamma=0.01;, score=0.761 total time=   2.6s\n",
      "[CV 5/5; 32/36] START classifier__C=100, classifier__gamma=0.01.................\n",
      "[CV 5/5; 32/36] END classifier__C=100, classifier__gamma=0.01;, score=0.758 total time=   1.4s\n",
      "[CV 1/5; 33/36] START classifier__C=100, classifier__gamma=0.1..................\n",
      "[CV 1/5; 33/36] END classifier__C=100, classifier__gamma=0.1;, score=0.757 total time=   1.3s\n",
      "[CV 2/5; 33/36] START classifier__C=100, classifier__gamma=0.1..................\n",
      "[CV 2/5; 33/36] END classifier__C=100, classifier__gamma=0.1;, score=0.758 total time=   2.6s\n",
      "[CV 3/5; 33/36] START classifier__C=100, classifier__gamma=0.1..................\n",
      "[CV 3/5; 33/36] END classifier__C=100, classifier__gamma=0.1;, score=0.759 total time=   1.4s\n",
      "[CV 4/5; 33/36] START classifier__C=100, classifier__gamma=0.1..................\n",
      "[CV 4/5; 33/36] END classifier__C=100, classifier__gamma=0.1;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 33/36] START classifier__C=100, classifier__gamma=0.1..................\n",
      "[CV 5/5; 33/36] END classifier__C=100, classifier__gamma=0.1;, score=0.758 total time=   2.6s\n",
      "[CV 1/5; 34/36] START classifier__C=100, classifier__gamma=1....................\n",
      "[CV 1/5; 34/36] END classifier__C=100, classifier__gamma=1;, score=0.757 total time=   1.5s\n",
      "[CV 2/5; 34/36] START classifier__C=100, classifier__gamma=1....................\n",
      "[CV 2/5; 34/36] END classifier__C=100, classifier__gamma=1;, score=0.757 total time=   1.2s\n",
      "[CV 3/5; 34/36] START classifier__C=100, classifier__gamma=1....................\n",
      "[CV 3/5; 34/36] END classifier__C=100, classifier__gamma=1;, score=0.758 total time=   2.6s\n",
      "[CV 4/5; 34/36] START classifier__C=100, classifier__gamma=1....................\n",
      "[CV 4/5; 34/36] END classifier__C=100, classifier__gamma=1;, score=0.758 total time=   1.7s\n",
      "[CV 5/5; 34/36] START classifier__C=100, classifier__gamma=1....................\n",
      "[CV 5/5; 34/36] END classifier__C=100, classifier__gamma=1;, score=0.758 total time=   1.2s\n",
      "[CV 1/5; 35/36] START classifier__C=100, classifier__gamma=10...................\n",
      "[CV 1/5; 35/36] END classifier__C=100, classifier__gamma=10;, score=0.757 total time=   2.6s\n",
      "[CV 2/5; 35/36] START classifier__C=100, classifier__gamma=10...................\n",
      "[CV 2/5; 35/36] END classifier__C=100, classifier__gamma=10;, score=0.757 total time=   1.7s\n",
      "[CV 3/5; 35/36] START classifier__C=100, classifier__gamma=10...................\n",
      "[CV 3/5; 35/36] END classifier__C=100, classifier__gamma=10;, score=0.758 total time=   1.2s\n",
      "[CV 4/5; 35/36] START classifier__C=100, classifier__gamma=10...................\n",
      "[CV 4/5; 35/36] END classifier__C=100, classifier__gamma=10;, score=0.758 total time=   2.6s\n",
      "[CV 5/5; 35/36] START classifier__C=100, classifier__gamma=10...................\n",
      "[CV 5/5; 35/36] END classifier__C=100, classifier__gamma=10;, score=0.758 total time=   1.6s\n",
      "[CV 1/5; 36/36] START classifier__C=100, classifier__gamma=100..................\n",
      "[CV 1/5; 36/36] END classifier__C=100, classifier__gamma=100;, score=0.757 total time=   1.1s\n",
      "[CV 2/5; 36/36] START classifier__C=100, classifier__gamma=100..................\n",
      "[CV 2/5; 36/36] END classifier__C=100, classifier__gamma=100;, score=0.757 total time=   2.5s\n",
      "[CV 3/5; 36/36] START classifier__C=100, classifier__gamma=100..................\n",
      "[CV 3/5; 36/36] END classifier__C=100, classifier__gamma=100;, score=0.758 total time=   1.8s\n",
      "[CV 4/5; 36/36] START classifier__C=100, classifier__gamma=100..................\n",
      "[CV 4/5; 36/36] END classifier__C=100, classifier__gamma=100;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 36/36] START classifier__C=100, classifier__gamma=100..................\n",
      "[CV 5/5; 36/36] END classifier__C=100, classifier__gamma=100;, score=0.758 total time=   2.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10, 'classifier__gamma': 0.01}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline),\n",
    "    ('classifier', SVC(kernel='rbf'))])\n",
    "\n",
    "param_grid = {\n",
    "            'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "svc_rbf = GridSearchCV(pipe, param_grid, cv=kfold, verbose=10)\n",
    "svc_rbf.fit(X_train, y_train)\n",
    "svc_rbf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d94d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV 1/5; 1/6] START classifier__C=0.001.........................................\n",
      "[CV 1/5; 1/6] END ..........classifier__C=0.001;, score=0.754 total time= 3.7min\n",
      "[CV 2/5; 1/6] START classifier__C=0.001.........................................\n",
      "[CV 2/5; 1/6] END ..........classifier__C=0.001;, score=0.738 total time= 4.7min\n",
      "[CV 3/5; 1/6] START classifier__C=0.001.........................................\n",
      "[CV 3/5; 1/6] END ..........classifier__C=0.001;, score=0.758 total time= 4.8min\n",
      "[CV 4/5; 1/6] START classifier__C=0.001.........................................\n",
      "[CV 4/5; 1/6] END ..........classifier__C=0.001;, score=0.761 total time= 5.0min\n",
      "[CV 5/5; 1/6] START classifier__C=0.001.........................................\n",
      "[CV 5/5; 1/6] END ..........classifier__C=0.001;, score=0.746 total time= 4.5min\n",
      "[CV 1/5; 2/6] START classifier__C=0.01..........................................\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline),\n",
    "    ('classifier', SVC(kernel='linear'))])\n",
    "\n",
    "param_grid = {\n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "svc_linear = GridSearchCV(pipe, param_grid, cv=kfold, verbose=10)\n",
    "svc_linear.fit(X_train, y_train)\n",
    "svc_linear.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "942ff07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV 1/5; 1/15] START classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905\n",
      "[CV 1/5; 1/15] END classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905;, score=0.844 total time=   0.5s\n",
      "[CV 2/5; 1/15] START classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905\n",
      "[CV 2/5; 1/15] END classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905;, score=0.836 total time=   0.5s\n",
      "[CV 3/5; 1/15] START classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905\n",
      "[CV 3/5; 1/15] END classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905;, score=0.812 total time=   0.4s\n",
      "[CV 4/5; 1/15] START classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905\n",
      "[CV 4/5; 1/15] END classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905;, score=0.819 total time=   0.4s\n",
      "[CV 5/5; 1/15] START classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905\n",
      "[CV 5/5; 1/15] END classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905;, score=0.832 total time=   1.1s\n",
      "[CV 1/5; 2/15] START classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769\n",
      "[CV 1/5; 2/15] END classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769;, score=0.838 total time=   3.0s\n",
      "[CV 2/5; 2/15] START classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769\n",
      "[CV 2/5; 2/15] END classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769;, score=0.833 total time=   2.9s\n",
      "[CV 3/5; 2/15] START classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769\n",
      "[CV 3/5; 2/15] END classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769;, score=0.815 total time=   2.8s\n",
      "[CV 4/5; 2/15] START classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769\n",
      "[CV 4/5; 2/15] END classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769;, score=0.832 total time=   2.4s\n",
      "[CV 5/5; 2/15] START classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769\n",
      "[CV 5/5; 2/15] END classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769;, score=0.844 total time=   2.9s\n",
      "[CV 1/5; 3/15] START classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407\n",
      "[CV 1/5; 3/15] END classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407;, score=0.838 total time=   2.4s\n",
      "[CV 2/5; 3/15] START classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407\n",
      "[CV 2/5; 3/15] END classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407;, score=0.834 total time=   2.0s\n",
      "[CV 3/5; 3/15] START classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407\n",
      "[CV 3/5; 3/15] END classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407;, score=0.821 total time=   2.9s\n",
      "[CV 4/5; 3/15] START classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407\n",
      "[CV 4/5; 3/15] END classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407;, score=0.831 total time=   1.7s\n",
      "[CV 5/5; 3/15] START classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 3/15] END classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407;, score=0.844 total time=   2.8s\n",
      "[CV 1/5; 4/15] START classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183\n",
      "[CV 1/5; 4/15] END classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183;, score=0.814 total time=   2.0s\n",
      "[CV 2/5; 4/15] START classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183\n",
      "[CV 2/5; 4/15] END classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183;, score=0.840 total time=   1.2s\n",
      "[CV 3/5; 4/15] START classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183\n",
      "[CV 3/5; 4/15] END classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183;, score=0.818 total time=   2.9s\n",
      "[CV 4/5; 4/15] START classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183\n",
      "[CV 4/5; 4/15] END classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183;, score=0.828 total time=   2.6s\n",
      "[CV 5/5; 4/15] START classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183\n",
      "[CV 5/5; 4/15] END classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183;, score=0.835 total time=   1.4s\n",
      "[CV 1/5; 5/15] START classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958\n",
      "[CV 1/5; 5/15] END classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958;, score=0.843 total time=   3.7s\n",
      "[CV 2/5; 5/15] START classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958\n",
      "[CV 2/5; 5/15] END classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958;, score=0.853 total time=   4.3s\n",
      "[CV 3/5; 5/15] START classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958\n",
      "[CV 3/5; 5/15] END classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958;, score=0.816 total time=   3.0s\n",
      "[CV 4/5; 5/15] START classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958\n",
      "[CV 4/5; 5/15] END classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958;, score=0.831 total time=   3.9s\n",
      "[CV 5/5; 5/15] START classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958\n",
      "[CV 5/5; 5/15] END classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958;, score=0.837 total time=   3.1s\n",
      "[CV 1/5; 6/15] START classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175\n",
      "[CV 1/5; 6/15] END classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175;, score=0.843 total time=   1.2s\n",
      "[CV 2/5; 6/15] START classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175\n",
      "[CV 2/5; 6/15] END classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175;, score=0.850 total time=   1.1s\n",
      "[CV 3/5; 6/15] START classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175\n",
      "[CV 3/5; 6/15] END classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175;, score=0.821 total time=   1.2s\n",
      "[CV 4/5; 6/15] START classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 6/15] END classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175;, score=0.828 total time=   1.2s\n",
      "[CV 5/5; 6/15] START classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175\n",
      "[CV 5/5; 6/15] END classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175;, score=0.841 total time=   1.2s\n",
      "[CV 1/5; 7/15] START classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595\n",
      "[CV 1/5; 7/15] END classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595;, score=0.828 total time=   4.6s\n",
      "[CV 2/5; 7/15] START classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595\n",
      "[CV 2/5; 7/15] END classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595;, score=0.840 total time=   3.5s\n",
      "[CV 3/5; 7/15] START classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595\n",
      "[CV 3/5; 7/15] END classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595;, score=0.810 total time=   4.5s\n",
      "[CV 4/5; 7/15] START classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595\n",
      "[CV 4/5; 7/15] END classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595;, score=0.825 total time=   4.4s\n",
      "[CV 5/5; 7/15] START classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595\n",
      "[CV 5/5; 7/15] END classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595;, score=0.827 total time=   4.3s\n",
      "[CV 1/5; 8/15] START classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613\n",
      "[CV 1/5; 8/15] END classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613;, score=0.841 total time=   0.7s\n",
      "[CV 2/5; 8/15] START classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613\n",
      "[CV 2/5; 8/15] END classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613;, score=0.853 total time=   1.5s\n",
      "[CV 3/5; 8/15] START classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613\n",
      "[CV 3/5; 8/15] END classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613;, score=0.813 total time=   1.7s\n",
      "[CV 4/5; 8/15] START classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613\n",
      "[CV 4/5; 8/15] END classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613;, score=0.834 total time=   1.7s\n",
      "[CV 5/5; 8/15] START classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613\n",
      "[CV 5/5; 8/15] END classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613;, score=0.847 total time=   1.7s\n",
      "[CV 1/5; 9/15] START classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534\n",
      "[CV 1/5; 9/15] END classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534;, score=0.843 total time=   1.8s\n",
      "[CV 2/5; 9/15] START classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534\n",
      "[CV 2/5; 9/15] END classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534;, score=0.844 total time=   1.1s\n",
      "[CV 3/5; 9/15] START classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 9/15] END classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534;, score=0.827 total time=   1.6s\n",
      "[CV 4/5; 9/15] START classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534\n",
      "[CV 4/5; 9/15] END classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534;, score=0.838 total time=   1.8s\n",
      "[CV 5/5; 9/15] START classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534\n",
      "[CV 5/5; 9/15] END classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534;, score=0.850 total time=   1.9s\n",
      "[CV 1/5; 10/15] START classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617\n",
      "[CV 1/5; 10/15] END classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617;, score=0.843 total time=   1.3s\n",
      "[CV 2/5; 10/15] START classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617\n",
      "[CV 2/5; 10/15] END classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617;, score=0.854 total time=   1.3s\n",
      "[CV 3/5; 10/15] START classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617\n",
      "[CV 3/5; 10/15] END classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617;, score=0.816 total time=   1.7s\n",
      "[CV 4/5; 10/15] START classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617\n",
      "[CV 4/5; 10/15] END classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617;, score=0.825 total time=   1.1s\n",
      "[CV 5/5; 10/15] START classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617\n",
      "[CV 5/5; 10/15] END classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617;, score=0.851 total time=   1.1s\n",
      "[CV 1/5; 11/15] START classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158\n",
      "[CV 1/5; 11/15] END classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158;, score=0.843 total time=   2.0s\n",
      "[CV 2/5; 11/15] START classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158\n",
      "[CV 2/5; 11/15] END classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158;, score=0.843 total time=   1.8s\n",
      "[CV 3/5; 11/15] START classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158\n",
      "[CV 3/5; 11/15] END classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158;, score=0.816 total time=   2.1s\n",
      "[CV 4/5; 11/15] START classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158\n",
      "[CV 4/5; 11/15] END classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158;, score=0.835 total time=   0.8s\n",
      "[CV 5/5; 11/15] START classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158\n",
      "[CV 5/5; 11/15] END classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158;, score=0.845 total time=   2.0s\n",
      "[CV 1/5; 12/15] START classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963\n",
      "[CV 1/5; 12/15] END classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963;, score=0.841 total time=   2.2s\n",
      "[CV 2/5; 12/15] START classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 12/15] END classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963;, score=0.847 total time=   2.1s\n",
      "[CV 3/5; 12/15] START classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963\n",
      "[CV 3/5; 12/15] END classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963;, score=0.810 total time=   1.9s\n",
      "[CV 4/5; 12/15] START classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963\n",
      "[CV 4/5; 12/15] END classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963;, score=0.821 total time=   2.1s\n",
      "[CV 5/5; 12/15] START classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963\n",
      "[CV 5/5; 12/15] END classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963;, score=0.843 total time=   2.2s\n",
      "[CV 1/5; 13/15] START classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747\n",
      "[CV 1/5; 13/15] END classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747;, score=0.840 total time=   2.8s\n",
      "[CV 2/5; 13/15] START classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747\n",
      "[CV 2/5; 13/15] END classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747;, score=0.843 total time=   3.4s\n",
      "[CV 3/5; 13/15] START classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747\n",
      "[CV 3/5; 13/15] END classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747;, score=0.815 total time=   1.9s\n",
      "[CV 4/5; 13/15] START classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747\n",
      "[CV 4/5; 13/15] END classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747;, score=0.829 total time=   2.7s\n",
      "[CV 5/5; 13/15] START classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747\n",
      "[CV 5/5; 13/15] END classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747;, score=0.835 total time=   2.7s\n",
      "[CV 1/5; 14/15] START classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038\n",
      "[CV 1/5; 14/15] END classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038;, score=0.831 total time=   2.2s\n",
      "[CV 2/5; 14/15] START classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038\n",
      "[CV 2/5; 14/15] END classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038;, score=0.836 total time=   2.1s\n",
      "[CV 3/5; 14/15] START classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038\n",
      "[CV 3/5; 14/15] END classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038;, score=0.819 total time=   2.4s\n",
      "[CV 4/5; 14/15] START classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038\n",
      "[CV 4/5; 14/15] END classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038;, score=0.835 total time=   1.3s\n",
      "[CV 5/5; 14/15] START classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038\n",
      "[CV 5/5; 14/15] END classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038;, score=0.847 total time=   2.2s\n",
      "[CV 1/5; 15/15] START classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 15/15] END classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207;, score=0.833 total time=   0.5s\n",
      "[CV 2/5; 15/15] START classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207\n",
      "[CV 2/5; 15/15] END classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207;, score=0.847 total time=   0.6s\n",
      "[CV 3/5; 15/15] START classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207\n",
      "[CV 3/5; 15/15] END classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207;, score=0.810 total time=   0.5s\n",
      "[CV 4/5; 15/15] START classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207\n",
      "[CV 4/5; 15/15] END classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207;, score=0.824 total time=   0.5s\n",
      "[CV 5/5; 15/15] START classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207\n",
      "[CV 5/5; 15/15] END classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207;, score=0.844 total time=   0.5s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'randm_src' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18356/131948785.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mxgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distributions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_distribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandm_src\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'randm_src' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats.distributions import uniform, randint\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline),\n",
    "    ('classifier', XGBClassifier())])\n",
    "\n",
    "param_distribution = {\n",
    "    'classifier__max_depth': randint(3, 11),\n",
    "    'classifier__learning_rate': uniform(0.001, 0.1-0.001),\n",
    "    'classifier__n_estimators': randint(50, 400),\n",
    "    'classifier__gamma': uniform(0,2),\n",
    "    'classifier__colsample_bytree': uniform(0.5, 0.5),\n",
    "    'classifier__subsample': uniform(0.5, 0.5),\n",
    "    'classifier__min_child_weight': randint(1, 11)\n",
    "}\n",
    "\n",
    "\n",
    "xgb = RandomizedSearchCV(pipe, param_distributions = param_distribution, n_iter = 15, verbose=10)\n",
    "xgb.fit(X_train, y_train)\n",
    "print(xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08696ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan 0.75757614        nan 0.75582687        nan 0.75757614\n",
      "        nan 0.75582687        nan 0.75845078        nan 0.75582687\n",
      "        nan 0.75582687        nan 0.75582687        nan 0.75582687\n",
      "        nan 0.75582687        nan 0.75582687        nan 0.75582687\n",
      "        nan 0.75582687        nan 0.75582687        nan 0.75582687\n",
      "        nan 0.75582687]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.01, 'classifier__penalty': 'l2'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline),\n",
    "    ('classifier', LogisticRegression())])\n",
    "\n",
    "param_grid = {\n",
    "            'classifier__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "            'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none']\n",
    "}\n",
    "grid_4 = GridSearchCV(pipe, param_grid, cv=kfold)\n",
    "grid_4.fit(X_train, y_train)\n",
    "grid_4.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f310ce1b",
   "metadata": {},
   "source": [
    "Add decision tree, ensenble i  AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e158d36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM rbf\n",
      "precision_score: 0.7598889659958362\n",
      "recall_score: 0.9838274932614556\n",
      "f1_score: 0.8574784651527017\n",
      "accuracy_score: 0.752549286199864\n",
      "forest\n",
      "precision_score: 0.8408910103420844\n",
      "recall_score: 0.949685534591195\n",
      "f1_score: 0.8919831223628693\n",
      "accuracy_score: 0.8259687287559483\n",
      "Logistic\n",
      "precision_score: 0.7566281441196465\n",
      "recall_score: 1.0\n",
      "f1_score: 0.8614551083591332\n",
      "accuracy_score: 0.7566281441196465\n",
      "XGB\n",
      "precision_score: 0.8738197424892704\n",
      "recall_score: 0.9146451033243486\n",
      "f1_score: 0.893766461808604\n",
      "accuracy_score: 0.8354860639021074\n"
     ]
    }
   ],
   "source": [
    "from sklearn import  metrics\n",
    "\n",
    "models = []\n",
    "models.append(('SVM rbf', grid_1.best_estimator_))\n",
    "models.append(('Random Forest', forest.best_estimator_))\n",
    "models.append(('Logistic', grid_4.best_estimator_))\n",
    "models.append(('XGB', xgb.best_estimator_))\n",
    "\n",
    "\n",
    "precision_score = []\n",
    "recall_score = []\n",
    "f1_score = []\n",
    "accuracy_score = []\n",
    "for name, model in models:\n",
    "    print(name)\n",
    "    print(\"precision_score: {}\".format(metrics.precision_score(y_test, model.predict(X_test)) ))\n",
    "    print(\"recall_score: {}\".format( metrics.recall_score(y_test, model.predict(X_test)) ))\n",
    "    print(\"f1_score: {}\".format( metrics.f1_score(y_test, model.predict(X_test)) ))\n",
    "    print(\"accuracy_score: {}\".format( metrics.accuracy_score(y_test, model.predict(X_test)) ))\n",
    "    precision_score.append(metrics.precision_score(y_test, model.predict(X_test)))\n",
    "    recall_score.append(metrics.recall_score(y_test, model.predict(X_test)))\n",
    "    f1_score.append( metrics.f1_score(y_test, model.predict(X_test)))\n",
    "    accuracy_score.append(metrics.accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c47079",
   "metadata": {},
   "source": [
    "# Głębokie uczenie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab1ee227",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline.fit(X)\n",
    "X_train2 = preprocess_pipeline.transform(X_train)\n",
    "X_test2 = preprocess_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2c0e0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model_1 = keras.Sequential([\n",
    "              Dense(100, activation='relu', name=\"1st_layer\", input_shape=(X_train2.shape[1], )),\n",
    "              Dropout(0.2),\n",
    "              Dense(50, activation='relu', name=\"2nd_layer\"),\n",
    "              Dropout(0.2),\n",
    "              Dense(20, activation='relu', name=\"3nd_layer\"),\n",
    "              Dropout(0.2),\n",
    "              Dense(10, activation='relu', name=\"4nd_layer\"),\n",
    "              Dense(2, activation='sigmoid', name=\"output_layer\")\n",
    "], name=\"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7adec96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8063fd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "108/108 [==============================] - 1s 3ms/step - loss: 404.3959 - accuracy: 0.5994 - val_loss: 2.1154 - val_accuracy: 0.7566\n",
      "Epoch 2/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 62.1587 - accuracy: 0.6935 - val_loss: 0.6635 - val_accuracy: 0.7566\n",
      "Epoch 3/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 26.6581 - accuracy: 0.7165 - val_loss: 0.6433 - val_accuracy: 0.7566\n",
      "Epoch 4/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 15.0847 - accuracy: 0.7287 - val_loss: 0.6266 - val_accuracy: 0.7566\n",
      "Epoch 5/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 14.1399 - accuracy: 0.7191 - val_loss: 0.6116 - val_accuracy: 0.7566\n",
      "Epoch 6/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 7.1456 - accuracy: 0.7398 - val_loss: 0.5991 - val_accuracy: 0.7566\n",
      "Epoch 7/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 4.8391 - accuracy: 0.7453 - val_loss: 0.5887 - val_accuracy: 0.7566\n",
      "Epoch 8/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 4.7874 - accuracy: 0.7488 - val_loss: 0.5801 - val_accuracy: 0.7566\n",
      "Epoch 9/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 3.7293 - accuracy: 0.7550 - val_loss: 0.5742 - val_accuracy: 0.7566\n",
      "Epoch 10/100\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 1.5872 - accuracy: 0.7547 - val_loss: 0.5695 - val_accuracy: 0.7566\n",
      "Epoch 11/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.5087 - accuracy: 0.7558 - val_loss: 0.5659 - val_accuracy: 0.7566\n",
      "Epoch 12/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 2.4398 - accuracy: 0.7558 - val_loss: 0.5632 - val_accuracy: 0.7566\n",
      "Epoch 13/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.5379 - accuracy: 0.7573 - val_loss: 0.5612 - val_accuracy: 0.7566\n",
      "Epoch 14/100\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 1.7688 - accuracy: 0.7535 - val_loss: 0.5596 - val_accuracy: 0.7566\n",
      "Epoch 15/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.8406 - accuracy: 0.7552 - val_loss: 0.5583 - val_accuracy: 0.7566\n",
      "Epoch 16/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 2.4426 - accuracy: 0.7561 - val_loss: 0.5573 - val_accuracy: 0.7566\n",
      "Epoch 17/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.9682 - accuracy: 0.7552 - val_loss: 0.5566 - val_accuracy: 0.7566\n",
      "Epoch 18/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.2669 - accuracy: 0.7567 - val_loss: 0.5561 - val_accuracy: 0.7566\n",
      "Epoch 19/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.3019 - accuracy: 0.7550 - val_loss: 0.5557 - val_accuracy: 0.7566\n",
      "Epoch 20/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.3314 - accuracy: 0.7550 - val_loss: 0.5555 - val_accuracy: 0.7566\n",
      "Epoch 21/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.9397 - accuracy: 0.7579 - val_loss: 0.5553 - val_accuracy: 0.7566\n",
      "Epoch 22/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.6466 - accuracy: 0.7573 - val_loss: 0.5552 - val_accuracy: 0.7566\n",
      "Epoch 23/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.8904 - accuracy: 0.7555 - val_loss: 0.5551 - val_accuracy: 0.7566\n",
      "Epoch 24/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.1632 - accuracy: 0.7558 - val_loss: 0.5550 - val_accuracy: 0.7566\n",
      "Epoch 25/100\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.1160 - accuracy: 0.7555 - val_loss: 0.5550 - val_accuracy: 0.7566\n",
      "Epoch 26/100\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.7854 - accuracy: 0.7555 - val_loss: 0.5550 - val_accuracy: 0.7566\n",
      "Epoch 27/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 2.1413 - accuracy: 0.7523 - val_loss: 0.5550 - val_accuracy: 0.7566\n",
      "Epoch 28/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0591 - accuracy: 0.7564 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 29/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9893 - accuracy: 0.7555 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 30/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0634 - accuracy: 0.7564 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 31/100\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.7029 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 32/100\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.6421 - accuracy: 0.7564 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 33/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5857 - accuracy: 0.7550 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 34/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.7579 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 35/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.7388 - accuracy: 0.7538 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 36/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.7267 - accuracy: 0.7558 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 37/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.9249 - accuracy: 0.7564 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 38/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.7570 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 39/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.6458 - accuracy: 0.7564 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 40/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.8907 - accuracy: 0.7567 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 41/100\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.6966 - accuracy: 0.7567 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 42/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6878 - accuracy: 0.7552 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 43/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0394 - accuracy: 0.7564 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 44/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6573 - accuracy: 0.7570 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 45/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8853 - accuracy: 0.7564 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 46/100\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.6825 - accuracy: 0.7555 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 47/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7429 - accuracy: 0.7570 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 48/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6041 - accuracy: 0.7567 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 49/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5883 - accuracy: 0.7570 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 50/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.7570 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 51/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5726 - accuracy: 0.7567 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 52/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5861 - accuracy: 0.7567 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 53/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5597 - accuracy: 0.7570 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 54/100\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.5785 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 55/100\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.5541 - accuracy: 0.7573 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 56/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5869 - accuracy: 0.7567 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 57/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5542 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7462 - accuracy: 0.7558 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 59/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5826 - accuracy: 0.7564 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 60/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5996 - accuracy: 0.7567 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 61/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6195 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 62/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6889 - accuracy: 0.7567 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 63/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5546 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 64/100\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5840 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 65/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5536 - accuracy: 0.7579 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 66/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6201 - accuracy: 0.7567 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 67/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 68/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5556 - accuracy: 0.7573 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 69/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7914 - accuracy: 0.7567 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 70/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6149 - accuracy: 0.7573 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 71/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5546 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 72/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5533 - accuracy: 0.7579 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 73/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5852 - accuracy: 0.7570 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 74/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.6364 - accuracy: 0.7573 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 75/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5850 - accuracy: 0.7567 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 76/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5704 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 77/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.6256 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 78/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5533 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 79/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.6348 - accuracy: 0.7570 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 80/100\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.6076 - accuracy: 0.7570 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 81/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5542 - accuracy: 0.7573 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 82/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5537 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 83/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5602 - accuracy: 0.7573 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 84/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5764 - accuracy: 0.7573 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 85/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6404 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 86/100\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5994 - accuracy: 0.7570 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 87/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5586 - accuracy: 0.7573 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 88/100\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6076 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 89/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5731 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 90/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5640 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 91/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6247 - accuracy: 0.7573 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 92/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5564 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 93/100\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5559 - accuracy: 0.7573 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 94/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5534 - accuracy: 0.7579 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 95/100\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5538 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 96/100\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5537 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 97/100\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5545 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 98/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5538 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 99/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5536 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n",
      "Epoch 100/100\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5539 - accuracy: 0.7576 - val_loss: 0.5549 - val_accuracy: 0.7566\n"
     ]
    }
   ],
   "source": [
    "history_1 = model_1.fit(X_train2, y_train, \n",
    "                      epochs=100, \n",
    "                      batch_size=32, \n",
    "                      validation_data=(X_test2, y_test),\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a7f917cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history_1.history['accuracy']), label='acc')\n",
    "plt.plot(pd.DataFrame(history_1.history['val_accuracy']), label='val_acc')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b3206ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history_1.history).plot(figsize=(10, 7));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10bf4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
