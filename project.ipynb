{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9929e2d",
   "metadata": {},
   "source": [
    "# Salary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1560f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248124e",
   "metadata": {},
   "source": [
    "Wczytajmy dane znajdujące się w folderze ./data , plik salary.csv i spojrzmy na ich strukturę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172746e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_data(filename, FOLDER=\"./data\"):\n",
    "    csv_path = os.path.join(FOLDER, filename)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0150371d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  salary  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_data(\"salary.csv\")\n",
    "display(data.head())\n",
    "\n",
    "data = data[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf14312",
   "metadata": {},
   "source": [
    "Columns are:\n",
    " - age: continuous.\n",
    " - workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    " - fnlwgt: continuous.\n",
    " - education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th,    10th, Doctorate, 5th-6th, Preschool.\n",
    " - education-num: continuous.\n",
    " - marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    " - occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op- inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    " - relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    " - race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    " - sex: Female, Male.\n",
    " - capital-gain: continuous.\n",
    " - capital-loss: continuous.\n",
    " - hours-per-week: continuous.\n",
    " - native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    " - salary: <=50K or >50K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5ec5f",
   "metadata": {},
   "source": [
    "Just to make some not obvious things clear:\n",
    "\n",
    "- The continuous variable fnlwgt represents final weight, which is the number of units in the target population that the responding unit represents.\n",
    "- education-num - number of years of education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5672110",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.columns] = data[data.columns].replace(\" ?\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4edfb7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             10000 non-null  int64 \n",
      " 1   workclass       9415 non-null   object\n",
      " 2   fnlwgt          10000 non-null  int64 \n",
      " 3   education       10000 non-null  object\n",
      " 4   education-num   10000 non-null  int64 \n",
      " 5   marital-status  10000 non-null  object\n",
      " 6   occupation      9414 non-null   object\n",
      " 7   relationship    10000 non-null  object\n",
      " 8   race            10000 non-null  object\n",
      " 9   sex             10000 non-null  object\n",
      " 10  capital-gain    10000 non-null  int64 \n",
      " 11  capital-loss    10000 non-null  int64 \n",
      " 12  hours-per-week  10000 non-null  int64 \n",
      " 13  native-country  9819 non-null   object\n",
      " 14  salary          10000 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb83db",
   "metadata": {},
   "source": [
    "Widzimy, że w ałym datasecie mamy 32561 wiersze.\n",
    "\n",
    "W kolumnie workclass mamy 1836 wartości null (5.6%) \n",
    "\n",
    "W kolumnie occupation mamy 1843 wartości null (5.7%)\n",
    "\n",
    "W kolumnie native-country mamy 583 wartości null (1.8%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd95d65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 0\n",
       "workclass         585\n",
       "fnlwgt              0\n",
       "education           0\n",
       "education-num       0\n",
       "marital-status      0\n",
       "occupation        586\n",
       "relationship        0\n",
       "race                0\n",
       "sex                 0\n",
       "capital-gain        0\n",
       "capital-loss        0\n",
       "hours-per-week      0\n",
       "native-country    181\n",
       "salary              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73124e5",
   "metadata": {},
   "source": [
    "Widzimy, że mamy stosunkowo mało wierszy z nullową kollumną native-country. \\\n",
    "Najlepszym i najprostszym podejściem w takiej sytuacji wydaje się być po prostu usunięcie tych rekordów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9094541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data['native-country'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30df2635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 0\n",
      "workclass         574\n",
      "fnlwgt              0\n",
      "education           0\n",
      "education-num       0\n",
      "marital-status      0\n",
      "occupation        575\n",
      "relationship        0\n",
      "race                0\n",
      "sex                 0\n",
      "capital-gain        0\n",
      "capital-loss        0\n",
      "hours-per-week      0\n",
      "native-country      0\n",
      "salary              0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9819 entries, 0 to 9999\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             9819 non-null   int64 \n",
      " 1   workclass       9245 non-null   object\n",
      " 2   fnlwgt          9819 non-null   int64 \n",
      " 3   education       9819 non-null   object\n",
      " 4   education-num   9819 non-null   int64 \n",
      " 5   marital-status  9819 non-null   object\n",
      " 6   occupation      9244 non-null   object\n",
      " 7   relationship    9819 non-null   object\n",
      " 8   race            9819 non-null   object\n",
      " 9   sex             9819 non-null   object\n",
      " 10  capital-gain    9819 non-null   int64 \n",
      " 11  capital-loss    9819 non-null   int64 \n",
      " 12  hours-per-week  9819 non-null   int64 \n",
      " 13  native-country  9819 non-null   object\n",
      " 14  salary          9819 non-null   object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.isna().sum())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d35738c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Prof-specialty       1219\n",
       " Exec-managerial      1197\n",
       " Craft-repair         1187\n",
       " Adm-clerical         1172\n",
       " Sales                1159\n",
       " Other-service        1005\n",
       " Machine-op-inspct     610\n",
       " Transport-moving      500\n",
       " Handlers-cleaners     389\n",
       " Farming-fishing       289\n",
       " Tech-support          280\n",
       " Protective-serv       195\n",
       " Priv-house-serv        40\n",
       " Armed-Forces            2\n",
       "Name: occupation, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"occupation\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66158e31",
   "metadata": {},
   "source": [
    "Wartości nullowe w kolumnie occupation wypełnimy wartością domyślną Other-service, a dla wartości nullowych kolumny workclass wprowadzimy nową wartość Other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d9ac145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Other-service        1580\n",
      " Prof-specialty       1219\n",
      " Exec-managerial      1197\n",
      " Craft-repair         1187\n",
      " Adm-clerical         1172\n",
      " Sales                1159\n",
      " Machine-op-inspct     610\n",
      " Transport-moving      500\n",
      " Handlers-cleaners     389\n",
      " Farming-fishing       289\n",
      " Tech-support          280\n",
      " Protective-serv       195\n",
      " Priv-house-serv        40\n",
      " Armed-Forces            2\n",
      "Name: occupation, dtype: int64\n",
      " Private             6821\n",
      " Self-emp-not-inc     793\n",
      " Local-gov            638\n",
      " Other                574\n",
      " State-gov            394\n",
      " Self-emp-inc         332\n",
      " Federal-gov          264\n",
      " Without-pay            2\n",
      " Never-worked           1\n",
      "Name: workclass, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data[\"occupation\"].fillna(value=\" Other-service\", inplace=True)\n",
    "data[\"workclass\"].fillna(value=\" Other\", inplace=True)\n",
    "\n",
    "print(data[\"occupation\"].value_counts())\n",
    "print(data[\"workclass\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76421e6e",
   "metadata": {},
   "source": [
    "### Zamienimy kluczy na binarne wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba57a27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  salary  \n",
       "0          2174             0              40   United-States       1  \n",
       "1             0             0              13   United-States       1  \n",
       "2             0             0              40   United-States       1  \n",
       "3             0             0              40   United-States       1  \n",
       "4             0             0              40            Cuba       1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.replace([\">50K\", \"<=50K\"], [0, 1], regex = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5004d1",
   "metadata": {},
   "source": [
    "### Zrobimy to samo dla innej binarnej kolumny ''sex\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "184ae83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race  sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White    1   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White    1   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White    1   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black    1   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black    0   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  salary  \n",
       "0          2174             0              40   United-States       1  \n",
       "1             0             0              13   United-States       1  \n",
       "2             0             0              40   United-States       1  \n",
       "3             0             0              40   United-States       1  \n",
       "4             0             0              40            Cuba       1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.replace([\"Female\", \"Male\"], [0, 1], regex = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad94f84",
   "metadata": {},
   "source": [
    "Podzielimy dane na kolumny kategoryczne, numeryczne i binarne\n",
    "\n",
    "Kolumny numeryczne : age, fnlwgt, education-num, capital-gain, capital-loss, hours-per-week \\\n",
    "Kolumny kategoryczne : workclass, education, marital-status, occupation, relationship, race, native-country \\\n",
    "Kolumny binarne : sex, salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c422ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_fields = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "categorical_fields = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"native-country\"]\n",
    "binary_fields = [\"sex\", \"salary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98d868",
   "metadata": {},
   "source": [
    "## Przyjżyjmy się dokładniej danym numerycznym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6210fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9819.000000</td>\n",
       "      <td>9.819000e+03</td>\n",
       "      <td>9819.000000</td>\n",
       "      <td>9819.000000</td>\n",
       "      <td>9819.000000</td>\n",
       "      <td>9819.000000</td>\n",
       "      <td>9819.000000</td>\n",
       "      <td>9819.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.441185</td>\n",
       "      <td>1.907656e+05</td>\n",
       "      <td>10.067930</td>\n",
       "      <td>0.669009</td>\n",
       "      <td>1050.364497</td>\n",
       "      <td>86.617782</td>\n",
       "      <td>40.517670</td>\n",
       "      <td>0.763214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.623277</td>\n",
       "      <td>1.066171e+05</td>\n",
       "      <td>2.535342</td>\n",
       "      <td>0.470594</td>\n",
       "      <td>7255.633708</td>\n",
       "      <td>397.912735</td>\n",
       "      <td>12.274902</td>\n",
       "      <td>0.425131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.930200e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>1.179730e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.792030e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>2.399515e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.226583e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age        fnlwgt  education-num          sex  capital-gain  \\\n",
       "count  9819.000000  9.819000e+03    9819.000000  9819.000000   9819.000000   \n",
       "mean     38.441185  1.907656e+05      10.067930     0.669009   1050.364497   \n",
       "std      13.623277  1.066171e+05       2.535342     0.470594   7255.633708   \n",
       "min      17.000000  1.930200e+04       1.000000     0.000000      0.000000   \n",
       "25%      27.000000  1.179730e+05       9.000000     0.000000      0.000000   \n",
       "50%      37.000000  1.792030e+05      10.000000     1.000000      0.000000   \n",
       "75%      47.000000  2.399515e+05      12.000000     1.000000      0.000000   \n",
       "max      90.000000  1.226583e+06      16.000000     1.000000  99999.000000   \n",
       "\n",
       "       capital-loss  hours-per-week       salary  \n",
       "count   9819.000000     9819.000000  9819.000000  \n",
       "mean      86.617782       40.517670     0.763214  \n",
       "std      397.912735       12.274902     0.425131  \n",
       "min        0.000000        1.000000     0.000000  \n",
       "25%        0.000000       40.000000     1.000000  \n",
       "50%        0.000000       40.000000     1.000000  \n",
       "75%        0.000000       45.000000     1.000000  \n",
       "max     4356.000000       99.000000     1.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea161e",
   "metadata": {},
   "source": [
    "Najprawdopodobniej będziemy ignorować kolumny capital-gain i kapital-loss z powodu tego, że prawie zawsze mają wartośc 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf628967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       9360\n",
      "1902      66\n",
      "1977      56\n",
      "1887      36\n",
      "1485      19\n",
      "        ... \n",
      "2547       1\n",
      "419        1\n",
      "1651       1\n",
      "653        1\n",
      "4356       1\n",
      "Name: capital-loss, Length: 71, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"capital-loss\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c5473eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        9013\n",
      "15024     111\n",
      "7688       83\n",
      "7298       64\n",
      "99999      46\n",
      "         ... \n",
      "1111        1\n",
      "1086        1\n",
      "4931        1\n",
      "991         1\n",
      "25124       1\n",
      "Name: capital-gain, Length: 102, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"capital-gain\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd21b8",
   "metadata": {},
   "source": [
    "### Przyjrzyjmy się pozostałym kolumnam numerycznym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c13eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121124    6\n",
      "123983    6\n",
      "177675    5\n",
      "168071    5\n",
      "116632    5\n",
      "         ..\n",
      "117674    1\n",
      "197023    1\n",
      "134671    1\n",
      "243607    1\n",
      "126743    1\n",
      "Name: fnlwgt, Length: 8397, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"fnlwgt\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5464b5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "plt.hist(data['fnlwgt'], bins=40, color=\"red\", label='fnlwgt')\n",
    "plt.xlabel('fnlwgt')\n",
    "plt.ylabel('Number of rows')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45763a",
   "metadata": {},
   "source": [
    "#### popatrzmy jak jest rozpodzielony wiek w naszy datasecie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c994174c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "plt.hist(data['age'], bins=73, color=\"blue\", label='Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Number of rows')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c28a67be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "plt.hist(data['education-num'], bins=10, color=\"orange\", label='education-num')\n",
    "plt.xlabel('education-num')\n",
    "plt.ylabel('Number of rows')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac89c80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "plt.hist(data['hours-per-week'], bins=20, color=\"blue\", label='hours-per-week')\n",
    "plt.xlabel('hours-per-week')\n",
    "plt.ylabel('Number of rows')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "199803ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='salary', ylabel='count'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "sns.countplot(data[\"salary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a0cff",
   "metadata": {},
   "source": [
    "Widzimy, że klasa negatywna i pozytywna są przedstawione w datasecie w różnych proporscjach, więc musimy pamiętać, aby koniecznie używać StratifiedKFold()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde6478",
   "metadata": {},
   "source": [
    "### Znormalizujemy numeryczne kolumny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8c909e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_field(dataset, field_name):\n",
    "    dataset[field_name] = dataset[field_name] / dataset[field_name].max()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b4af354",
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in numerical_fields:\n",
    "    data = normalize_field(data, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9183c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>0.063197</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>0.067921</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131313</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.422222</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.175810</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588889</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.191362</td>\n",
       "      <td>11th</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.311111</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.275896</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.422222</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.065469</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.342136</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.233333</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.260205</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.411111</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.103330</td>\n",
       "      <td>1st-4th</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.245860</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9819 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age          workclass    fnlwgt      education  education-num  \\\n",
       "0     0.433333          State-gov  0.063197      Bachelors         0.8125   \n",
       "1     0.555556   Self-emp-not-inc  0.067921      Bachelors         0.8125   \n",
       "2     0.422222            Private  0.175810        HS-grad         0.5625   \n",
       "3     0.588889            Private  0.191362           11th         0.4375   \n",
       "4     0.311111            Private  0.275896      Bachelors         0.8125   \n",
       "...        ...                ...       ...            ...            ...   \n",
       "9995  0.422222            Private  0.065469   Some-college         0.6250   \n",
       "9996  0.277778            Private  0.342136        HS-grad         0.5625   \n",
       "9997  0.233333            Private  0.260205   Some-college         0.6250   \n",
       "9998  0.411111            Private  0.103330        1st-4th         0.1250   \n",
       "9999  0.433333            Private  0.245860   Some-college         0.6250   \n",
       "\n",
       "           marital-status          occupation    relationship    race  sex  \\\n",
       "0           Never-married        Adm-clerical   Not-in-family   White    1   \n",
       "1      Married-civ-spouse     Exec-managerial         Husband   White    1   \n",
       "2                Divorced   Handlers-cleaners   Not-in-family   White    1   \n",
       "3      Married-civ-spouse   Handlers-cleaners         Husband   Black    1   \n",
       "4      Married-civ-spouse      Prof-specialty            Wife   Black    0   \n",
       "...                   ...                 ...             ...     ...  ...   \n",
       "9995   Married-civ-spouse               Sales         Husband   White    1   \n",
       "9996        Never-married       Other-service       Own-child   White    0   \n",
       "9997        Never-married    Transport-moving       Own-child   Black    1   \n",
       "9998   Married-civ-spouse       Other-service         Husband   White    1   \n",
       "9999             Divorced               Sales       Unmarried   White    0   \n",
       "\n",
       "      capital-gain  capital-loss  hours-per-week  native-country  salary  \n",
       "0          0.02174           0.0        0.404040   United-States       1  \n",
       "1          0.00000           0.0        0.131313   United-States       1  \n",
       "2          0.00000           0.0        0.404040   United-States       1  \n",
       "3          0.00000           0.0        0.404040   United-States       1  \n",
       "4          0.00000           0.0        0.404040            Cuba       1  \n",
       "...            ...           ...             ...             ...     ...  \n",
       "9995       0.00000           0.0        0.606061   United-States       0  \n",
       "9996       0.00000           0.0        0.080808   United-States       1  \n",
       "9997       0.00000           0.0        0.404040   United-States       1  \n",
       "9998       0.00000           0.0        0.535354          Mexico       1  \n",
       "9999       0.00000           0.0        0.404040   United-States       1  \n",
       "\n",
       "[9819 rows x 15 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1a3c8",
   "metadata": {},
   "source": [
    "Resztę pozostawiamy bez zmian "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d2af47",
   "metadata": {},
   "source": [
    "## Przyjrzyjmy się kolumnam kategorycznym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4a1b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60785f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Private             6821\n",
      " Self-emp-not-inc     793\n",
      " Local-gov            638\n",
      " Other                574\n",
      " State-gov            394\n",
      " Self-emp-inc         332\n",
      " Federal-gov          264\n",
      " Without-pay            2\n",
      " Never-worked           1\n",
      "Name: workclass, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"workclass\"].value_counts())\n",
    "# le.fit(data[\"workclass\"])\n",
    "# data[\"workclass\"] = le.transform(data[\"workclass\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75e421c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HS-grad         3187\n",
      " Some-college    2272\n",
      " Bachelors       1585\n",
      " Masters          519\n",
      " Assoc-voc        412\n",
      " 11th             361\n",
      " Assoc-acdm       314\n",
      " 10th             290\n",
      " 7th-8th          200\n",
      " Prof-school      171\n",
      " 9th              151\n",
      " Doctorate        106\n",
      " 12th             105\n",
      " 5th-6th           84\n",
      " 1st-4th           46\n",
      " Preschool         16\n",
      "Name: education, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"education\"].value_counts())\n",
    "# le.fit(data[\"education\"])\n",
    "# data[\"education\"] = le.transform(data[\"education\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "183ca2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Married-civ-spouse       4461\n",
      " Never-married            3252\n",
      " Divorced                 1373\n",
      " Separated                 314\n",
      " Widowed                   290\n",
      " Married-spouse-absent     122\n",
      " Married-AF-spouse           7\n",
      "Name: marital-status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"marital-status\"].value_counts())\n",
    "# le.fit(data[\"marital-status\"])\n",
    "# data[\"marital-status\"] = le.transform(data[\"marital-status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca2ed69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Other-service        1580\n",
      " Prof-specialty       1219\n",
      " Exec-managerial      1197\n",
      " Craft-repair         1187\n",
      " Adm-clerical         1172\n",
      " Sales                1159\n",
      " Machine-op-inspct     610\n",
      " Transport-moving      500\n",
      " Handlers-cleaners     389\n",
      " Farming-fishing       289\n",
      " Tech-support          280\n",
      " Protective-serv       195\n",
      " Priv-house-serv        40\n",
      " Armed-Forces            2\n",
      "Name: occupation, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"occupation\"].value_counts())\n",
    "# le.fit(data[\"occupation\"])\n",
    "# data[\"occupation\"] = le.transform(data[\"occupation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37b13d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Husband           3919\n",
      " Not-in-family     2543\n",
      " Own-child         1547\n",
      " Unmarried         1036\n",
      " Wife               484\n",
      " Other-relative     290\n",
      "Name: relationship, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"relationship\"].value_counts())\n",
    "# le.fit(data[\"relationship\"])\n",
    "# data[\"relationship\"] = le.transform(data[\"relationship\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc9fd676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " White                 8428\n",
      " Black                  929\n",
      " Asian-Pac-Islander     286\n",
      " Amer-Indian-Eskimo      99\n",
      " Other                   77\n",
      "Name: race, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"race\"].value_counts())\n",
    "# le.fit(data[\"race\"])\n",
    "# data[\"race\"] = le.transform(data[\"race\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a276b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " United-States                 8930\n",
      " Mexico                         208\n",
      " Philippines                     51\n",
      " Canada                          47\n",
      " Germany                         45\n",
      " Puerto-Rico                     35\n",
      " Cuba                            31\n",
      " Jamaica                         31\n",
      " China                           30\n",
      " El-Salvador                     30\n",
      " England                         29\n",
      " South                           27\n",
      " India                           26\n",
      " Vietnam                         23\n",
      " Dominican-Republic              23\n",
      " Iran                            22\n",
      " Poland                          22\n",
      " Guatemala                       21\n",
      " Japan                           20\n",
      " Italy                           19\n",
      " Taiwan                          18\n",
      " Greece                          17\n",
      " Haiti                           15\n",
      " Portugal                        12\n",
      " Nicaragua                       10\n",
      " Columbia                        10\n",
      " Cambodia                         7\n",
      " Peru                             7\n",
      " Thailand                         7\n",
      " Ecuador                          7\n",
      " Trinadad&Tobago                  6\n",
      " Ireland                          6\n",
      " France                           5\n",
      " Honduras                         5\n",
      " Yugoslavia                       4\n",
      " Outlying-US(Guam-USVI-etc)       4\n",
      " Laos                             4\n",
      " Scotland                         3\n",
      " Hong                             1\n",
      " Hungary                          1\n",
      "Name: native-country, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"native-country\"].value_counts())\n",
    "# le.fit(data[\"native-country\"])\n",
    "# data[\"native-country\"] = le.transform(data[\"native-country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5450aaff",
   "metadata": {},
   "source": [
    "Będziemy brać pod uwagę wszystkie kolumny kategoryczne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c61b0996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>0.063197</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>0.067921</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131313</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.422222</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.175810</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588889</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.191362</td>\n",
       "      <td>11th</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.311111</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.275896</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.422222</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.065469</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.342136</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.233333</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.260205</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.411111</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.103330</td>\n",
       "      <td>1st-4th</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.245860</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9819 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age          workclass    fnlwgt      education  education-num  \\\n",
       "0     0.433333          State-gov  0.063197      Bachelors         0.8125   \n",
       "1     0.555556   Self-emp-not-inc  0.067921      Bachelors         0.8125   \n",
       "2     0.422222            Private  0.175810        HS-grad         0.5625   \n",
       "3     0.588889            Private  0.191362           11th         0.4375   \n",
       "4     0.311111            Private  0.275896      Bachelors         0.8125   \n",
       "...        ...                ...       ...            ...            ...   \n",
       "9995  0.422222            Private  0.065469   Some-college         0.6250   \n",
       "9996  0.277778            Private  0.342136        HS-grad         0.5625   \n",
       "9997  0.233333            Private  0.260205   Some-college         0.6250   \n",
       "9998  0.411111            Private  0.103330        1st-4th         0.1250   \n",
       "9999  0.433333            Private  0.245860   Some-college         0.6250   \n",
       "\n",
       "           marital-status          occupation    relationship    race  sex  \\\n",
       "0           Never-married        Adm-clerical   Not-in-family   White    1   \n",
       "1      Married-civ-spouse     Exec-managerial         Husband   White    1   \n",
       "2                Divorced   Handlers-cleaners   Not-in-family   White    1   \n",
       "3      Married-civ-spouse   Handlers-cleaners         Husband   Black    1   \n",
       "4      Married-civ-spouse      Prof-specialty            Wife   Black    0   \n",
       "...                   ...                 ...             ...     ...  ...   \n",
       "9995   Married-civ-spouse               Sales         Husband   White    1   \n",
       "9996        Never-married       Other-service       Own-child   White    0   \n",
       "9997        Never-married    Transport-moving       Own-child   Black    1   \n",
       "9998   Married-civ-spouse       Other-service         Husband   White    1   \n",
       "9999             Divorced               Sales       Unmarried   White    0   \n",
       "\n",
       "      capital-gain  capital-loss  hours-per-week  native-country  salary  \n",
       "0          0.02174           0.0        0.404040   United-States       1  \n",
       "1          0.00000           0.0        0.131313   United-States       1  \n",
       "2          0.00000           0.0        0.404040   United-States       1  \n",
       "3          0.00000           0.0        0.404040   United-States       1  \n",
       "4          0.00000           0.0        0.404040            Cuba       1  \n",
       "...            ...           ...             ...             ...     ...  \n",
       "9995       0.00000           0.0        0.606061   United-States       0  \n",
       "9996       0.00000           0.0        0.080808   United-States       1  \n",
       "9997       0.00000           0.0        0.404040   United-States       1  \n",
       "9998       0.00000           0.0        0.535354          Mexico       1  \n",
       "9999       0.00000           0.0        0.404040   United-States       1  \n",
       "\n",
       "[9819 rows x 15 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffde781",
   "metadata": {},
   "source": [
    "### Stworzymy DataFrame Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f345d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced98f5e",
   "metadata": {},
   "source": [
    "Zbudujmy pipeline dla atrybutów numerycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26f912f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numerical_fields = [\"age\", \"fnlwgt\", \"education-num\", \"hours-per-week\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        (\"select_numeric\", DataFrameSelector(numerical_fields)),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b57e85",
   "metadata": {},
   "source": [
    "Imputer dla kategorycznych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4cf315f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n",
    "                                        index=X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.most_frequent_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc034f9b",
   "metadata": {},
   "source": [
    "Zbudujmy pipeline dla atrybutów kategorycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50e262ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "catbin = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"native-country\", \"sex\"]\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        (\"select_cat\", DataFrameSelector(catbin)),\n",
    "        (\"imputer\", MostFrequentImputer()),\n",
    "        (\"cat_encoder\", OneHotEncoder(sparse=False, handle_unknown = 'ignore')),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e756056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "preprocess_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7de22b",
   "metadata": {},
   "source": [
    "## Podzielimy zbiór danych na X i y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c93a1926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>0.063197</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>0.067921</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131313</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.422222</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.175810</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588889</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.191362</td>\n",
       "      <td>11th</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.311111</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.275896</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.422222</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.065469</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.342136</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.233333</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.260205</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.411111</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.103330</td>\n",
       "      <td>1st-4th</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>Mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.245860</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9819 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age          workclass    fnlwgt      education  education-num  \\\n",
       "0     0.433333          State-gov  0.063197      Bachelors         0.8125   \n",
       "1     0.555556   Self-emp-not-inc  0.067921      Bachelors         0.8125   \n",
       "2     0.422222            Private  0.175810        HS-grad         0.5625   \n",
       "3     0.588889            Private  0.191362           11th         0.4375   \n",
       "4     0.311111            Private  0.275896      Bachelors         0.8125   \n",
       "...        ...                ...       ...            ...            ...   \n",
       "9995  0.422222            Private  0.065469   Some-college         0.6250   \n",
       "9996  0.277778            Private  0.342136        HS-grad         0.5625   \n",
       "9997  0.233333            Private  0.260205   Some-college         0.6250   \n",
       "9998  0.411111            Private  0.103330        1st-4th         0.1250   \n",
       "9999  0.433333            Private  0.245860   Some-college         0.6250   \n",
       "\n",
       "           marital-status          occupation    relationship    race  sex  \\\n",
       "0           Never-married        Adm-clerical   Not-in-family   White    1   \n",
       "1      Married-civ-spouse     Exec-managerial         Husband   White    1   \n",
       "2                Divorced   Handlers-cleaners   Not-in-family   White    1   \n",
       "3      Married-civ-spouse   Handlers-cleaners         Husband   Black    1   \n",
       "4      Married-civ-spouse      Prof-specialty            Wife   Black    0   \n",
       "...                   ...                 ...             ...     ...  ...   \n",
       "9995   Married-civ-spouse               Sales         Husband   White    1   \n",
       "9996        Never-married       Other-service       Own-child   White    0   \n",
       "9997        Never-married    Transport-moving       Own-child   Black    1   \n",
       "9998   Married-civ-spouse       Other-service         Husband   White    1   \n",
       "9999             Divorced               Sales       Unmarried   White    0   \n",
       "\n",
       "      capital-gain  capital-loss  hours-per-week  native-country  \n",
       "0          0.02174           0.0        0.404040   United-States  \n",
       "1          0.00000           0.0        0.131313   United-States  \n",
       "2          0.00000           0.0        0.404040   United-States  \n",
       "3          0.00000           0.0        0.404040   United-States  \n",
       "4          0.00000           0.0        0.404040            Cuba  \n",
       "...            ...           ...             ...             ...  \n",
       "9995       0.00000           0.0        0.606061   United-States  \n",
       "9996       0.00000           0.0        0.080808   United-States  \n",
       "9997       0.00000           0.0        0.404040   United-States  \n",
       "9998       0.00000           0.0        0.535354          Mexico  \n",
       "9999       0.00000           0.0        0.404040   United-States  \n",
       "\n",
       "[9819 rows x 14 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop(['salary'], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78aff974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['salary'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a0be84",
   "metadata": {},
   "source": [
    "## Podzielimy zbiór danych na train i test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "049a9ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da172503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.17270743, 0.625     , 0.16161616],\n",
       "       [0.61111111, 0.26602113, 0.5625    , 0.25252525],\n",
       "       [0.28888889, 0.14355082, 0.625     , 0.4040404 ],\n",
       "       ...,\n",
       "       [0.28888889, 0.7477847 , 0.5       , 0.4040404 ],\n",
       "       [0.67777778, 0.18797586, 0.8125    , 0.4040404 ],\n",
       "       [0.52222222, 0.04506177, 0.625     , 0.4040404 ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd9fa38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d512c0",
   "metadata": {},
   "source": [
    "# Płytkie uczenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bff8127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "seed=123\n",
    "kfold = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2000a1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "[CV 1/5; 1/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 1/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.815 total time=   0.5s\n",
      "[CV 2/5; 1/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 1/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.809 total time=   0.5s\n",
      "[CV 3/5; 1/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 1/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.812 total time=   0.5s\n",
      "[CV 4/5; 1/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 1/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.809 total time=   0.5s\n",
      "[CV 5/5; 1/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 1/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.817 total time=   1.0s\n",
      "[CV 1/5; 2/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 2/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.814 total time=   2.7s\n",
      "[CV 2/5; 2/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 2/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.805 total time=   1.5s\n",
      "[CV 3/5; 2/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 2/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.815 total time=   2.7s\n",
      "[CV 4/5; 2/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 2/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.810 total time=   2.0s\n",
      "[CV 5/5; 2/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 2/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.816 total time=   1.7s\n",
      "[CV 1/5; 3/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 3/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.809 total time=   1.2s\n",
      "[CV 2/5; 3/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 3/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.810 total time=   1.2s\n",
      "[CV 3/5; 3/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 3/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.815 total time=   0.6s\n",
      "[CV 4/5; 3/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 3/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.811 total time=   0.5s\n",
      "[CV 5/5; 3/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 3/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.818 total time=   0.5s\n",
      "[CV 1/5; 4/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 4/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.815 total time=   2.5s\n",
      "[CV 2/5; 4/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 4/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.805 total time=   2.2s\n",
      "[CV 3/5; 4/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 4/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.813 total time=   1.6s\n",
      "[CV 4/5; 4/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 4/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.817 total time=   3.0s\n",
      "[CV 5/5; 4/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 4/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.818 total time=   1.4s\n",
      "[CV 1/5; 5/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 5/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.782 total time=   0.4s\n",
      "[CV 2/5; 5/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 5/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.775 total time=   0.8s\n",
      "[CV 3/5; 5/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 5/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.777 total time=   1.0s\n",
      "[CV 4/5; 5/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 5/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.782 total time=   0.6s\n",
      "[CV 5/5; 5/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 5/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.782 total time=   0.5s\n",
      "[CV 1/5; 6/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 6/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.789 total time=   1.3s\n",
      "[CV 2/5; 6/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 6/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.774 total time=   3.1s\n",
      "[CV 3/5; 6/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 6/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.782 total time=   1.5s\n",
      "[CV 4/5; 6/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 6/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.787 total time=   1.8s\n",
      "[CV 5/5; 6/60] START classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 6/60] END classifier__criterion=gini, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.778 total time=   2.5s\n",
      "[CV 1/5; 7/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 7/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.821 total time=   0.6s\n",
      "[CV 2/5; 7/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 7/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.820 total time=   0.6s\n",
      "[CV 3/5; 7/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 7/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.813 total time=   0.8s\n",
      "[CV 4/5; 7/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 7/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.817 total time=   1.5s\n",
      "[CV 5/5; 7/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 7/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.820 total time=   1.4s\n",
      "[CV 1/5; 8/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 8/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.822 total time=   1.6s\n",
      "[CV 2/5; 8/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 8/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.818 total time=   2.7s\n",
      "[CV 3/5; 8/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 8/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.815 total time=   2.3s\n",
      "[CV 4/5; 8/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 8/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.820 total time=   1.7s\n",
      "[CV 5/5; 8/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 8/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.822 total time=   3.0s\n",
      "[CV 1/5; 9/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 9/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.821 total time=   1.0s\n",
      "[CV 2/5; 9/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 9/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.817 total time=   0.6s\n",
      "[CV 3/5; 9/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 9/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.815 total time=   0.6s\n",
      "[CV 4/5; 9/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 9/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.822 total time=   0.8s\n",
      "[CV 5/5; 9/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 9/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.821 total time=   1.4s\n",
      "[CV 1/5; 10/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 10/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.822 total time=   2.5s\n",
      "[CV 2/5; 10/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 10/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.817 total time=   2.1s\n",
      "[CV 3/5; 10/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 10/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.812 total time=   3.0s\n",
      "[CV 4/5; 10/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 10/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.819 total time=   1.7s\n",
      "[CV 5/5; 10/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 10/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.822 total time=   3.4s\n",
      "[CV 1/5; 11/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 11/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.812 total time=   0.5s\n",
      "[CV 2/5; 11/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 11/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.793 total time=   0.5s\n",
      "[CV 3/5; 11/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 11/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.813 total time=   0.5s\n",
      "[CV 4/5; 11/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 11/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.801 total time=   0.7s\n",
      "[CV 5/5; 11/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 11/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.811 total time=   1.2s\n",
      "[CV 1/5; 12/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 12/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.805 total time=   2.2s\n",
      "[CV 2/5; 12/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 12/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.797 total time=   1.3s\n",
      "[CV 3/5; 12/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 12/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.815 total time=   2.6s\n",
      "[CV 4/5; 12/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 12/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.803 total time=   1.8s\n",
      "[CV 5/5; 12/60] START classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 12/60] END classifier__criterion=gini, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.803 total time=   1.4s\n",
      "[CV 1/5; 13/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 13/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.816 total time=   1.4s\n",
      "[CV 2/5; 13/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 13/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.823 total time=   1.5s\n",
      "[CV 3/5; 13/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 13/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.817 total time=   0.7s\n",
      "[CV 4/5; 13/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 13/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.820 total time=   0.6s\n",
      "[CV 5/5; 13/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 13/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.824 total time=   0.6s\n",
      "[CV 1/5; 14/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 14/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.819 total time=   3.5s\n",
      "[CV 2/5; 14/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 14/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.825 total time=   1.8s\n",
      "[CV 3/5; 14/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 14/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.817 total time=   3.5s\n",
      "[CV 4/5; 14/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 14/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.823 total time=   1.7s\n",
      "[CV 5/5; 14/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 14/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.824 total time=   3.1s\n",
      "[CV 1/5; 15/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 15/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.821 total time=   1.1s\n",
      "[CV 2/5; 15/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 15/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.825 total time=   0.7s\n",
      "[CV 3/5; 15/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 15/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.815 total time=   0.7s\n",
      "[CV 4/5; 15/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 15/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.820 total time=   1.3s\n",
      "[CV 5/5; 15/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 15/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.823 total time=   1.6s\n",
      "[CV 1/5; 16/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 16/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.820 total time=   1.9s\n",
      "[CV 2/5; 16/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 16/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.825 total time=   2.8s\n",
      "[CV 3/5; 16/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 16/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.818 total time=   2.4s\n",
      "[CV 4/5; 16/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 16/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.822 total time=   1.7s\n",
      "[CV 5/5; 16/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 16/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.825 total time=   3.3s\n",
      "[CV 1/5; 17/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 17/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.824 total time=   0.8s\n",
      "[CV 2/5; 17/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 17/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.812 total time=   0.6s\n",
      "[CV 3/5; 17/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 17/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.820 total time=   0.6s\n",
      "[CV 4/5; 17/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 17/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.812 total time=   0.6s\n",
      "[CV 5/5; 17/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 17/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.814 total time=   1.2s\n",
      "[CV 1/5; 18/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 18/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.817 total time=   2.4s\n",
      "[CV 2/5; 18/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 18/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.809 total time=   1.4s\n",
      "[CV 3/5; 18/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 18/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.811 total time=   2.8s\n",
      "[CV 4/5; 18/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 18/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.816 total time=   1.8s\n",
      "[CV 5/5; 18/60] START classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 18/60] END classifier__criterion=gini, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.817 total time=   1.8s\n",
      "[CV 1/5; 19/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 19/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.819 total time=   1.5s\n",
      "[CV 2/5; 19/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 19/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.827 total time=   1.3s\n",
      "[CV 3/5; 19/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 19/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.819 total time=   0.7s\n",
      "[CV 4/5; 19/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 19/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.825 total time=   0.7s\n",
      "[CV 5/5; 19/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 19/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.825 total time=   1.2s\n",
      "[CV 1/5; 20/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 20/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.821 total time=   3.0s\n",
      "[CV 2/5; 20/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 20/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.828 total time=   2.2s\n",
      "[CV 3/5; 20/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 20/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.820 total time=   3.3s\n",
      "[CV 4/5; 20/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 20/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.823 total time=   2.1s\n",
      "[CV 5/5; 20/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 20/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.822 total time=   3.4s\n",
      "[CV 1/5; 21/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 21/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.819 total time=   0.7s\n",
      "[CV 2/5; 21/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 21/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.828 total time=   0.8s\n",
      "[CV 3/5; 21/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 21/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.820 total time=   1.7s\n",
      "[CV 4/5; 21/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 21/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.823 total time=   1.5s\n",
      "[CV 5/5; 21/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 21/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.825 total time=   0.7s\n",
      "[CV 1/5; 22/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 22/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.819 total time=   2.3s\n",
      "[CV 2/5; 22/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 22/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.826 total time=   3.1s\n",
      "[CV 3/5; 22/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 22/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.822 total time=   2.4s\n",
      "[CV 4/5; 22/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 22/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.826 total time=   3.1s\n",
      "[CV 5/5; 22/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 22/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.822 total time=   2.4s\n",
      "[CV 1/5; 23/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 23/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.825 total time=   1.3s\n",
      "[CV 2/5; 23/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 23/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.817 total time=   1.1s\n",
      "[CV 3/5; 23/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 23/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.820 total time=   0.6s\n",
      "[CV 4/5; 23/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 23/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.819 total time=   0.6s\n",
      "[CV 5/5; 23/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 23/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.821 total time=   0.6s\n",
      "[CV 1/5; 24/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 24/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.822 total time=   3.4s\n",
      "[CV 2/5; 24/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 24/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.815 total time=   1.6s\n",
      "[CV 3/5; 24/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 24/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.815 total time=   3.1s\n",
      "[CV 4/5; 24/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 24/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.819 total time=   1.9s\n",
      "[CV 5/5; 24/60] START classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 24/60] END classifier__criterion=gini, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.820 total time=   2.1s\n",
      "[CV 1/5; 25/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 25/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.822 total time=   1.7s\n",
      "[CV 2/5; 25/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 25/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.832 total time=   1.0s\n",
      "[CV 3/5; 25/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 25/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.831 total time=   0.7s\n",
      "[CV 4/5; 25/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 25/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.827 total time=   0.8s\n",
      "[CV 5/5; 25/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 25/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.824 total time=   1.7s\n",
      "[CV 1/5; 26/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 26/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.819 total time=   2.8s\n",
      "[CV 2/5; 26/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 26/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.833 total time=   4.1s\n",
      "[CV 3/5; 26/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 26/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.827 total time=   2.1s\n",
      "[CV 4/5; 26/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 26/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.825 total time=   3.9s\n",
      "[CV 5/5; 26/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 26/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.826 total time=   2.6s\n",
      "[CV 1/5; 27/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 27/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.820 total time=   1.9s\n",
      "[CV 2/5; 27/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 27/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.833 total time=   1.2s\n",
      "[CV 3/5; 27/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 27/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.828 total time=   0.9s\n",
      "[CV 4/5; 27/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 27/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.828 total time=   1.3s\n",
      "[CV 5/5; 27/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 27/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.827 total time=   2.8s\n",
      "[CV 1/5; 28/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 28/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.824 total time=   4.8s\n",
      "[CV 2/5; 28/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 28/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.833 total time=   2.7s\n",
      "[CV 3/5; 28/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 28/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.828 total time=   4.9s\n",
      "[CV 4/5; 28/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 28/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.827 total time=   2.4s\n",
      "[CV 5/5; 28/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 28/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.828 total time=   3.6s\n",
      "[CV 1/5; 29/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 29/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.822 total time=   0.6s\n",
      "[CV 2/5; 29/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 29/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.823 total time=   0.7s\n",
      "[CV 3/5; 29/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 29/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.818 total time=   1.5s\n",
      "[CV 4/5; 29/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 29/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.820 total time=   1.7s\n",
      "[CV 5/5; 29/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 29/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.822 total time=   0.6s\n",
      "[CV 1/5; 30/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 30/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.823 total time=   3.1s\n",
      "[CV 2/5; 30/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 30/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.825 total time=   3.9s\n",
      "[CV 3/5; 30/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 30/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.818 total time=   2.0s\n",
      "[CV 4/5; 30/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 30/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.821 total time=   3.8s\n",
      "[CV 5/5; 30/60] START classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 30/60] END classifier__criterion=gini, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.826 total time=   1.7s\n",
      "[CV 1/5; 31/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 31/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.815 total time=   1.1s\n",
      "[CV 2/5; 31/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 31/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.809 total time=   1.4s\n",
      "[CV 3/5; 31/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 31/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.814 total time=   1.2s\n",
      "[CV 4/5; 31/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 31/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.809 total time=   0.6s\n",
      "[CV 5/5; 31/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 31/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=200;, score=0.820 total time=   0.6s\n",
      "[CV 1/5; 32/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 32/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.814 total time=   2.7s\n",
      "[CV 2/5; 32/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 32/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.807 total time=   3.6s\n",
      "[CV 3/5; 32/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 32/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.812 total time=   1.7s\n",
      "[CV 4/5; 32/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 32/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.816 total time=   3.3s\n",
      "[CV 5/5; 32/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 32/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=auto, classifier__n_estimators=500;, score=0.816 total time=   1.7s\n",
      "[CV 1/5; 33/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 33/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.817 total time=   0.6s\n",
      "[CV 2/5; 33/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 33/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.805 total time=   1.2s\n",
      "[CV 3/5; 33/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 33/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.813 total time=   1.4s\n",
      "[CV 4/5; 33/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 33/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.810 total time=   1.0s\n",
      "[CV 5/5; 33/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 33/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.814 total time=   0.6s\n",
      "[CV 1/5; 34/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 34/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.814 total time=   3.5s\n",
      "[CV 2/5; 34/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 34/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.807 total time=   1.6s\n",
      "[CV 3/5; 34/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 34/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.814 total time=   3.0s\n",
      "[CV 4/5; 34/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 34/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.810 total time=   2.1s\n",
      "[CV 5/5; 34/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 34/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.812 total time=   3.9s\n",
      "[CV 1/5; 35/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 35/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.778 total time=   1.1s\n",
      "[CV 2/5; 35/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 35/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.780 total time=   1.2s\n",
      "[CV 3/5; 35/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 35/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.782 total time=   1.4s\n",
      "[CV 4/5; 35/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 35/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.771 total time=   1.3s\n",
      "[CV 5/5; 35/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 35/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=200;, score=0.770 total time=   0.8s\n",
      "[CV 1/5; 36/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 36/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.788 total time=   3.2s\n",
      "[CV 2/5; 36/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 36/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.768 total time=   4.7s\n",
      "[CV 3/5; 36/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 36/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.781 total time=   4.0s\n",
      "[CV 4/5; 36/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 36/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.769 total time=   3.0s\n",
      "[CV 5/5; 36/60] START classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 36/60] END classifier__criterion=entropy, classifier__max_depth=4, classifier__max_features=log2, classifier__n_estimators=500;, score=0.769 total time=   1.5s\n",
      "[CV 1/5; 37/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 37/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.822 total time=   0.6s\n",
      "[CV 2/5; 37/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 37/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.817 total time=   1.0s\n",
      "[CV 3/5; 37/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 37/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.814 total time=   1.4s\n",
      "[CV 4/5; 37/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 37/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.820 total time=   1.3s\n",
      "[CV 5/5; 37/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 37/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=200;, score=0.819 total time=   0.8s\n",
      "[CV 1/5; 38/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 38/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.824 total time=   2.5s\n",
      "[CV 2/5; 38/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 38/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.817 total time=   3.7s\n",
      "[CV 3/5; 38/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 38/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.813 total time=   2.0s\n",
      "[CV 4/5; 38/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 38/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.821 total time=   3.3s\n",
      "[CV 5/5; 38/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 38/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=auto, classifier__n_estimators=500;, score=0.822 total time=   2.2s\n",
      "[CV 1/5; 39/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 39/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.819 total time=   0.9s\n",
      "[CV 2/5; 39/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 39/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.820 total time=   2.0s\n",
      "[CV 3/5; 39/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 39/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.813 total time=   2.0s\n",
      "[CV 4/5; 39/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 39/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.818 total time=   1.8s\n",
      "[CV 5/5; 39/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 39/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.822 total time=   1.9s\n",
      "[CV 1/5; 40/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 40/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.824 total time=   2.8s\n",
      "[CV 2/5; 40/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 40/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.815 total time=   2.5s\n",
      "[CV 3/5; 40/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 40/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.813 total time=   2.8s\n",
      "[CV 4/5; 40/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 40/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.820 total time=   2.0s\n",
      "[CV 5/5; 40/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 40/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.822 total time=   4.3s\n",
      "[CV 1/5; 41/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 41/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.815 total time=   0.9s\n",
      "[CV 2/5; 41/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 41/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.796 total time=   0.5s\n",
      "[CV 3/5; 41/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 41/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.810 total time=   0.5s\n",
      "[CV 4/5; 41/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 41/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.805 total time=   0.5s\n",
      "[CV 5/5; 41/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 41/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=200;, score=0.804 total time=   1.2s\n",
      "[CV 1/5; 42/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 42/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.812 total time=   2.6s\n",
      "[CV 2/5; 42/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 42/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.799 total time=   1.4s\n",
      "[CV 3/5; 42/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 42/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.811 total time=   2.9s\n",
      "[CV 4/5; 42/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 42/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.803 total time=   1.7s\n",
      "[CV 5/5; 42/60] START classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 42/60] END classifier__criterion=entropy, classifier__max_depth=5, classifier__max_features=log2, classifier__n_estimators=500;, score=0.801 total time=   1.7s\n",
      "[CV 1/5; 43/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 43/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.819 total time=   1.5s\n",
      "[CV 2/5; 43/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 43/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.825 total time=   1.3s\n",
      "[CV 3/5; 43/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 43/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.815 total time=   0.7s\n",
      "[CV 4/5; 43/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 43/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.822 total time=   0.7s\n",
      "[CV 5/5; 43/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 43/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=200;, score=0.821 total time=   1.2s\n",
      "[CV 1/5; 44/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 44/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.823 total time=   3.1s\n",
      "[CV 2/5; 44/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 44/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.823 total time=   2.6s\n",
      "[CV 3/5; 44/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 44/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.813 total time=   3.1s\n",
      "[CV 4/5; 44/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 44/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.820 total time=   3.1s\n",
      "[CV 5/5; 44/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 44/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=auto, classifier__n_estimators=500;, score=0.825 total time=   2.6s\n",
      "[CV 1/5; 45/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 45/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.825 total time=   0.7s\n",
      "[CV 2/5; 45/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 45/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.825 total time=   1.3s\n",
      "[CV 3/5; 45/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 45/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.814 total time=   1.7s\n",
      "[CV 4/5; 45/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 45/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.820 total time=   1.0s\n",
      "[CV 5/5; 45/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 45/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.825 total time=   0.7s\n",
      "[CV 1/5; 46/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 46/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.820 total time=   2.8s\n",
      "[CV 2/5; 46/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 46/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.820 total time=   3.2s\n",
      "[CV 3/5; 46/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 46/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.812 total time=   3.8s\n",
      "[CV 4/5; 46/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 46/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.822 total time=   1.8s\n",
      "[CV 5/5; 46/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 46/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.825 total time=   3.8s\n",
      "[CV 1/5; 47/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 47/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.821 total time=   0.6s\n",
      "[CV 2/5; 47/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 47/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.805 total time=   0.6s\n",
      "[CV 3/5; 47/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 47/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.815 total time=   0.7s\n",
      "[CV 4/5; 47/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 47/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.812 total time=   1.3s\n",
      "[CV 5/5; 47/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 47/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=200;, score=0.817 total time=   1.3s\n",
      "[CV 1/5; 48/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 48/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.817 total time=   2.3s\n",
      "[CV 2/5; 48/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 48/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.807 total time=   3.1s\n",
      "[CV 3/5; 48/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 48/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.815 total time=   1.8s\n",
      "[CV 4/5; 48/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 48/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.811 total time=   3.3s\n",
      "[CV 5/5; 48/60] START classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 48/60] END classifier__criterion=entropy, classifier__max_depth=6, classifier__max_features=log2, classifier__n_estimators=500;, score=0.818 total time=   2.0s\n",
      "[CV 1/5; 49/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 49/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.819 total time=   0.8s\n",
      "[CV 2/5; 49/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 49/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.828 total time=   2.1s\n",
      "[CV 3/5; 49/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 49/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.822 total time=   2.4s\n",
      "[CV 4/5; 49/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 49/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.824 total time=   2.0s\n",
      "[CV 5/5; 49/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 5/5; 49/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=200;, score=0.822 total time=   1.5s\n",
      "[CV 1/5; 50/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 50/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.822 total time=   3.2s\n",
      "[CV 2/5; 50/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 50/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.827 total time=   5.9s\n",
      "[CV 3/5; 50/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 50/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.819 total time=   4.3s\n",
      "[CV 4/5; 50/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 50/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.822 total time=   3.4s\n",
      "[CV 5/5; 50/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 50/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=auto, classifier__n_estimators=500;, score=0.822 total time=   5.6s\n",
      "[CV 1/5; 51/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 51/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.819 total time=   1.7s\n",
      "[CV 2/5; 51/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 51/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.825 total time=   1.8s\n",
      "[CV 3/5; 51/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 51/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.818 total time=   1.8s\n",
      "[CV 4/5; 51/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 51/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.824 total time=   1.9s\n",
      "[CV 5/5; 51/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 51/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.822 total time=   1.1s\n",
      "[CV 1/5; 52/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 52/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.819 total time=   4.1s\n",
      "[CV 2/5; 52/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 52/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.828 total time=   4.9s\n",
      "[CV 3/5; 52/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 52/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.819 total time=   3.2s\n",
      "[CV 4/5; 52/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 52/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.823 total time=   2.9s\n",
      "[CV 5/5; 52/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 52/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.822 total time=   2.8s\n",
      "[CV 1/5; 53/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 53/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.824 total time=   0.6s\n",
      "[CV 2/5; 53/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 53/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.820 total time=   1.1s\n",
      "[CV 3/5; 53/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 53/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.815 total time=   1.4s\n",
      "[CV 4/5; 53/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 53/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.822 total time=   1.1s\n",
      "[CV 5/5; 53/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 53/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=200;, score=0.821 total time=   0.6s\n",
      "[CV 1/5; 54/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 54/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.823 total time=   1.9s\n",
      "[CV 2/5; 54/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 54/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.815 total time=   3.0s\n",
      "[CV 3/5; 54/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 54/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.813 total time=   1.6s\n",
      "[CV 4/5; 54/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 54/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.820 total time=   3.6s\n",
      "[CV 5/5; 54/60] START classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 54/60] END classifier__criterion=entropy, classifier__max_depth=7, classifier__max_features=log2, classifier__n_estimators=500;, score=0.821 total time=   2.0s\n",
      "[CV 1/5; 55/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 1/5; 55/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.821 total time=   1.7s\n",
      "[CV 2/5; 55/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 2/5; 55/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.828 total time=   2.0s\n",
      "[CV 3/5; 55/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 3/5; 55/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.823 total time=   1.9s\n",
      "[CV 4/5; 55/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n",
      "[CV 4/5; 55/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.825 total time=   1.4s\n",
      "[CV 5/5; 55/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 55/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=200;, score=0.828 total time=   0.8s\n",
      "[CV 1/5; 56/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 1/5; 56/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.824 total time=   3.4s\n",
      "[CV 2/5; 56/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 2/5; 56/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.834 total time=   2.6s\n",
      "[CV 3/5; 56/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 3/5; 56/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.826 total time=   4.0s\n",
      "[CV 4/5; 56/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 4/5; 56/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.825 total time=   2.2s\n",
      "[CV 5/5; 56/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500\n",
      "[CV 5/5; 56/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=auto, classifier__n_estimators=500;, score=0.826 total time=   3.8s\n",
      "[CV 1/5; 57/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 1/5; 57/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.818 total time=   0.8s\n",
      "[CV 2/5; 57/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 2/5; 57/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.833 total time=   0.9s\n",
      "[CV 3/5; 57/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 3/5; 57/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.826 total time=   1.8s\n",
      "[CV 4/5; 57/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 4/5; 57/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.828 total time=   1.5s\n",
      "[CV 5/5; 57/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200\n",
      "[CV 5/5; 57/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=200;, score=0.823 total time=   0.9s\n",
      "[CV 1/5; 58/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 1/5; 58/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.820 total time=   3.8s\n",
      "[CV 2/5; 58/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 2/5; 58/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.831 total time=   2.4s\n",
      "[CV 3/5; 58/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 3/5; 58/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.823 total time=   3.9s\n",
      "[CV 4/5; 58/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 4/5; 58/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.828 total time=   2.5s\n",
      "[CV 5/5; 58/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500\n",
      "[CV 5/5; 58/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=sqrt, classifier__n_estimators=500;, score=0.826 total time=   3.7s\n",
      "[CV 1/5; 59/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 1/5; 59/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.823 total time=   0.7s\n",
      "[CV 2/5; 59/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 2/5; 59/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.823 total time=   0.9s\n",
      "[CV 3/5; 59/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 3/5; 59/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.815 total time=   1.5s\n",
      "[CV 4/5; 59/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 4/5; 59/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.821 total time=   1.4s\n",
      "[CV 5/5; 59/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200\n",
      "[CV 5/5; 59/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=200;, score=0.822 total time=   1.1s\n",
      "[CV 1/5; 60/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 1/5; 60/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.820 total time=   1.7s\n",
      "[CV 2/5; 60/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 2/5; 60/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.823 total time=   3.4s\n",
      "[CV 3/5; 60/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 3/5; 60/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.815 total time=   1.7s\n",
      "[CV 4/5; 60/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 4/5; 60/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.822 total time=   3.1s\n",
      "[CV 5/5; 60/60] START classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500\n",
      "[CV 5/5; 60/60] END classifier__criterion=entropy, classifier__max_depth=8, classifier__max_features=log2, classifier__n_estimators=500;, score=0.822 total time=   2.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=Pipeline(steps=[('preprocessing',\n",
       "                                        FeatureUnion(transformer_list=[('num_pipeline',\n",
       "                                                                        Pipeline(steps=[('select_numeric',\n",
       "                                                                                         DataFrameSelector(attribute_names=['age',\n",
       "                                                                                                                            'fnlwgt',\n",
       "                                                                                                                            'education-num',\n",
       "                                                                                                                            'hours-per-week'])),\n",
       "                                                                                        ('imputer',\n",
       "                                                                                         SimpleImputer(strategy='median'))])),\n",
       "                                                                       ('cat_pipeline',...\n",
       "                                                                                                                            'native-country',\n",
       "                                                                                                                            'sex'])),\n",
       "                                                                                        ('imputer',\n",
       "                                                                                         MostFrequentImputer()),\n",
       "                                                                                        ('cat_encoder',\n",
       "                                                                                         OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                                       sparse=False))]))])),\n",
       "                                       ('classifier',\n",
       "                                        RandomForestClassifier())]),\n",
       "             param_grid={'classifier__criterion': ['gini', 'entropy'],\n",
       "                         'classifier__max_depth': [4, 5, 6, 7, 8],\n",
       "                         'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'classifier__n_estimators': [200, 500]},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline),\n",
    "    ('classifier', RandomForestClassifier())])\n",
    "\n",
    "param_grid = { \n",
    "    'classifier__n_estimators': [200, 500],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'classifier__max_depth': [4,5,6,7,8],\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "forest = GridSearchCV(pipe, param_grid, cv=kfold, verbose=10)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c47ae60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV 1/5; 1/36] START classifier__C=0.001, classifier__gamma=0.001...............\n",
      "[CV 1/5; 1/36] END classifier__C=0.001, classifier__gamma=0.001;, score=0.757 total time=   0.5s\n",
      "[CV 2/5; 1/36] START classifier__C=0.001, classifier__gamma=0.001...............\n",
      "[CV 2/5; 1/36] END classifier__C=0.001, classifier__gamma=0.001;, score=0.757 total time=   0.5s\n",
      "[CV 3/5; 1/36] START classifier__C=0.001, classifier__gamma=0.001...............\n",
      "[CV 3/5; 1/36] END classifier__C=0.001, classifier__gamma=0.001;, score=0.758 total time=   0.5s\n",
      "[CV 4/5; 1/36] START classifier__C=0.001, classifier__gamma=0.001...............\n",
      "[CV 4/5; 1/36] END classifier__C=0.001, classifier__gamma=0.001;, score=0.758 total time=   0.5s\n",
      "[CV 5/5; 1/36] START classifier__C=0.001, classifier__gamma=0.001...............\n",
      "[CV 5/5; 1/36] END classifier__C=0.001, classifier__gamma=0.001;, score=0.758 total time=   0.7s\n",
      "[CV 1/5; 2/36] START classifier__C=0.001, classifier__gamma=0.01................\n",
      "[CV 1/5; 2/36] END classifier__C=0.001, classifier__gamma=0.01;, score=0.757 total time=   1.6s\n",
      "[CV 2/5; 2/36] START classifier__C=0.001, classifier__gamma=0.01................\n",
      "[CV 2/5; 2/36] END classifier__C=0.001, classifier__gamma=0.01;, score=0.757 total time=   0.8s\n",
      "[CV 3/5; 2/36] START classifier__C=0.001, classifier__gamma=0.01................\n",
      "[CV 3/5; 2/36] END classifier__C=0.001, classifier__gamma=0.01;, score=0.758 total time=   0.6s\n",
      "[CV 4/5; 2/36] START classifier__C=0.001, classifier__gamma=0.01................\n",
      "[CV 4/5; 2/36] END classifier__C=0.001, classifier__gamma=0.01;, score=0.758 total time=   0.6s\n",
      "[CV 5/5; 2/36] START classifier__C=0.001, classifier__gamma=0.01................\n",
      "[CV 5/5; 2/36] END classifier__C=0.001, classifier__gamma=0.01;, score=0.758 total time=   0.7s\n",
      "[CV 1/5; 3/36] START classifier__C=0.001, classifier__gamma=0.1.................\n",
      "[CV 1/5; 3/36] END classifier__C=0.001, classifier__gamma=0.1;, score=0.757 total time=   1.3s\n",
      "[CV 2/5; 3/36] START classifier__C=0.001, classifier__gamma=0.1.................\n",
      "[CV 2/5; 3/36] END classifier__C=0.001, classifier__gamma=0.1;, score=0.757 total time=   1.6s\n",
      "[CV 3/5; 3/36] START classifier__C=0.001, classifier__gamma=0.1.................\n",
      "[CV 3/5; 3/36] END classifier__C=0.001, classifier__gamma=0.1;, score=0.758 total time=   0.8s\n",
      "[CV 4/5; 3/36] START classifier__C=0.001, classifier__gamma=0.1.................\n",
      "[CV 4/5; 3/36] END classifier__C=0.001, classifier__gamma=0.1;, score=0.758 total time=   0.8s\n",
      "[CV 5/5; 3/36] START classifier__C=0.001, classifier__gamma=0.1.................\n",
      "[CV 5/5; 3/36] END classifier__C=0.001, classifier__gamma=0.1;, score=0.758 total time=   0.8s\n",
      "[CV 1/5; 4/36] START classifier__C=0.001, classifier__gamma=1...................\n",
      "[CV 1/5; 4/36] END classifier__C=0.001, classifier__gamma=1;, score=0.757 total time=   1.3s\n",
      "[CV 2/5; 4/36] START classifier__C=0.001, classifier__gamma=1...................\n",
      "[CV 2/5; 4/36] END classifier__C=0.001, classifier__gamma=1;, score=0.757 total time=   1.5s\n",
      "[CV 3/5; 4/36] START classifier__C=0.001, classifier__gamma=1...................\n",
      "[CV 3/5; 4/36] END classifier__C=0.001, classifier__gamma=1;, score=0.758 total time=   0.7s\n",
      "[CV 4/5; 4/36] START classifier__C=0.001, classifier__gamma=1...................\n",
      "[CV 4/5; 4/36] END classifier__C=0.001, classifier__gamma=1;, score=0.758 total time=   0.7s\n",
      "[CV 5/5; 4/36] START classifier__C=0.001, classifier__gamma=1...................\n",
      "[CV 5/5; 4/36] END classifier__C=0.001, classifier__gamma=1;, score=0.758 total time=   0.8s\n",
      "[CV 1/5; 5/36] START classifier__C=0.001, classifier__gamma=10..................\n",
      "[CV 1/5; 5/36] END classifier__C=0.001, classifier__gamma=10;, score=0.757 total time=   1.1s\n",
      "[CV 2/5; 5/36] START classifier__C=0.001, classifier__gamma=10..................\n",
      "[CV 2/5; 5/36] END classifier__C=0.001, classifier__gamma=10;, score=0.757 total time=   1.7s\n",
      "[CV 3/5; 5/36] START classifier__C=0.001, classifier__gamma=10..................\n",
      "[CV 3/5; 5/36] END classifier__C=0.001, classifier__gamma=10;, score=0.758 total time=   0.8s\n",
      "[CV 4/5; 5/36] START classifier__C=0.001, classifier__gamma=10..................\n",
      "[CV 4/5; 5/36] END classifier__C=0.001, classifier__gamma=10;, score=0.758 total time=   0.8s\n",
      "[CV 5/5; 5/36] START classifier__C=0.001, classifier__gamma=10..................\n",
      "[CV 5/5; 5/36] END classifier__C=0.001, classifier__gamma=10;, score=0.758 total time=   0.8s\n",
      "[CV 1/5; 6/36] START classifier__C=0.001, classifier__gamma=100.................\n",
      "[CV 1/5; 6/36] END classifier__C=0.001, classifier__gamma=100;, score=0.757 total time=   1.2s\n",
      "[CV 2/5; 6/36] START classifier__C=0.001, classifier__gamma=100.................\n",
      "[CV 2/5; 6/36] END classifier__C=0.001, classifier__gamma=100;, score=0.757 total time=   1.7s\n",
      "[CV 3/5; 6/36] START classifier__C=0.001, classifier__gamma=100.................\n",
      "[CV 3/5; 6/36] END classifier__C=0.001, classifier__gamma=100;, score=0.758 total time=   0.8s\n",
      "[CV 4/5; 6/36] START classifier__C=0.001, classifier__gamma=100.................\n",
      "[CV 4/5; 6/36] END classifier__C=0.001, classifier__gamma=100;, score=0.758 total time=   0.8s\n",
      "[CV 5/5; 6/36] START classifier__C=0.001, classifier__gamma=100.................\n",
      "[CV 5/5; 6/36] END classifier__C=0.001, classifier__gamma=100;, score=0.758 total time=   1.5s\n",
      "[CV 1/5; 7/36] START classifier__C=0.01, classifier__gamma=0.001................\n",
      "[CV 1/5; 7/36] END classifier__C=0.01, classifier__gamma=0.001;, score=0.757 total time=   2.2s\n",
      "[CV 2/5; 7/36] START classifier__C=0.01, classifier__gamma=0.001................\n",
      "[CV 2/5; 7/36] END classifier__C=0.01, classifier__gamma=0.001;, score=0.757 total time=   1.0s\n",
      "[CV 3/5; 7/36] START classifier__C=0.01, classifier__gamma=0.001................\n",
      "[CV 3/5; 7/36] END classifier__C=0.01, classifier__gamma=0.001;, score=0.758 total time=   1.1s\n",
      "[CV 4/5; 7/36] START classifier__C=0.01, classifier__gamma=0.001................\n",
      "[CV 4/5; 7/36] END classifier__C=0.01, classifier__gamma=0.001;, score=0.758 total time=   2.3s\n",
      "[CV 5/5; 7/36] START classifier__C=0.01, classifier__gamma=0.001................\n",
      "[CV 5/5; 7/36] END classifier__C=0.01, classifier__gamma=0.001;, score=0.758 total time=   1.5s\n",
      "[CV 1/5; 8/36] START classifier__C=0.01, classifier__gamma=0.01.................\n",
      "[CV 1/5; 8/36] END classifier__C=0.01, classifier__gamma=0.01;, score=0.757 total time=   1.1s\n",
      "[CV 2/5; 8/36] START classifier__C=0.01, classifier__gamma=0.01.................\n",
      "[CV 2/5; 8/36] END classifier__C=0.01, classifier__gamma=0.01;, score=0.757 total time=   2.1s\n",
      "[CV 3/5; 8/36] START classifier__C=0.01, classifier__gamma=0.01.................\n",
      "[CV 3/5; 8/36] END classifier__C=0.01, classifier__gamma=0.01;, score=0.758 total time=   1.3s\n",
      "[CV 4/5; 8/36] START classifier__C=0.01, classifier__gamma=0.01.................\n",
      "[CV 4/5; 8/36] END classifier__C=0.01, classifier__gamma=0.01;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 8/36] START classifier__C=0.01, classifier__gamma=0.01.................\n",
      "[CV 5/5; 8/36] END classifier__C=0.01, classifier__gamma=0.01;, score=0.758 total time=   2.2s\n",
      "[CV 1/5; 9/36] START classifier__C=0.01, classifier__gamma=0.1..................\n",
      "[CV 1/5; 9/36] END classifier__C=0.01, classifier__gamma=0.1;, score=0.757 total time=   1.7s\n",
      "[CV 2/5; 9/36] START classifier__C=0.01, classifier__gamma=0.1..................\n",
      "[CV 2/5; 9/36] END classifier__C=0.01, classifier__gamma=0.1;, score=0.757 total time=   1.1s\n",
      "[CV 3/5; 9/36] START classifier__C=0.01, classifier__gamma=0.1..................\n",
      "[CV 3/5; 9/36] END classifier__C=0.01, classifier__gamma=0.1;, score=0.758 total time=   1.8s\n",
      "[CV 4/5; 9/36] START classifier__C=0.01, classifier__gamma=0.1..................\n",
      "[CV 4/5; 9/36] END classifier__C=0.01, classifier__gamma=0.1;, score=0.758 total time=   2.1s\n",
      "[CV 5/5; 9/36] START classifier__C=0.01, classifier__gamma=0.1..................\n",
      "[CV 5/5; 9/36] END classifier__C=0.01, classifier__gamma=0.1;, score=0.758 total time=   1.1s\n",
      "[CV 1/5; 10/36] START classifier__C=0.01, classifier__gamma=1...................\n",
      "[CV 1/5; 10/36] END classifier__C=0.01, classifier__gamma=1;, score=0.757 total time=   1.4s\n",
      "[CV 2/5; 10/36] START classifier__C=0.01, classifier__gamma=1...................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 10/36] END classifier__C=0.01, classifier__gamma=1;, score=0.757 total time=   2.6s\n",
      "[CV 3/5; 10/36] START classifier__C=0.01, classifier__gamma=1...................\n",
      "[CV 3/5; 10/36] END classifier__C=0.01, classifier__gamma=1;, score=0.758 total time=   1.1s\n",
      "[CV 4/5; 10/36] START classifier__C=0.01, classifier__gamma=1...................\n",
      "[CV 4/5; 10/36] END classifier__C=0.01, classifier__gamma=1;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 10/36] START classifier__C=0.01, classifier__gamma=1...................\n",
      "[CV 5/5; 10/36] END classifier__C=0.01, classifier__gamma=1;, score=0.758 total time=   2.4s\n",
      "[CV 1/5; 11/36] START classifier__C=0.01, classifier__gamma=10..................\n",
      "[CV 1/5; 11/36] END classifier__C=0.01, classifier__gamma=10;, score=0.757 total time=   1.6s\n",
      "[CV 2/5; 11/36] START classifier__C=0.01, classifier__gamma=10..................\n",
      "[CV 2/5; 11/36] END classifier__C=0.01, classifier__gamma=10;, score=0.757 total time=   1.1s\n",
      "[CV 3/5; 11/36] START classifier__C=0.01, classifier__gamma=10..................\n",
      "[CV 3/5; 11/36] END classifier__C=0.01, classifier__gamma=10;, score=0.758 total time=   2.0s\n",
      "[CV 4/5; 11/36] START classifier__C=0.01, classifier__gamma=10..................\n",
      "[CV 4/5; 11/36] END classifier__C=0.01, classifier__gamma=10;, score=0.758 total time=   1.4s\n",
      "[CV 5/5; 11/36] START classifier__C=0.01, classifier__gamma=10..................\n",
      "[CV 5/5; 11/36] END classifier__C=0.01, classifier__gamma=10;, score=0.758 total time=   1.1s\n",
      "[CV 1/5; 12/36] START classifier__C=0.01, classifier__gamma=100.................\n",
      "[CV 1/5; 12/36] END classifier__C=0.01, classifier__gamma=100;, score=0.757 total time=   2.2s\n",
      "[CV 2/5; 12/36] START classifier__C=0.01, classifier__gamma=100.................\n",
      "[CV 2/5; 12/36] END classifier__C=0.01, classifier__gamma=100;, score=0.757 total time=   1.8s\n",
      "[CV 3/5; 12/36] START classifier__C=0.01, classifier__gamma=100.................\n",
      "[CV 3/5; 12/36] END classifier__C=0.01, classifier__gamma=100;, score=0.758 total time=   1.0s\n",
      "[CV 4/5; 12/36] START classifier__C=0.01, classifier__gamma=100.................\n",
      "[CV 4/5; 12/36] END classifier__C=0.01, classifier__gamma=100;, score=0.758 total time=   1.7s\n",
      "[CV 5/5; 12/36] START classifier__C=0.01, classifier__gamma=100.................\n",
      "[CV 5/5; 12/36] END classifier__C=0.01, classifier__gamma=100;, score=0.758 total time=   2.2s\n",
      "[CV 1/5; 13/36] START classifier__C=0.1, classifier__gamma=0.001................\n",
      "[CV 1/5; 13/36] END classifier__C=0.1, classifier__gamma=0.001;, score=0.757 total time=   1.0s\n",
      "[CV 2/5; 13/36] START classifier__C=0.1, classifier__gamma=0.001................\n",
      "[CV 2/5; 13/36] END classifier__C=0.1, classifier__gamma=0.001;, score=0.757 total time=   1.3s\n",
      "[CV 3/5; 13/36] START classifier__C=0.1, classifier__gamma=0.001................\n",
      "[CV 3/5; 13/36] END classifier__C=0.1, classifier__gamma=0.001;, score=0.758 total time=   2.4s\n",
      "[CV 4/5; 13/36] START classifier__C=0.1, classifier__gamma=0.001................\n",
      "[CV 4/5; 13/36] END classifier__C=0.1, classifier__gamma=0.001;, score=0.758 total time=   1.5s\n",
      "[CV 5/5; 13/36] START classifier__C=0.1, classifier__gamma=0.001................\n",
      "[CV 5/5; 13/36] END classifier__C=0.1, classifier__gamma=0.001;, score=0.758 total time=   1.2s\n",
      "[CV 1/5; 14/36] START classifier__C=0.1, classifier__gamma=0.01.................\n",
      "[CV 1/5; 14/36] END classifier__C=0.1, classifier__gamma=0.01;, score=0.757 total time=   2.8s\n",
      "[CV 2/5; 14/36] START classifier__C=0.1, classifier__gamma=0.01.................\n",
      "[CV 2/5; 14/36] END classifier__C=0.1, classifier__gamma=0.01;, score=0.757 total time=   1.3s\n",
      "[CV 3/5; 14/36] START classifier__C=0.1, classifier__gamma=0.01.................\n",
      "[CV 3/5; 14/36] END classifier__C=0.1, classifier__gamma=0.01;, score=0.758 total time=   1.1s\n",
      "[CV 4/5; 14/36] START classifier__C=0.1, classifier__gamma=0.01.................\n",
      "[CV 4/5; 14/36] END classifier__C=0.1, classifier__gamma=0.01;, score=0.758 total time=   2.3s\n",
      "[CV 5/5; 14/36] START classifier__C=0.1, classifier__gamma=0.01.................\n",
      "[CV 5/5; 14/36] END classifier__C=0.1, classifier__gamma=0.01;, score=0.758 total time=   1.7s\n",
      "[CV 1/5; 15/36] START classifier__C=0.1, classifier__gamma=0.1..................\n",
      "[CV 1/5; 15/36] END classifier__C=0.1, classifier__gamma=0.1;, score=0.757 total time=   1.1s\n",
      "[CV 2/5; 15/36] START classifier__C=0.1, classifier__gamma=0.1..................\n",
      "[CV 2/5; 15/36] END classifier__C=0.1, classifier__gamma=0.1;, score=0.757 total time=   2.0s\n",
      "[CV 3/5; 15/36] START classifier__C=0.1, classifier__gamma=0.1..................\n",
      "[CV 3/5; 15/36] END classifier__C=0.1, classifier__gamma=0.1;, score=0.758 total time=   2.0s\n",
      "[CV 4/5; 15/36] START classifier__C=0.1, classifier__gamma=0.1..................\n",
      "[CV 4/5; 15/36] END classifier__C=0.1, classifier__gamma=0.1;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 15/36] START classifier__C=0.1, classifier__gamma=0.1..................\n",
      "[CV 5/5; 15/36] END classifier__C=0.1, classifier__gamma=0.1;, score=0.758 total time=   1.8s\n",
      "[CV 1/5; 16/36] START classifier__C=0.1, classifier__gamma=1....................\n",
      "[CV 1/5; 16/36] END classifier__C=0.1, classifier__gamma=1;, score=0.757 total time=   2.3s\n",
      "[CV 2/5; 16/36] START classifier__C=0.1, classifier__gamma=1....................\n",
      "[CV 2/5; 16/36] END classifier__C=0.1, classifier__gamma=1;, score=0.757 total time=   1.1s\n",
      "[CV 3/5; 16/36] START classifier__C=0.1, classifier__gamma=1....................\n",
      "[CV 3/5; 16/36] END classifier__C=0.1, classifier__gamma=1;, score=0.758 total time=   1.4s\n",
      "[CV 4/5; 16/36] START classifier__C=0.1, classifier__gamma=1....................\n",
      "[CV 4/5; 16/36] END classifier__C=0.1, classifier__gamma=1;, score=0.758 total time=   2.4s\n",
      "[CV 5/5; 16/36] START classifier__C=0.1, classifier__gamma=1....................\n",
      "[CV 5/5; 16/36] END classifier__C=0.1, classifier__gamma=1;, score=0.758 total time=   1.2s\n",
      "[CV 1/5; 17/36] START classifier__C=0.1, classifier__gamma=10...................\n",
      "[CV 1/5; 17/36] END classifier__C=0.1, classifier__gamma=10;, score=0.757 total time=   1.1s\n",
      "[CV 2/5; 17/36] START classifier__C=0.1, classifier__gamma=10...................\n",
      "[CV 2/5; 17/36] END classifier__C=0.1, classifier__gamma=10;, score=0.757 total time=   2.5s\n",
      "[CV 3/5; 17/36] START classifier__C=0.1, classifier__gamma=10...................\n",
      "[CV 3/5; 17/36] END classifier__C=0.1, classifier__gamma=10;, score=0.758 total time=   1.6s\n",
      "[CV 4/5; 17/36] START classifier__C=0.1, classifier__gamma=10...................\n",
      "[CV 4/5; 17/36] END classifier__C=0.1, classifier__gamma=10;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 17/36] START classifier__C=0.1, classifier__gamma=10...................\n",
      "[CV 5/5; 17/36] END classifier__C=0.1, classifier__gamma=10;, score=0.758 total time=   2.2s\n",
      "[CV 1/5; 18/36] START classifier__C=0.1, classifier__gamma=100..................\n",
      "[CV 1/5; 18/36] END classifier__C=0.1, classifier__gamma=100;, score=0.757 total time=   1.8s\n",
      "[CV 2/5; 18/36] START classifier__C=0.1, classifier__gamma=100..................\n",
      "[CV 2/5; 18/36] END classifier__C=0.1, classifier__gamma=100;, score=0.757 total time=   1.1s\n",
      "[CV 3/5; 18/36] START classifier__C=0.1, classifier__gamma=100..................\n",
      "[CV 3/5; 18/36] END classifier__C=0.1, classifier__gamma=100;, score=0.758 total time=   1.8s\n",
      "[CV 4/5; 18/36] START classifier__C=0.1, classifier__gamma=100..................\n",
      "[CV 4/5; 18/36] END classifier__C=0.1, classifier__gamma=100;, score=0.758 total time=   2.2s\n",
      "[CV 5/5; 18/36] START classifier__C=0.1, classifier__gamma=100..................\n",
      "[CV 5/5; 18/36] END classifier__C=0.1, classifier__gamma=100;, score=0.758 total time=   1.1s\n",
      "[CV 1/5; 19/36] START classifier__C=1, classifier__gamma=0.001..................\n",
      "[CV 1/5; 19/36] END classifier__C=1, classifier__gamma=0.001;, score=0.750 total time=   1.4s\n",
      "[CV 2/5; 19/36] START classifier__C=1, classifier__gamma=0.001..................\n",
      "[CV 2/5; 19/36] END classifier__C=1, classifier__gamma=0.001;, score=0.751 total time=   2.6s\n",
      "[CV 3/5; 19/36] START classifier__C=1, classifier__gamma=0.001..................\n",
      "[CV 3/5; 19/36] END classifier__C=1, classifier__gamma=0.001;, score=0.757 total time=   1.3s\n",
      "[CV 4/5; 19/36] START classifier__C=1, classifier__gamma=0.001..................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 19/36] END classifier__C=1, classifier__gamma=0.001;, score=0.758 total time=   1.3s\n",
      "[CV 5/5; 19/36] START classifier__C=1, classifier__gamma=0.001..................\n",
      "[CV 5/5; 19/36] END classifier__C=1, classifier__gamma=0.001;, score=0.761 total time=   2.5s\n",
      "[CV 1/5; 20/36] START classifier__C=1, classifier__gamma=0.01...................\n",
      "[CV 1/5; 20/36] END classifier__C=1, classifier__gamma=0.01;, score=0.760 total time=   1.3s\n",
      "[CV 2/5; 20/36] START classifier__C=1, classifier__gamma=0.01...................\n",
      "[CV 2/5; 20/36] END classifier__C=1, classifier__gamma=0.01;, score=0.761 total time=   1.1s\n",
      "[CV 3/5; 20/36] START classifier__C=1, classifier__gamma=0.01...................\n",
      "[CV 3/5; 20/36] END classifier__C=1, classifier__gamma=0.01;, score=0.762 total time=   2.3s\n",
      "[CV 4/5; 20/36] START classifier__C=1, classifier__gamma=0.01...................\n",
      "[CV 4/5; 20/36] END classifier__C=1, classifier__gamma=0.01;, score=0.759 total time=   1.7s\n",
      "[CV 5/5; 20/36] START classifier__C=1, classifier__gamma=0.01...................\n",
      "[CV 5/5; 20/36] END classifier__C=1, classifier__gamma=0.01;, score=0.757 total time=   1.1s\n",
      "[CV 1/5; 21/36] START classifier__C=1, classifier__gamma=0.1....................\n",
      "[CV 1/5; 21/36] END classifier__C=1, classifier__gamma=0.1;, score=0.757 total time=   2.5s\n",
      "[CV 2/5; 21/36] START classifier__C=1, classifier__gamma=0.1....................\n",
      "[CV 2/5; 21/36] END classifier__C=1, classifier__gamma=0.1;, score=0.757 total time=   2.0s\n",
      "[CV 3/5; 21/36] START classifier__C=1, classifier__gamma=0.1....................\n",
      "[CV 3/5; 21/36] END classifier__C=1, classifier__gamma=0.1;, score=0.758 total time=   1.2s\n",
      "[CV 4/5; 21/36] START classifier__C=1, classifier__gamma=0.1....................\n",
      "[CV 4/5; 21/36] END classifier__C=1, classifier__gamma=0.1;, score=0.758 total time=   2.5s\n",
      "[CV 5/5; 21/36] START classifier__C=1, classifier__gamma=0.1....................\n",
      "[CV 5/5; 21/36] END classifier__C=1, classifier__gamma=0.1;, score=0.758 total time=   1.7s\n",
      "[CV 1/5; 22/36] START classifier__C=1, classifier__gamma=1......................\n",
      "[CV 1/5; 22/36] END classifier__C=1, classifier__gamma=1;, score=0.757 total time=   1.2s\n",
      "[CV 2/5; 22/36] START classifier__C=1, classifier__gamma=1......................\n",
      "[CV 2/5; 22/36] END classifier__C=1, classifier__gamma=1;, score=0.757 total time=   2.4s\n",
      "[CV 3/5; 22/36] START classifier__C=1, classifier__gamma=1......................\n",
      "[CV 3/5; 22/36] END classifier__C=1, classifier__gamma=1;, score=0.758 total time=   1.8s\n",
      "[CV 4/5; 22/36] START classifier__C=1, classifier__gamma=1......................\n",
      "[CV 4/5; 22/36] END classifier__C=1, classifier__gamma=1;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 22/36] START classifier__C=1, classifier__gamma=1......................\n",
      "[CV 5/5; 22/36] END classifier__C=1, classifier__gamma=1;, score=0.758 total time=   2.5s\n",
      "[CV 1/5; 23/36] START classifier__C=1, classifier__gamma=10.....................\n",
      "[CV 1/5; 23/36] END classifier__C=1, classifier__gamma=10;, score=0.757 total time=   1.8s\n",
      "[CV 2/5; 23/36] START classifier__C=1, classifier__gamma=10.....................\n",
      "[CV 2/5; 23/36] END classifier__C=1, classifier__gamma=10;, score=0.757 total time=   1.2s\n",
      "[CV 3/5; 23/36] START classifier__C=1, classifier__gamma=10.....................\n",
      "[CV 3/5; 23/36] END classifier__C=1, classifier__gamma=10;, score=0.758 total time=   2.3s\n",
      "[CV 4/5; 23/36] START classifier__C=1, classifier__gamma=10.....................\n",
      "[CV 4/5; 23/36] END classifier__C=1, classifier__gamma=10;, score=0.758 total time=   1.8s\n",
      "[CV 5/5; 23/36] START classifier__C=1, classifier__gamma=10.....................\n",
      "[CV 5/5; 23/36] END classifier__C=1, classifier__gamma=10;, score=0.758 total time=   1.1s\n",
      "[CV 1/5; 24/36] START classifier__C=1, classifier__gamma=100....................\n",
      "[CV 1/5; 24/36] END classifier__C=1, classifier__gamma=100;, score=0.757 total time=   2.2s\n",
      "[CV 2/5; 24/36] START classifier__C=1, classifier__gamma=100....................\n",
      "[CV 2/5; 24/36] END classifier__C=1, classifier__gamma=100;, score=0.757 total time=   2.0s\n",
      "[CV 3/5; 24/36] START classifier__C=1, classifier__gamma=100....................\n",
      "[CV 3/5; 24/36] END classifier__C=1, classifier__gamma=100;, score=0.758 total time=   1.2s\n",
      "[CV 4/5; 24/36] START classifier__C=1, classifier__gamma=100....................\n",
      "[CV 4/5; 24/36] END classifier__C=1, classifier__gamma=100;, score=0.758 total time=   2.2s\n",
      "[CV 5/5; 24/36] START classifier__C=1, classifier__gamma=100....................\n",
      "[CV 5/5; 24/36] END classifier__C=1, classifier__gamma=100;, score=0.758 total time=   2.0s\n",
      "[CV 1/5; 25/36] START classifier__C=10, classifier__gamma=0.001.................\n",
      "[CV 1/5; 25/36] END classifier__C=10, classifier__gamma=0.001;, score=0.737 total time=   1.1s\n",
      "[CV 2/5; 25/36] START classifier__C=10, classifier__gamma=0.001.................\n",
      "[CV 2/5; 25/36] END classifier__C=10, classifier__gamma=0.001;, score=0.707 total time=   2.0s\n",
      "[CV 3/5; 25/36] START classifier__C=10, classifier__gamma=0.001.................\n",
      "[CV 3/5; 25/36] END classifier__C=10, classifier__gamma=0.001;, score=0.732 total time=   2.1s\n",
      "[CV 4/5; 25/36] START classifier__C=10, classifier__gamma=0.001.................\n",
      "[CV 4/5; 25/36] END classifier__C=10, classifier__gamma=0.001;, score=0.735 total time=   1.1s\n",
      "[CV 5/5; 25/36] START classifier__C=10, classifier__gamma=0.001.................\n",
      "[CV 5/5; 25/36] END classifier__C=10, classifier__gamma=0.001;, score=0.746 total time=   1.9s\n",
      "[CV 1/5; 26/36] START classifier__C=10, classifier__gamma=0.01..................\n",
      "[CV 1/5; 26/36] END classifier__C=10, classifier__gamma=0.01;, score=0.764 total time=   2.3s\n",
      "[CV 2/5; 26/36] START classifier__C=10, classifier__gamma=0.01..................\n",
      "[CV 2/5; 26/36] END classifier__C=10, classifier__gamma=0.01;, score=0.764 total time=   1.2s\n",
      "[CV 3/5; 26/36] START classifier__C=10, classifier__gamma=0.01..................\n",
      "[CV 3/5; 26/36] END classifier__C=10, classifier__gamma=0.01;, score=0.762 total time=   2.0s\n",
      "[CV 4/5; 26/36] START classifier__C=10, classifier__gamma=0.01..................\n",
      "[CV 4/5; 26/36] END classifier__C=10, classifier__gamma=0.01;, score=0.761 total time=   2.2s\n",
      "[CV 5/5; 26/36] START classifier__C=10, classifier__gamma=0.01..................\n",
      "[CV 5/5; 26/36] END classifier__C=10, classifier__gamma=0.01;, score=0.758 total time=   1.1s\n",
      "[CV 1/5; 27/36] START classifier__C=10, classifier__gamma=0.1...................\n",
      "[CV 1/5; 27/36] END classifier__C=10, classifier__gamma=0.1;, score=0.757 total time=   1.8s\n",
      "[CV 2/5; 27/36] START classifier__C=10, classifier__gamma=0.1...................\n",
      "[CV 2/5; 27/36] END classifier__C=10, classifier__gamma=0.1;, score=0.758 total time=   2.3s\n",
      "[CV 3/5; 27/36] START classifier__C=10, classifier__gamma=0.1...................\n",
      "[CV 3/5; 27/36] END classifier__C=10, classifier__gamma=0.1;, score=0.759 total time=   1.1s\n",
      "[CV 4/5; 27/36] START classifier__C=10, classifier__gamma=0.1...................\n",
      "[CV 4/5; 27/36] END classifier__C=10, classifier__gamma=0.1;, score=0.758 total time=   1.6s\n",
      "[CV 5/5; 27/36] START classifier__C=10, classifier__gamma=0.1...................\n",
      "[CV 5/5; 27/36] END classifier__C=10, classifier__gamma=0.1;, score=0.758 total time=   2.5s\n",
      "[CV 1/5; 28/36] START classifier__C=10, classifier__gamma=1.....................\n",
      "[CV 1/5; 28/36] END classifier__C=10, classifier__gamma=1;, score=0.757 total time=   1.3s\n",
      "[CV 2/5; 28/36] START classifier__C=10, classifier__gamma=1.....................\n",
      "[CV 2/5; 28/36] END classifier__C=10, classifier__gamma=1;, score=0.757 total time=   1.8s\n",
      "[CV 3/5; 28/36] START classifier__C=10, classifier__gamma=1.....................\n",
      "[CV 3/5; 28/36] END classifier__C=10, classifier__gamma=1;, score=0.758 total time=   2.4s\n",
      "[CV 4/5; 28/36] START classifier__C=10, classifier__gamma=1.....................\n",
      "[CV 4/5; 28/36] END classifier__C=10, classifier__gamma=1;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 28/36] START classifier__C=10, classifier__gamma=1.....................\n",
      "[CV 5/5; 28/36] END classifier__C=10, classifier__gamma=1;, score=0.758 total time=   1.7s\n",
      "[CV 1/5; 29/36] START classifier__C=10, classifier__gamma=10....................\n",
      "[CV 1/5; 29/36] END classifier__C=10, classifier__gamma=10;, score=0.757 total time=   2.5s\n",
      "[CV 2/5; 29/36] START classifier__C=10, classifier__gamma=10....................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 29/36] END classifier__C=10, classifier__gamma=10;, score=0.757 total time=   1.1s\n",
      "[CV 3/5; 29/36] START classifier__C=10, classifier__gamma=10....................\n",
      "[CV 3/5; 29/36] END classifier__C=10, classifier__gamma=10;, score=0.758 total time=   1.5s\n",
      "[CV 4/5; 29/36] START classifier__C=10, classifier__gamma=10....................\n",
      "[CV 4/5; 29/36] END classifier__C=10, classifier__gamma=10;, score=0.758 total time=   2.7s\n",
      "[CV 5/5; 29/36] START classifier__C=10, classifier__gamma=10....................\n",
      "[CV 5/5; 29/36] END classifier__C=10, classifier__gamma=10;, score=0.758 total time=   1.2s\n",
      "[CV 1/5; 30/36] START classifier__C=10, classifier__gamma=100...................\n",
      "[CV 1/5; 30/36] END classifier__C=10, classifier__gamma=100;, score=0.757 total time=   1.4s\n",
      "[CV 2/5; 30/36] START classifier__C=10, classifier__gamma=100...................\n",
      "[CV 2/5; 30/36] END classifier__C=10, classifier__gamma=100;, score=0.757 total time=   2.6s\n",
      "[CV 3/5; 30/36] START classifier__C=10, classifier__gamma=100...................\n",
      "[CV 3/5; 30/36] END classifier__C=10, classifier__gamma=100;, score=0.758 total time=   1.2s\n",
      "[CV 4/5; 30/36] START classifier__C=10, classifier__gamma=100...................\n",
      "[CV 4/5; 30/36] END classifier__C=10, classifier__gamma=100;, score=0.758 total time=   1.3s\n",
      "[CV 5/5; 30/36] START classifier__C=10, classifier__gamma=100...................\n",
      "[CV 5/5; 30/36] END classifier__C=10, classifier__gamma=100;, score=0.758 total time=   2.6s\n",
      "[CV 1/5; 31/36] START classifier__C=100, classifier__gamma=0.001................\n",
      "[CV 1/5; 31/36] END classifier__C=100, classifier__gamma=0.001;, score=0.737 total time=   1.4s\n",
      "[CV 2/5; 31/36] START classifier__C=100, classifier__gamma=0.001................\n",
      "[CV 2/5; 31/36] END classifier__C=100, classifier__gamma=0.001;, score=0.706 total time=   1.2s\n",
      "[CV 3/5; 31/36] START classifier__C=100, classifier__gamma=0.001................\n",
      "[CV 3/5; 31/36] END classifier__C=100, classifier__gamma=0.001;, score=0.732 total time=   2.7s\n",
      "[CV 4/5; 31/36] START classifier__C=100, classifier__gamma=0.001................\n",
      "[CV 4/5; 31/36] END classifier__C=100, classifier__gamma=0.001;, score=0.733 total time=   1.8s\n",
      "[CV 5/5; 31/36] START classifier__C=100, classifier__gamma=0.001................\n",
      "[CV 5/5; 31/36] END classifier__C=100, classifier__gamma=0.001;, score=0.746 total time=   1.4s\n",
      "[CV 1/5; 32/36] START classifier__C=100, classifier__gamma=0.01.................\n",
      "[CV 1/5; 32/36] END classifier__C=100, classifier__gamma=0.01;, score=0.764 total time=   2.6s\n",
      "[CV 2/5; 32/36] START classifier__C=100, classifier__gamma=0.01.................\n",
      "[CV 2/5; 32/36] END classifier__C=100, classifier__gamma=0.01;, score=0.764 total time=   1.3s\n",
      "[CV 3/5; 32/36] START classifier__C=100, classifier__gamma=0.01.................\n",
      "[CV 3/5; 32/36] END classifier__C=100, classifier__gamma=0.01;, score=0.762 total time=   1.3s\n",
      "[CV 4/5; 32/36] START classifier__C=100, classifier__gamma=0.01.................\n",
      "[CV 4/5; 32/36] END classifier__C=100, classifier__gamma=0.01;, score=0.761 total time=   2.6s\n",
      "[CV 5/5; 32/36] START classifier__C=100, classifier__gamma=0.01.................\n",
      "[CV 5/5; 32/36] END classifier__C=100, classifier__gamma=0.01;, score=0.758 total time=   1.4s\n",
      "[CV 1/5; 33/36] START classifier__C=100, classifier__gamma=0.1..................\n",
      "[CV 1/5; 33/36] END classifier__C=100, classifier__gamma=0.1;, score=0.757 total time=   1.3s\n",
      "[CV 2/5; 33/36] START classifier__C=100, classifier__gamma=0.1..................\n",
      "[CV 2/5; 33/36] END classifier__C=100, classifier__gamma=0.1;, score=0.758 total time=   2.6s\n",
      "[CV 3/5; 33/36] START classifier__C=100, classifier__gamma=0.1..................\n",
      "[CV 3/5; 33/36] END classifier__C=100, classifier__gamma=0.1;, score=0.759 total time=   1.4s\n",
      "[CV 4/5; 33/36] START classifier__C=100, classifier__gamma=0.1..................\n",
      "[CV 4/5; 33/36] END classifier__C=100, classifier__gamma=0.1;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 33/36] START classifier__C=100, classifier__gamma=0.1..................\n",
      "[CV 5/5; 33/36] END classifier__C=100, classifier__gamma=0.1;, score=0.758 total time=   2.6s\n",
      "[CV 1/5; 34/36] START classifier__C=100, classifier__gamma=1....................\n",
      "[CV 1/5; 34/36] END classifier__C=100, classifier__gamma=1;, score=0.757 total time=   1.5s\n",
      "[CV 2/5; 34/36] START classifier__C=100, classifier__gamma=1....................\n",
      "[CV 2/5; 34/36] END classifier__C=100, classifier__gamma=1;, score=0.757 total time=   1.2s\n",
      "[CV 3/5; 34/36] START classifier__C=100, classifier__gamma=1....................\n",
      "[CV 3/5; 34/36] END classifier__C=100, classifier__gamma=1;, score=0.758 total time=   2.6s\n",
      "[CV 4/5; 34/36] START classifier__C=100, classifier__gamma=1....................\n",
      "[CV 4/5; 34/36] END classifier__C=100, classifier__gamma=1;, score=0.758 total time=   1.7s\n",
      "[CV 5/5; 34/36] START classifier__C=100, classifier__gamma=1....................\n",
      "[CV 5/5; 34/36] END classifier__C=100, classifier__gamma=1;, score=0.758 total time=   1.2s\n",
      "[CV 1/5; 35/36] START classifier__C=100, classifier__gamma=10...................\n",
      "[CV 1/5; 35/36] END classifier__C=100, classifier__gamma=10;, score=0.757 total time=   2.6s\n",
      "[CV 2/5; 35/36] START classifier__C=100, classifier__gamma=10...................\n",
      "[CV 2/5; 35/36] END classifier__C=100, classifier__gamma=10;, score=0.757 total time=   1.7s\n",
      "[CV 3/5; 35/36] START classifier__C=100, classifier__gamma=10...................\n",
      "[CV 3/5; 35/36] END classifier__C=100, classifier__gamma=10;, score=0.758 total time=   1.2s\n",
      "[CV 4/5; 35/36] START classifier__C=100, classifier__gamma=10...................\n",
      "[CV 4/5; 35/36] END classifier__C=100, classifier__gamma=10;, score=0.758 total time=   2.6s\n",
      "[CV 5/5; 35/36] START classifier__C=100, classifier__gamma=10...................\n",
      "[CV 5/5; 35/36] END classifier__C=100, classifier__gamma=10;, score=0.758 total time=   1.6s\n",
      "[CV 1/5; 36/36] START classifier__C=100, classifier__gamma=100..................\n",
      "[CV 1/5; 36/36] END classifier__C=100, classifier__gamma=100;, score=0.757 total time=   1.1s\n",
      "[CV 2/5; 36/36] START classifier__C=100, classifier__gamma=100..................\n",
      "[CV 2/5; 36/36] END classifier__C=100, classifier__gamma=100;, score=0.757 total time=   2.5s\n",
      "[CV 3/5; 36/36] START classifier__C=100, classifier__gamma=100..................\n",
      "[CV 3/5; 36/36] END classifier__C=100, classifier__gamma=100;, score=0.758 total time=   1.8s\n",
      "[CV 4/5; 36/36] START classifier__C=100, classifier__gamma=100..................\n",
      "[CV 4/5; 36/36] END classifier__C=100, classifier__gamma=100;, score=0.758 total time=   1.1s\n",
      "[CV 5/5; 36/36] START classifier__C=100, classifier__gamma=100..................\n",
      "[CV 5/5; 36/36] END classifier__C=100, classifier__gamma=100;, score=0.758 total time=   2.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10, 'classifier__gamma': 0.01}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline),\n",
    "    ('classifier', SVC(kernel='rbf'))])\n",
    "\n",
    "param_grid = {\n",
    "            'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "svc_rbf = GridSearchCV(pipe, param_grid, cv=kfold, verbose=10)\n",
    "svc_rbf.fit(X_train, y_train)\n",
    "svc_rbf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d94d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV 1/5; 1/6] START classifier__C=0.001.........................................\n",
      "[CV 1/5; 1/6] END ..........classifier__C=0.001;, score=0.754 total time= 3.7min\n",
      "[CV 2/5; 1/6] START classifier__C=0.001.........................................\n",
      "[CV 2/5; 1/6] END ..........classifier__C=0.001;, score=0.738 total time= 4.7min\n",
      "[CV 3/5; 1/6] START classifier__C=0.001.........................................\n",
      "[CV 3/5; 1/6] END ..........classifier__C=0.001;, score=0.758 total time= 4.8min\n",
      "[CV 4/5; 1/6] START classifier__C=0.001.........................................\n",
      "[CV 4/5; 1/6] END ..........classifier__C=0.001;, score=0.761 total time= 5.0min\n",
      "[CV 5/5; 1/6] START classifier__C=0.001.........................................\n",
      "[CV 5/5; 1/6] END ..........classifier__C=0.001;, score=0.746 total time= 4.5min\n",
      "[CV 1/5; 2/6] START classifier__C=0.01..........................................\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline),\n",
    "    ('classifier', SVC(kernel='linear'))])\n",
    "\n",
    "param_grid = {\n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "svc_linear = GridSearchCV(pipe, param_grid, cv=kfold, verbose=10)\n",
    "svc_linear.fit(X_train, y_train)\n",
    "svc_linear.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "942ff07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV 1/5; 1/15] START classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905\n",
      "[CV 1/5; 1/15] END classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905;, score=0.844 total time=   0.5s\n",
      "[CV 2/5; 1/15] START classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905\n",
      "[CV 2/5; 1/15] END classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905;, score=0.836 total time=   0.5s\n",
      "[CV 3/5; 1/15] START classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905\n",
      "[CV 3/5; 1/15] END classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905;, score=0.812 total time=   0.4s\n",
      "[CV 4/5; 1/15] START classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905\n",
      "[CV 4/5; 1/15] END classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905;, score=0.819 total time=   0.4s\n",
      "[CV 5/5; 1/15] START classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905\n",
      "[CV 5/5; 1/15] END classifier__colsample_bytree=0.6521556741098171, classifier__gamma=0.9094809443804246, classifier__learning_rate=0.0806269776788234, classifier__max_depth=9, classifier__min_child_weight=1, classifier__n_estimators=88, classifier__subsample=0.7855196625941905;, score=0.832 total time=   1.1s\n",
      "[CV 1/5; 2/15] START classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769\n",
      "[CV 1/5; 2/15] END classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769;, score=0.838 total time=   3.0s\n",
      "[CV 2/5; 2/15] START classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769\n",
      "[CV 2/5; 2/15] END classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769;, score=0.833 total time=   2.9s\n",
      "[CV 3/5; 2/15] START classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769\n",
      "[CV 3/5; 2/15] END classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769;, score=0.815 total time=   2.8s\n",
      "[CV 4/5; 2/15] START classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769\n",
      "[CV 4/5; 2/15] END classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769;, score=0.832 total time=   2.4s\n",
      "[CV 5/5; 2/15] START classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769\n",
      "[CV 5/5; 2/15] END classifier__colsample_bytree=0.9502400496265515, classifier__gamma=0.32270344119960725, classifier__learning_rate=0.006588644789914578, classifier__max_depth=5, classifier__min_child_weight=9, classifier__n_estimators=379, classifier__subsample=0.6007285404708769;, score=0.844 total time=   2.9s\n",
      "[CV 1/5; 3/15] START classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407\n",
      "[CV 1/5; 3/15] END classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407;, score=0.838 total time=   2.4s\n",
      "[CV 2/5; 3/15] START classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407\n",
      "[CV 2/5; 3/15] END classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407;, score=0.834 total time=   2.0s\n",
      "[CV 3/5; 3/15] START classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407\n",
      "[CV 3/5; 3/15] END classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407;, score=0.821 total time=   2.9s\n",
      "[CV 4/5; 3/15] START classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407\n",
      "[CV 4/5; 3/15] END classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407;, score=0.831 total time=   1.7s\n",
      "[CV 5/5; 3/15] START classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 3/15] END classifier__colsample_bytree=0.9585725425280502, classifier__gamma=0.5163602166055337, classifier__learning_rate=0.011211106742502967, classifier__max_depth=7, classifier__min_child_weight=10, classifier__n_estimators=252, classifier__subsample=0.7503833794040407;, score=0.844 total time=   2.8s\n",
      "[CV 1/5; 4/15] START classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183\n",
      "[CV 1/5; 4/15] END classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183;, score=0.814 total time=   2.0s\n",
      "[CV 2/5; 4/15] START classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183\n",
      "[CV 2/5; 4/15] END classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183;, score=0.840 total time=   1.2s\n",
      "[CV 3/5; 4/15] START classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183\n",
      "[CV 3/5; 4/15] END classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183;, score=0.818 total time=   2.9s\n",
      "[CV 4/5; 4/15] START classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183\n",
      "[CV 4/5; 4/15] END classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183;, score=0.828 total time=   2.6s\n",
      "[CV 5/5; 4/15] START classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183\n",
      "[CV 5/5; 4/15] END classifier__colsample_bytree=0.9092570964848536, classifier__gamma=1.398698405590885, classifier__learning_rate=0.002588541237866123, classifier__max_depth=5, classifier__min_child_weight=2, classifier__n_estimators=222, classifier__subsample=0.9911307824654183;, score=0.835 total time=   1.4s\n",
      "[CV 1/5; 5/15] START classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958\n",
      "[CV 1/5; 5/15] END classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958;, score=0.843 total time=   3.7s\n",
      "[CV 2/5; 5/15] START classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958\n",
      "[CV 2/5; 5/15] END classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958;, score=0.853 total time=   4.3s\n",
      "[CV 3/5; 5/15] START classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958\n",
      "[CV 3/5; 5/15] END classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958;, score=0.816 total time=   3.0s\n",
      "[CV 4/5; 5/15] START classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958\n",
      "[CV 4/5; 5/15] END classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958;, score=0.831 total time=   3.9s\n",
      "[CV 5/5; 5/15] START classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958\n",
      "[CV 5/5; 5/15] END classifier__colsample_bytree=0.5670777428431342, classifier__gamma=1.3579272111574079, classifier__learning_rate=0.014197375515146205, classifier__max_depth=9, classifier__min_child_weight=2, classifier__n_estimators=358, classifier__subsample=0.5115280957501958;, score=0.837 total time=   3.1s\n",
      "[CV 1/5; 6/15] START classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175\n",
      "[CV 1/5; 6/15] END classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175;, score=0.843 total time=   1.2s\n",
      "[CV 2/5; 6/15] START classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175\n",
      "[CV 2/5; 6/15] END classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175;, score=0.850 total time=   1.1s\n",
      "[CV 3/5; 6/15] START classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175\n",
      "[CV 3/5; 6/15] END classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175;, score=0.821 total time=   1.2s\n",
      "[CV 4/5; 6/15] START classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 6/15] END classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175;, score=0.828 total time=   1.2s\n",
      "[CV 5/5; 6/15] START classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175\n",
      "[CV 5/5; 6/15] END classifier__colsample_bytree=0.5155322255304156, classifier__gamma=0.9812011637139368, classifier__learning_rate=0.06332965777893791, classifier__max_depth=7, classifier__min_child_weight=8, classifier__n_estimators=124, classifier__subsample=0.9113824665682175;, score=0.841 total time=   1.2s\n",
      "[CV 1/5; 7/15] START classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595\n",
      "[CV 1/5; 7/15] END classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595;, score=0.828 total time=   4.6s\n",
      "[CV 2/5; 7/15] START classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595\n",
      "[CV 2/5; 7/15] END classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595;, score=0.840 total time=   3.5s\n",
      "[CV 3/5; 7/15] START classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595\n",
      "[CV 3/5; 7/15] END classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595;, score=0.810 total time=   4.5s\n",
      "[CV 4/5; 7/15] START classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595\n",
      "[CV 4/5; 7/15] END classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595;, score=0.825 total time=   4.4s\n",
      "[CV 5/5; 7/15] START classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595\n",
      "[CV 5/5; 7/15] END classifier__colsample_bytree=0.7142131387462136, classifier__gamma=1.312123806222483, classifier__learning_rate=0.0881118347682515, classifier__max_depth=8, classifier__min_child_weight=5, classifier__n_estimators=391, classifier__subsample=0.8763667128138595;, score=0.827 total time=   4.3s\n",
      "[CV 1/5; 8/15] START classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613\n",
      "[CV 1/5; 8/15] END classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613;, score=0.841 total time=   0.7s\n",
      "[CV 2/5; 8/15] START classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613\n",
      "[CV 2/5; 8/15] END classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613;, score=0.853 total time=   1.5s\n",
      "[CV 3/5; 8/15] START classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613\n",
      "[CV 3/5; 8/15] END classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613;, score=0.813 total time=   1.7s\n",
      "[CV 4/5; 8/15] START classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613\n",
      "[CV 4/5; 8/15] END classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613;, score=0.834 total time=   1.7s\n",
      "[CV 5/5; 8/15] START classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613\n",
      "[CV 5/5; 8/15] END classifier__colsample_bytree=0.9648900677069183, classifier__gamma=0.9892102801439695, classifier__learning_rate=0.07148107054179426, classifier__max_depth=3, classifier__min_child_weight=8, classifier__n_estimators=233, classifier__subsample=0.8088888993857613;, score=0.847 total time=   1.7s\n",
      "[CV 1/5; 9/15] START classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534\n",
      "[CV 1/5; 9/15] END classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534;, score=0.843 total time=   1.8s\n",
      "[CV 2/5; 9/15] START classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534\n",
      "[CV 2/5; 9/15] END classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534;, score=0.844 total time=   1.1s\n",
      "[CV 3/5; 9/15] START classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 9/15] END classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534;, score=0.827 total time=   1.6s\n",
      "[CV 4/5; 9/15] START classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534\n",
      "[CV 4/5; 9/15] END classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534;, score=0.838 total time=   1.8s\n",
      "[CV 5/5; 9/15] START classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534\n",
      "[CV 5/5; 9/15] END classifier__colsample_bytree=0.61573383679854, classifier__gamma=0.3644169608918786, classifier__learning_rate=0.03678580587145931, classifier__max_depth=3, classifier__min_child_weight=10, classifier__n_estimators=283, classifier__subsample=0.9489075776123534;, score=0.850 total time=   1.9s\n",
      "[CV 1/5; 10/15] START classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617\n",
      "[CV 1/5; 10/15] END classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617;, score=0.843 total time=   1.3s\n",
      "[CV 2/5; 10/15] START classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617\n",
      "[CV 2/5; 10/15] END classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617;, score=0.854 total time=   1.3s\n",
      "[CV 3/5; 10/15] START classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617\n",
      "[CV 3/5; 10/15] END classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617;, score=0.816 total time=   1.7s\n",
      "[CV 4/5; 10/15] START classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617\n",
      "[CV 4/5; 10/15] END classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617;, score=0.825 total time=   1.1s\n",
      "[CV 5/5; 10/15] START classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617\n",
      "[CV 5/5; 10/15] END classifier__colsample_bytree=0.6689135298180806, classifier__gamma=0.018782566099268427, classifier__learning_rate=0.0896431342607852, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=132, classifier__subsample=0.8021109820336617;, score=0.851 total time=   1.1s\n",
      "[CV 1/5; 11/15] START classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158\n",
      "[CV 1/5; 11/15] END classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158;, score=0.843 total time=   2.0s\n",
      "[CV 2/5; 11/15] START classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158\n",
      "[CV 2/5; 11/15] END classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158;, score=0.843 total time=   1.8s\n",
      "[CV 3/5; 11/15] START classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158\n",
      "[CV 3/5; 11/15] END classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158;, score=0.816 total time=   2.1s\n",
      "[CV 4/5; 11/15] START classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158\n",
      "[CV 4/5; 11/15] END classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158;, score=0.835 total time=   0.8s\n",
      "[CV 5/5; 11/15] START classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158\n",
      "[CV 5/5; 11/15] END classifier__colsample_bytree=0.621044843691878, classifier__gamma=1.5006496093824713, classifier__learning_rate=0.026487308876821623, classifier__max_depth=4, classifier__min_child_weight=6, classifier__n_estimators=240, classifier__subsample=0.5053525172219158;, score=0.845 total time=   2.0s\n",
      "[CV 1/5; 12/15] START classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963\n",
      "[CV 1/5; 12/15] END classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963;, score=0.841 total time=   2.2s\n",
      "[CV 2/5; 12/15] START classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 12/15] END classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963;, score=0.847 total time=   2.1s\n",
      "[CV 3/5; 12/15] START classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963\n",
      "[CV 3/5; 12/15] END classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963;, score=0.810 total time=   1.9s\n",
      "[CV 4/5; 12/15] START classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963\n",
      "[CV 4/5; 12/15] END classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963;, score=0.821 total time=   2.1s\n",
      "[CV 5/5; 12/15] START classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963\n",
      "[CV 5/5; 12/15] END classifier__colsample_bytree=0.6260419770264161, classifier__gamma=1.8089541167154652, classifier__learning_rate=0.06708351507466265, classifier__max_depth=9, classifier__min_child_weight=5, classifier__n_estimators=180, classifier__subsample=0.9739112092796963;, score=0.843 total time=   2.2s\n",
      "[CV 1/5; 13/15] START classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747\n",
      "[CV 1/5; 13/15] END classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747;, score=0.840 total time=   2.8s\n",
      "[CV 2/5; 13/15] START classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747\n",
      "[CV 2/5; 13/15] END classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747;, score=0.843 total time=   3.4s\n",
      "[CV 3/5; 13/15] START classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747\n",
      "[CV 3/5; 13/15] END classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747;, score=0.815 total time=   1.9s\n",
      "[CV 4/5; 13/15] START classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747\n",
      "[CV 4/5; 13/15] END classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747;, score=0.829 total time=   2.7s\n",
      "[CV 5/5; 13/15] START classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747\n",
      "[CV 5/5; 13/15] END classifier__colsample_bytree=0.7161298342087903, classifier__gamma=0.9422696836193882, classifier__learning_rate=0.05779134254485437, classifier__max_depth=10, classifier__min_child_weight=8, classifier__n_estimators=226, classifier__subsample=0.9120224175433747;, score=0.835 total time=   2.7s\n",
      "[CV 1/5; 14/15] START classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038\n",
      "[CV 1/5; 14/15] END classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038;, score=0.831 total time=   2.2s\n",
      "[CV 2/5; 14/15] START classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038\n",
      "[CV 2/5; 14/15] END classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038;, score=0.836 total time=   2.1s\n",
      "[CV 3/5; 14/15] START classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038\n",
      "[CV 3/5; 14/15] END classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038;, score=0.819 total time=   2.4s\n",
      "[CV 4/5; 14/15] START classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038\n",
      "[CV 4/5; 14/15] END classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038;, score=0.835 total time=   1.3s\n",
      "[CV 5/5; 14/15] START classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038\n",
      "[CV 5/5; 14/15] END classifier__colsample_bytree=0.950607424577984, classifier__gamma=1.0774178058242792, classifier__learning_rate=0.012262467227136548, classifier__max_depth=6, classifier__min_child_weight=7, classifier__n_estimators=212, classifier__subsample=0.5496561128741038;, score=0.847 total time=   2.2s\n",
      "[CV 1/5; 15/15] START classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 15/15] END classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207;, score=0.833 total time=   0.5s\n",
      "[CV 2/5; 15/15] START classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207\n",
      "[CV 2/5; 15/15] END classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207;, score=0.847 total time=   0.6s\n",
      "[CV 3/5; 15/15] START classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207\n",
      "[CV 3/5; 15/15] END classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207;, score=0.810 total time=   0.5s\n",
      "[CV 4/5; 15/15] START classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207\n",
      "[CV 4/5; 15/15] END classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207;, score=0.824 total time=   0.5s\n",
      "[CV 5/5; 15/15] START classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207\n",
      "[CV 5/5; 15/15] END classifier__colsample_bytree=0.9249440361430432, classifier__gamma=0.2956981961281695, classifier__learning_rate=0.06934680283137075, classifier__max_depth=5, classifier__min_child_weight=7, classifier__n_estimators=54, classifier__subsample=0.9616907152961207;, score=0.844 total time=   0.5s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'randm_src' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18356/131948785.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mxgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distributions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_distribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandm_src\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'randm_src' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats.distributions import uniform, randint\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline),\n",
    "    ('classifier', XGBClassifier())])\n",
    "\n",
    "param_distribution = {\n",
    "    'classifier__max_depth': randint(3, 11),\n",
    "    'classifier__learning_rate': uniform(0.001, 0.1-0.001),\n",
    "    'classifier__n_estimators': randint(50, 400),\n",
    "    'classifier__gamma': uniform(0,2),\n",
    "    'classifier__colsample_bytree': uniform(0.5, 0.5),\n",
    "    'classifier__subsample': uniform(0.5, 0.5),\n",
    "    'classifier__min_child_weight': randint(1, 11)\n",
    "}\n",
    "\n",
    "\n",
    "xgb = RandomizedSearchCV(pipe, param_distributions = param_distribution, n_iter = 15, verbose=10)\n",
    "xgb.fit(X_train, y_train)\n",
    "print(xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08696ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan 0.75757614        nan 0.75582687        nan 0.75757614\n",
      "        nan 0.75582687        nan 0.75845078        nan 0.75582687\n",
      "        nan 0.75582687        nan 0.75582687        nan 0.75582687\n",
      "        nan 0.75582687        nan 0.75582687        nan 0.75582687\n",
      "        nan 0.75582687        nan 0.75582687        nan 0.75582687\n",
      "        nan 0.75582687]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.01, 'classifier__penalty': 'l2'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline),\n",
    "    ('classifier', LogisticRegression())])\n",
    "\n",
    "param_grid = {\n",
    "            'classifier__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "            'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none']\n",
    "}\n",
    "grid_4 = GridSearchCV(pipe, param_grid, cv=kfold)\n",
    "grid_4.fit(X_train, y_train)\n",
    "grid_4.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4a3d6",
   "metadata": {},
   "source": [
    "Add decision tree, ensenble i  AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e158d36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "precision_score: 0.8440902021772939\n",
      "recall_score: 0.9580759046778464\n",
      "f1_score: 0.8974782968168664\n",
      "accuracy_score: 0.8316361167684997\n"
     ]
    }
   ],
   "source": [
    "from sklearn import  metrics\n",
    "\n",
    "models = []\n",
    "# models.append(('SVM rbf', grid_1.best_estimator_))\n",
    "models.append(('Random Forest', forest.best_estimator_))\n",
    "# models.append(('Logistic', grid_4.best_estimator_))\n",
    "# models.append(('XGB', xgb.best_estimator_))\n",
    "\n",
    "\n",
    "precision_score = []\n",
    "recall_score = []\n",
    "f1_score = []\n",
    "accuracy_score = []\n",
    "for name, model in models:\n",
    "    print(name)\n",
    "    print(\"precision_score: {}\".format(metrics.precision_score(y_test, model.predict(X_test)) ))\n",
    "    print(\"recall_score: {}\".format( metrics.recall_score(y_test, model.predict(X_test)) ))\n",
    "    print(\"f1_score: {}\".format( metrics.f1_score(y_test, model.predict(X_test)) ))\n",
    "    print(\"accuracy_score: {}\".format( metrics.accuracy_score(y_test, model.predict(X_test)) ))\n",
    "    precision_score.append(metrics.precision_score(y_test, model.predict(X_test)))\n",
    "    recall_score.append(metrics.recall_score(y_test, model.predict(X_test)))\n",
    "    f1_score.append( metrics.f1_score(y_test, model.predict(X_test)))\n",
    "    accuracy_score.append(metrics.accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba5bec7",
   "metadata": {},
   "source": [
    "# Głębokie uczenie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "277ae8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train2 = preprocess_pipeline.transform(X_train)\n",
    "X_test2 = preprocess_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84c4f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model_1 = keras.Sequential([\n",
    "              Dense(100, activation='relu', name=\"1st_layer\", input_shape=(X_train2.shape[1], )),\n",
    "              Dropout(0.2),\n",
    "              Dense(50, activation='relu', name=\"2nd_layer\"),\n",
    "              Dropout(0.2),\n",
    "              Dense(20, activation='relu', name=\"3nd_layer\"),\n",
    "              Dropout(0.2),\n",
    "              Dense(10, activation='relu', name=\"4nd_layer\"),\n",
    "              Dense(1, activation='sigmoid', name=\"output_layer\")\n",
    "], name=\"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6be4318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba3adba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.4317 - accuracy: 0.7899 - val_loss: 0.3724 - val_accuracy: 0.8259\n",
      "Epoch 2/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3747 - accuracy: 0.8244 - val_loss: 0.3595 - val_accuracy: 0.8327\n",
      "Epoch 3/100\n",
      "215/215 [==============================] - 1s 2ms/step - loss: 0.3652 - accuracy: 0.8290 - val_loss: 0.3584 - val_accuracy: 0.8364\n",
      "Epoch 4/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3612 - accuracy: 0.8318 - val_loss: 0.3562 - val_accuracy: 0.8296\n",
      "Epoch 5/100\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.3578 - accuracy: 0.8360 - val_loss: 0.3541 - val_accuracy: 0.8347\n",
      "Epoch 6/100\n",
      "215/215 [==============================] - 1s 2ms/step - loss: 0.3474 - accuracy: 0.8373 - val_loss: 0.3513 - val_accuracy: 0.8323\n",
      "Epoch 7/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3481 - accuracy: 0.8372 - val_loss: 0.3524 - val_accuracy: 0.8316\n",
      "Epoch 8/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3423 - accuracy: 0.8414 - val_loss: 0.3560 - val_accuracy: 0.8320\n",
      "Epoch 9/100\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.3373 - accuracy: 0.8430 - val_loss: 0.3516 - val_accuracy: 0.8354\n",
      "Epoch 10/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3374 - accuracy: 0.8432 - val_loss: 0.3531 - val_accuracy: 0.8374\n",
      "Epoch 11/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3315 - accuracy: 0.8469 - val_loss: 0.3556 - val_accuracy: 0.8340\n",
      "Epoch 12/100\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.3251 - accuracy: 0.8472 - val_loss: 0.3547 - val_accuracy: 0.8316\n",
      "Epoch 13/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3222 - accuracy: 0.8494 - val_loss: 0.3613 - val_accuracy: 0.8354\n",
      "Epoch 14/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3225 - accuracy: 0.8487 - val_loss: 0.3570 - val_accuracy: 0.8360\n",
      "Epoch 15/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3151 - accuracy: 0.8475 - val_loss: 0.3662 - val_accuracy: 0.8313\n",
      "Epoch 16/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.8519 - val_loss: 0.3675 - val_accuracy: 0.8347\n",
      "Epoch 17/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3114 - accuracy: 0.8538 - val_loss: 0.3686 - val_accuracy: 0.8286\n",
      "Epoch 18/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8557 - val_loss: 0.3729 - val_accuracy: 0.8255\n",
      "Epoch 19/100\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.3031 - accuracy: 0.8580 - val_loss: 0.3891 - val_accuracy: 0.8272\n",
      "Epoch 20/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3009 - accuracy: 0.8557 - val_loss: 0.3895 - val_accuracy: 0.8242\n",
      "Epoch 21/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3000 - accuracy: 0.8589 - val_loss: 0.3936 - val_accuracy: 0.8323\n",
      "Epoch 22/100\n",
      "215/215 [==============================] - 1s 2ms/step - loss: 0.2972 - accuracy: 0.8562 - val_loss: 0.3862 - val_accuracy: 0.8310\n",
      "Epoch 23/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2925 - accuracy: 0.8599 - val_loss: 0.3927 - val_accuracy: 0.8259\n",
      "Epoch 24/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2886 - accuracy: 0.8603 - val_loss: 0.3955 - val_accuracy: 0.8286\n",
      "Epoch 25/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2876 - accuracy: 0.8599 - val_loss: 0.4167 - val_accuracy: 0.8282\n",
      "Epoch 26/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2891 - accuracy: 0.8589 - val_loss: 0.4067 - val_accuracy: 0.8310\n",
      "Epoch 27/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2898 - accuracy: 0.8580 - val_loss: 0.3951 - val_accuracy: 0.8289\n",
      "Epoch 28/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2852 - accuracy: 0.8628 - val_loss: 0.4202 - val_accuracy: 0.8262\n",
      "Epoch 29/100\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.2819 - accuracy: 0.8635 - val_loss: 0.4143 - val_accuracy: 0.8255\n",
      "Epoch 30/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2799 - accuracy: 0.8648 - val_loss: 0.4235 - val_accuracy: 0.8242\n",
      "Epoch 31/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2813 - accuracy: 0.8615 - val_loss: 0.4351 - val_accuracy: 0.8248\n",
      "Epoch 32/100\n",
      "215/215 [==============================] - 1s 2ms/step - loss: 0.2811 - accuracy: 0.8622 - val_loss: 0.4402 - val_accuracy: 0.8269\n",
      "Epoch 33/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2772 - accuracy: 0.8677 - val_loss: 0.4376 - val_accuracy: 0.8303\n",
      "Epoch 34/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2741 - accuracy: 0.8653 - val_loss: 0.4504 - val_accuracy: 0.8303\n",
      "Epoch 35/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2714 - accuracy: 0.8667 - val_loss: 0.4368 - val_accuracy: 0.8259\n",
      "Epoch 36/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2714 - accuracy: 0.8654 - val_loss: 0.4713 - val_accuracy: 0.8252\n",
      "Epoch 37/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2721 - accuracy: 0.8641 - val_loss: 0.4728 - val_accuracy: 0.8286\n",
      "Epoch 38/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2654 - accuracy: 0.8701 - val_loss: 0.4774 - val_accuracy: 0.8238\n",
      "Epoch 39/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2688 - accuracy: 0.8675 - val_loss: 0.4531 - val_accuracy: 0.8204\n",
      "Epoch 40/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2696 - accuracy: 0.8685 - val_loss: 0.4666 - val_accuracy: 0.8313\n",
      "Epoch 41/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2672 - accuracy: 0.8691 - val_loss: 0.4501 - val_accuracy: 0.8191\n",
      "Epoch 42/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2637 - accuracy: 0.8693 - val_loss: 0.4784 - val_accuracy: 0.8204\n",
      "Epoch 43/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2630 - accuracy: 0.8731 - val_loss: 0.4775 - val_accuracy: 0.8228\n",
      "Epoch 44/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.8725 - val_loss: 0.4910 - val_accuracy: 0.8262\n",
      "Epoch 45/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2648 - accuracy: 0.8707 - val_loss: 0.4821 - val_accuracy: 0.8235\n",
      "Epoch 46/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2628 - accuracy: 0.8676 - val_loss: 0.4715 - val_accuracy: 0.8204\n",
      "Epoch 47/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2600 - accuracy: 0.8711 - val_loss: 0.4929 - val_accuracy: 0.8286\n",
      "Epoch 48/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2604 - accuracy: 0.8709 - val_loss: 0.5060 - val_accuracy: 0.8232\n",
      "Epoch 49/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2589 - accuracy: 0.8743 - val_loss: 0.4905 - val_accuracy: 0.8191\n",
      "Epoch 50/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2575 - accuracy: 0.8712 - val_loss: 0.5298 - val_accuracy: 0.8225\n",
      "Epoch 51/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2580 - accuracy: 0.8711 - val_loss: 0.4903 - val_accuracy: 0.8208\n",
      "Epoch 52/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2550 - accuracy: 0.8744 - val_loss: 0.4992 - val_accuracy: 0.8269\n",
      "Epoch 53/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2510 - accuracy: 0.8755 - val_loss: 0.5384 - val_accuracy: 0.8232\n",
      "Epoch 54/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2549 - accuracy: 0.8784 - val_loss: 0.5162 - val_accuracy: 0.8130\n",
      "Epoch 55/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2522 - accuracy: 0.8752 - val_loss: 0.5467 - val_accuracy: 0.8201\n",
      "Epoch 56/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2542 - accuracy: 0.8763 - val_loss: 0.5375 - val_accuracy: 0.8218\n",
      "Epoch 57/100\n",
      "215/215 [==============================] - 1s 2ms/step - loss: 0.2499 - accuracy: 0.8741 - val_loss: 0.5647 - val_accuracy: 0.8211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2477 - accuracy: 0.8744 - val_loss: 0.5713 - val_accuracy: 0.8201\n",
      "Epoch 59/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2529 - accuracy: 0.8736 - val_loss: 0.5401 - val_accuracy: 0.8201\n",
      "Epoch 60/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2539 - accuracy: 0.8752 - val_loss: 0.5419 - val_accuracy: 0.8232\n",
      "Epoch 61/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2455 - accuracy: 0.8797 - val_loss: 0.5673 - val_accuracy: 0.8248\n",
      "Epoch 62/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2472 - accuracy: 0.8795 - val_loss: 0.5728 - val_accuracy: 0.8140\n",
      "Epoch 63/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2446 - accuracy: 0.8778 - val_loss: 0.6098 - val_accuracy: 0.8313\n",
      "Epoch 64/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.8808 - val_loss: 0.5950 - val_accuracy: 0.8221\n",
      "Epoch 65/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.8773 - val_loss: 0.5875 - val_accuracy: 0.8201\n",
      "Epoch 66/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.8760 - val_loss: 0.5297 - val_accuracy: 0.8242\n",
      "Epoch 67/100\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.2451 - accuracy: 0.8794 - val_loss: 0.6210 - val_accuracy: 0.8248\n",
      "Epoch 68/100\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.2428 - accuracy: 0.8813 - val_loss: 0.6062 - val_accuracy: 0.8296\n",
      "Epoch 69/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2444 - accuracy: 0.8779 - val_loss: 0.5926 - val_accuracy: 0.8228\n",
      "Epoch 70/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2448 - accuracy: 0.8807 - val_loss: 0.5690 - val_accuracy: 0.8232\n",
      "Epoch 71/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2408 - accuracy: 0.8773 - val_loss: 0.5669 - val_accuracy: 0.8133\n",
      "Epoch 72/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2384 - accuracy: 0.8776 - val_loss: 0.6061 - val_accuracy: 0.8228\n",
      "Epoch 73/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.8811 - val_loss: 0.6162 - val_accuracy: 0.8248\n",
      "Epoch 74/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.8775 - val_loss: 0.6320 - val_accuracy: 0.8238\n",
      "Epoch 75/100\n",
      "215/215 [==============================] - 1s 2ms/step - loss: 0.2397 - accuracy: 0.8810 - val_loss: 0.6150 - val_accuracy: 0.8242\n",
      "Epoch 76/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2391 - accuracy: 0.8820 - val_loss: 0.6160 - val_accuracy: 0.8259\n",
      "Epoch 77/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2403 - accuracy: 0.8840 - val_loss: 0.6184 - val_accuracy: 0.8232\n",
      "Epoch 78/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2330 - accuracy: 0.8859 - val_loss: 0.6619 - val_accuracy: 0.8262\n",
      "Epoch 79/100\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.2387 - accuracy: 0.8810 - val_loss: 0.6049 - val_accuracy: 0.8248\n",
      "Epoch 80/100\n",
      "215/215 [==============================] - 1s 2ms/step - loss: 0.2370 - accuracy: 0.8829 - val_loss: 0.5966 - val_accuracy: 0.8218\n",
      "Epoch 81/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2312 - accuracy: 0.8816 - val_loss: 0.6621 - val_accuracy: 0.8265\n",
      "Epoch 82/100\n",
      "215/215 [==============================] - 1s 2ms/step - loss: 0.2378 - accuracy: 0.8819 - val_loss: 0.6747 - val_accuracy: 0.8272\n",
      "Epoch 83/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.8840 - val_loss: 0.6058 - val_accuracy: 0.8242\n",
      "Epoch 84/100\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.2362 - accuracy: 0.8805 - val_loss: 0.6469 - val_accuracy: 0.8269\n",
      "Epoch 85/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2345 - accuracy: 0.8846 - val_loss: 0.6805 - val_accuracy: 0.8282\n",
      "Epoch 86/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2308 - accuracy: 0.8874 - val_loss: 0.6781 - val_accuracy: 0.8245\n",
      "Epoch 87/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2335 - accuracy: 0.8839 - val_loss: 0.6942 - val_accuracy: 0.8252\n",
      "Epoch 88/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2368 - accuracy: 0.8807 - val_loss: 0.6539 - val_accuracy: 0.8235\n",
      "Epoch 89/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2326 - accuracy: 0.8804 - val_loss: 0.6902 - val_accuracy: 0.8221\n",
      "Epoch 90/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2319 - accuracy: 0.8820 - val_loss: 0.6577 - val_accuracy: 0.8232\n",
      "Epoch 91/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2361 - accuracy: 0.8821 - val_loss: 0.6955 - val_accuracy: 0.8262\n",
      "Epoch 92/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2308 - accuracy: 0.8849 - val_loss: 0.6549 - val_accuracy: 0.8252\n",
      "Epoch 93/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2299 - accuracy: 0.8836 - val_loss: 0.6735 - val_accuracy: 0.8198\n",
      "Epoch 94/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2311 - accuracy: 0.8839 - val_loss: 0.7233 - val_accuracy: 0.8228\n",
      "Epoch 95/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2338 - accuracy: 0.8835 - val_loss: 0.6824 - val_accuracy: 0.8259\n",
      "Epoch 96/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2331 - accuracy: 0.8855 - val_loss: 0.6972 - val_accuracy: 0.8272\n",
      "Epoch 97/100\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2320 - accuracy: 0.8817 - val_loss: 0.6692 - val_accuracy: 0.8279\n",
      "Epoch 98/100\n",
      "215/215 [==============================] - 1s 2ms/step - loss: 0.2299 - accuracy: 0.8801 - val_loss: 0.6978 - val_accuracy: 0.8235\n",
      "Epoch 99/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2336 - accuracy: 0.8807 - val_loss: 0.7023 - val_accuracy: 0.8255\n",
      "Epoch 100/100\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2300 - accuracy: 0.8851 - val_loss: 0.6783 - val_accuracy: 0.8242\n"
     ]
    }
   ],
   "source": [
    "history_1 = model_1.fit(X_train2, y_train, \n",
    "                      epochs=100, \n",
    "                      batch_size=32, \n",
    "                      validation_data=(X_test2, y_test),\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5849b229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history_1.history['accuracy']), label='acc')\n",
    "plt.plot(pd.DataFrame(history_1.history['val_accuracy']), label='val_acc')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4d58920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history_1.history).plot(figsize=(10, 7));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a48f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
